{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import copy\n",
    "from numpy import newaxis\n",
    "from collections import defaultdict\n",
    "from random import choices\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(28 * 28, 1000)\n",
    "            self.fc2 = nn.Linear(1000, 10)\n",
    "            \n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x)\n",
    "\n",
    "trained_nn=Net()\n",
    "trained_nn.load_state_dict(torch.load(\"Trained_MNIST_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n",
      "fc1.weight \t torch.Size([1000, 784])\n",
      "fc1.weight \t tensor([[ 0.0008, -0.0187,  0.0208,  ..., -0.0181,  0.0072,  0.0311],\n",
      "        [ 0.0129, -0.0191, -0.0032,  ..., -0.0051, -0.0218,  0.0027],\n",
      "        [ 0.0324,  0.0261,  0.0021,  ...,  0.0331, -0.0297, -0.0028],\n",
      "        ...,\n",
      "        [-0.0181, -0.0195, -0.0178,  ...,  0.0118, -0.0163, -0.0325],\n",
      "        [ 0.0254,  0.0184,  0.0181,  ...,  0.0081,  0.0158,  0.0264],\n",
      "        [-0.0274, -0.0291, -0.0372,  ..., -0.0074, -0.0170, -0.0284]])\n",
      "fc1.bias \t torch.Size([1000])\n",
      "fc1.bias \t tensor([-1.6859e-02,  1.3460e-02, -7.7439e-05, -1.3749e-02,  2.0384e-02,\n",
      "        -7.0187e-03, -2.0665e-02, -8.3884e-04, -3.3272e-02,  3.1202e-02,\n",
      "        -5.3926e-03,  1.7053e-02, -8.7766e-03, -6.1230e-03, -3.0541e-02,\n",
      "        -1.5238e-02,  1.1690e-02, -1.2237e-02, -2.2264e-02, -1.4238e-02,\n",
      "         2.8548e-02,  1.5772e-02, -3.2383e-02, -2.9137e-03,  1.1673e-02,\n",
      "         1.2503e-02,  3.5169e-02,  3.1178e-02,  1.2307e-03,  6.8792e-03,\n",
      "         1.4583e-02, -2.8054e-02,  1.1891e-02, -2.0236e-02, -2.8651e-02,\n",
      "        -3.3944e-02,  2.3523e-02, -7.7312e-03, -1.5719e-03, -3.7004e-03,\n",
      "        -4.5317e-03,  3.9485e-02,  1.1039e-02, -3.2398e-02, -6.7338e-03,\n",
      "        -3.3201e-02,  3.9959e-02,  2.7649e-02,  1.2438e-02,  2.4172e-02,\n",
      "         2.9244e-02, -3.9412e-03,  1.8805e-02,  9.3704e-03,  5.3456e-03,\n",
      "         1.4089e-02,  2.5796e-02,  2.5679e-03,  3.2045e-02, -2.0643e-02,\n",
      "        -2.8041e-02,  1.8608e-02,  1.0463e-02,  2.1094e-02,  2.8603e-02,\n",
      "        -2.8920e-02, -1.4576e-02, -9.7922e-03,  4.4662e-03,  1.8838e-02,\n",
      "         2.0700e-02, -2.8283e-02, -2.5483e-02,  2.1332e-02, -1.3758e-02,\n",
      "        -1.4950e-02, -2.0177e-02, -1.6998e-02,  4.7761e-04, -2.7114e-02,\n",
      "        -3.0164e-02, -1.8631e-02,  3.3790e-02,  1.4187e-03,  6.7427e-03,\n",
      "        -4.0292e-03, -2.5829e-02, -3.5074e-03, -2.7933e-03, -8.9478e-03,\n",
      "        -3.3876e-03, -2.7487e-02, -2.6416e-02,  3.1508e-02,  1.1353e-02,\n",
      "        -2.1050e-02,  1.3648e-02, -1.8206e-02, -2.2400e-02,  3.3383e-02,\n",
      "         2.3278e-02, -1.2263e-02, -3.7707e-02, -7.5198e-03, -4.0924e-03,\n",
      "         2.4632e-02, -1.9105e-02,  1.4256e-02,  2.8673e-02,  3.0765e-02,\n",
      "        -2.8913e-02,  3.5141e-02,  2.1355e-02, -2.4891e-02,  2.4797e-02,\n",
      "        -4.6514e-03, -2.3235e-02, -1.7997e-04, -1.2624e-02, -1.4760e-02,\n",
      "        -3.3273e-02, -7.0536e-03, -6.0006e-03,  4.1829e-03, -8.1847e-03,\n",
      "         8.2242e-03,  2.8102e-02, -5.6066e-03, -3.2138e-02,  3.2810e-02,\n",
      "        -3.7577e-02, -1.6275e-02, -1.2826e-02,  2.7613e-02, -1.9695e-02,\n",
      "        -2.9579e-02,  2.6522e-02,  1.8609e-03, -2.9071e-02, -3.6716e-02,\n",
      "        -3.4031e-02,  3.1426e-02,  7.9376e-03,  3.0186e-02,  2.8508e-02,\n",
      "         8.1561e-03,  3.0805e-02, -1.5377e-02, -1.9315e-02,  1.0547e-02,\n",
      "        -9.9232e-03, -2.8792e-03, -3.1791e-02,  1.1566e-02, -1.8887e-02,\n",
      "        -9.3176e-03,  3.4055e-02, -1.0936e-02, -3.3840e-02, -3.2402e-02,\n",
      "         1.5378e-03, -1.0825e-02,  3.2235e-03, -4.4056e-03, -1.9644e-02,\n",
      "         9.3230e-03, -1.8223e-02,  7.1267e-03, -3.1743e-02, -1.4983e-02,\n",
      "        -6.6903e-03,  1.1397e-02, -1.4394e-04, -1.7996e-02,  2.0776e-02,\n",
      "        -2.4328e-02,  3.8775e-02,  2.4633e-02, -2.2444e-02,  1.8341e-02,\n",
      "        -9.7510e-03,  1.3932e-04,  2.5034e-02, -4.3560e-03, -6.5161e-03,\n",
      "        -1.8475e-02,  3.1807e-02,  9.6688e-04, -2.8615e-02, -1.0130e-02,\n",
      "        -9.4076e-03, -1.8219e-02,  3.1576e-03,  3.1776e-02,  2.4489e-02,\n",
      "         1.0728e-02, -1.9516e-02,  1.3161e-02,  1.5823e-06, -9.0653e-03,\n",
      "         1.9601e-02, -1.2640e-02,  2.1566e-02, -1.3782e-02, -1.7915e-03,\n",
      "        -3.5319e-03, -7.8002e-04,  1.1874e-02,  3.7000e-02, -4.3763e-03,\n",
      "        -8.4175e-03, -3.1844e-02,  1.1120e-02,  2.5106e-02, -2.9080e-03,\n",
      "         2.9848e-03,  2.7033e-02, -6.6570e-03, -9.5792e-03,  5.8102e-04,\n",
      "         4.6327e-03,  1.8638e-02, -3.3958e-02, -3.7467e-03,  4.2578e-03,\n",
      "         5.9965e-04,  3.6407e-02,  1.8836e-02,  3.7980e-02, -6.7629e-03,\n",
      "         2.3661e-02,  2.6956e-04,  1.8455e-02, -2.2148e-02,  1.5825e-02,\n",
      "         1.5997e-02,  2.7452e-02, -1.1604e-02, -1.2023e-02, -2.5411e-02,\n",
      "         3.6011e-02,  3.2747e-02,  1.6033e-03,  2.4941e-02, -3.0610e-02,\n",
      "         2.1953e-02, -2.7360e-02, -2.7770e-02,  1.0782e-02, -3.3479e-02,\n",
      "        -2.7198e-02,  2.6573e-02, -2.5046e-02,  7.9420e-03,  3.2401e-02,\n",
      "         6.4453e-03,  1.5683e-02,  1.6029e-02, -2.2536e-02, -2.9484e-02,\n",
      "        -1.9416e-02,  1.7478e-02,  7.0941e-03,  1.8817e-02, -1.8567e-02,\n",
      "        -1.2144e-02, -1.6478e-02, -2.5853e-02,  2.3879e-02, -3.2549e-02,\n",
      "         1.2450e-02,  2.6691e-02, -2.9704e-02,  9.3801e-03, -3.3202e-02,\n",
      "        -2.6845e-02,  7.3531e-03,  2.8514e-02,  3.4737e-03,  2.1142e-02,\n",
      "        -6.7180e-04,  1.2243e-03, -2.6879e-02, -3.9832e-02, -2.1644e-02,\n",
      "         2.7655e-03, -1.7850e-02, -5.7381e-03, -2.4169e-03, -4.7464e-03,\n",
      "        -1.7254e-02, -2.9879e-02, -2.0908e-02,  1.2306e-02, -3.4759e-02,\n",
      "         2.5776e-02, -2.5007e-02,  1.7903e-02, -7.8976e-03, -3.4851e-02,\n",
      "         5.2703e-03, -1.2936e-02, -3.2879e-02,  9.8203e-03,  3.5353e-02,\n",
      "         3.1044e-02, -6.9357e-03, -1.0654e-02,  3.1226e-04, -2.6211e-02,\n",
      "        -2.5304e-02, -2.9473e-02,  1.2499e-02, -1.7286e-03, -6.7698e-03,\n",
      "         2.8735e-02,  2.2573e-02, -1.7346e-02, -3.7478e-03,  3.2612e-02,\n",
      "         1.2030e-02, -2.3887e-03, -4.0103e-03,  9.8798e-03, -3.7048e-02,\n",
      "         1.8162e-02, -8.9492e-03, -1.8441e-02,  3.5023e-02, -1.1574e-02,\n",
      "         5.9705e-03,  1.1949e-02, -1.3999e-02, -2.2407e-02, -1.9676e-03,\n",
      "         2.4447e-02,  2.1150e-02, -2.3664e-02, -6.8058e-03,  3.1489e-02,\n",
      "        -3.1358e-02, -8.5244e-03, -1.4621e-02,  4.6497e-03,  2.0578e-02,\n",
      "        -3.4926e-02, -3.6361e-02, -2.2970e-02,  3.9044e-02,  2.4049e-03,\n",
      "        -2.4684e-02,  1.4924e-02, -4.2351e-03,  3.4321e-02,  3.2509e-02,\n",
      "        -1.9783e-02,  1.0241e-02, -2.6026e-02, -2.5239e-02, -3.6797e-02,\n",
      "         2.2707e-02,  2.6848e-03,  3.4329e-02,  3.5793e-02, -1.5908e-02,\n",
      "         1.6972e-02, -2.2831e-02, -3.5532e-02,  3.9488e-03, -1.8188e-02,\n",
      "        -4.9939e-03, -3.3090e-02,  2.6901e-02, -6.0767e-04,  3.1327e-02,\n",
      "         1.0583e-02, -2.0820e-02, -2.4919e-02,  9.1142e-03,  1.1495e-02,\n",
      "        -2.9925e-02,  1.9034e-02, -2.2784e-02, -1.7611e-02, -1.1200e-02,\n",
      "        -3.2053e-03,  1.5466e-02, -1.8533e-02,  2.3724e-03,  3.5427e-02,\n",
      "         1.2065e-02, -2.1362e-02, -1.9177e-02, -2.9866e-02,  2.5494e-02,\n",
      "        -8.1749e-03, -1.8604e-02,  2.9149e-02,  2.9746e-02,  8.7027e-03,\n",
      "         2.9048e-02, -9.0787e-03, -1.6617e-02,  2.5716e-02, -5.2803e-03,\n",
      "         3.0528e-02, -3.4420e-02,  2.9412e-02, -2.2859e-02, -1.3110e-02,\n",
      "        -1.0653e-02,  3.9468e-03,  2.2499e-02, -3.2003e-03,  2.2555e-02,\n",
      "        -1.7004e-02,  2.7007e-02, -1.6955e-02, -1.7248e-02,  3.1350e-02,\n",
      "         2.0928e-02, -5.5677e-03, -5.4412e-03,  5.1529e-03, -4.9034e-03,\n",
      "         7.5332e-03,  2.3226e-02, -2.4154e-02,  1.6646e-02, -1.2026e-02,\n",
      "         1.6394e-02,  5.3271e-04,  3.3855e-02,  1.9687e-02,  1.0370e-02,\n",
      "        -1.3894e-02, -1.3655e-02,  9.3923e-03,  4.8331e-03, -1.7260e-02,\n",
      "         1.7349e-02,  2.4862e-02, -3.1586e-02,  2.9275e-02, -6.5938e-03,\n",
      "         5.6527e-03,  1.6568e-02, -2.7076e-02, -2.0198e-03, -2.2374e-02,\n",
      "        -1.8052e-02,  1.9144e-02,  2.7664e-02, -8.4953e-04,  2.6757e-02,\n",
      "        -4.5504e-03,  1.9892e-02, -1.4306e-02,  7.1105e-03,  3.7762e-02,\n",
      "         2.3847e-02,  2.4965e-02, -2.1920e-02,  4.0312e-02, -1.3173e-02,\n",
      "        -5.2861e-03, -3.5363e-02, -2.3227e-02, -3.4698e-03, -2.1368e-02,\n",
      "         3.4045e-02,  1.3994e-03, -1.5896e-02, -1.5125e-02, -1.3789e-02,\n",
      "        -7.3017e-03,  7.6252e-03,  8.0640e-03,  1.5374e-02,  3.4634e-02,\n",
      "        -3.1866e-02,  2.5007e-03,  7.6619e-03,  6.0747e-03,  1.8626e-02,\n",
      "        -2.8042e-03,  3.5003e-02,  2.9216e-02,  1.7812e-02, -1.8380e-02,\n",
      "         2.8252e-02, -1.4372e-02, -1.4691e-02, -2.6816e-02, -3.0506e-02,\n",
      "         5.6135e-03,  2.9473e-02, -9.0064e-03, -1.6374e-02, -1.1331e-02,\n",
      "        -1.0481e-02, -2.7522e-02,  1.3542e-02,  2.0611e-02, -6.2828e-03,\n",
      "         2.6816e-02,  3.3725e-02,  3.3997e-04, -2.0489e-02,  5.3857e-03,\n",
      "        -2.5742e-02,  1.9148e-02,  1.2377e-02,  7.2313e-03, -3.7452e-02,\n",
      "         2.2885e-02,  8.8947e-03, -3.0348e-02, -3.3915e-02, -2.7327e-02,\n",
      "         3.5974e-02, -9.0178e-03,  1.8018e-02, -2.1616e-02, -1.0715e-02,\n",
      "         1.4519e-02, -3.8186e-03,  2.9952e-03, -8.4410e-05, -1.8107e-02,\n",
      "         1.5353e-02, -8.8395e-03, -1.4761e-02,  2.0320e-02,  1.6429e-02,\n",
      "        -5.4936e-03,  2.8459e-02,  3.0983e-02,  3.1644e-02, -2.9516e-02,\n",
      "        -8.5194e-03, -2.3411e-02, -2.2855e-03, -9.5404e-03,  7.9828e-03,\n",
      "         2.1277e-02, -5.4410e-03,  2.0390e-02, -3.2354e-02, -1.1641e-02,\n",
      "        -5.3108e-03, -2.4734e-02, -1.7587e-02, -2.0852e-02,  3.1554e-02,\n",
      "         3.2099e-02, -1.1361e-02, -1.7204e-02, -3.1524e-03, -2.7776e-02,\n",
      "        -1.0160e-02,  2.4097e-02, -1.1787e-02, -1.9031e-02,  3.2482e-02,\n",
      "        -2.0089e-02, -1.4207e-02, -1.6822e-02,  2.6661e-02,  2.9094e-02,\n",
      "        -7.0493e-04,  3.0894e-02,  3.4258e-02, -1.2379e-02, -3.8460e-02,\n",
      "        -3.3450e-02,  1.6074e-02,  7.3245e-03,  2.0296e-02,  7.8330e-03,\n",
      "         2.0292e-02, -2.2893e-02,  1.7325e-02, -1.7516e-02, -3.6251e-03,\n",
      "         2.3599e-02, -1.0717e-02,  1.2064e-02, -3.3235e-02, -3.1049e-02,\n",
      "         9.9755e-03, -3.4804e-03, -9.2139e-03,  2.8075e-02,  2.7660e-02,\n",
      "        -2.8269e-02,  1.8722e-02, -3.1726e-02, -3.3104e-02,  3.4362e-02,\n",
      "        -2.5876e-02,  6.7439e-03, -1.8409e-02,  1.4823e-02,  2.6240e-03,\n",
      "         3.0980e-02,  2.8447e-02, -2.4241e-02,  2.0235e-02, -1.3473e-02,\n",
      "        -3.3415e-02,  1.8221e-02, -2.7005e-02, -1.8740e-02,  2.0403e-02,\n",
      "         2.9499e-02,  8.4409e-03, -2.3346e-03, -2.4521e-02, -1.7434e-02,\n",
      "        -1.4484e-02, -3.0119e-03, -2.5416e-02,  3.6909e-02, -2.7169e-02,\n",
      "         2.3431e-02,  3.7313e-02, -3.4502e-02, -2.5467e-02,  3.0231e-02,\n",
      "         2.7142e-02,  1.4485e-02,  1.2127e-02, -2.5843e-02, -2.2134e-02,\n",
      "        -8.6641e-03, -2.5088e-02, -1.7523e-02,  1.7729e-02, -1.8295e-02,\n",
      "        -2.0717e-02, -8.6116e-03, -5.8242e-03, -3.3423e-02, -1.1817e-02,\n",
      "         1.1313e-02,  1.0824e-02,  5.2580e-03, -2.6815e-02, -1.0093e-02,\n",
      "        -2.6711e-02,  2.6179e-02, -6.4594e-03, -3.4464e-02,  1.8325e-02,\n",
      "         2.3164e-02, -6.2270e-03, -3.1191e-02,  1.0330e-03, -1.9309e-02,\n",
      "         3.2902e-02,  8.1725e-03, -2.2966e-02,  1.9716e-02, -1.9503e-02,\n",
      "        -6.0171e-03, -1.4511e-02,  8.9028e-03,  9.5911e-03, -1.8292e-03,\n",
      "         3.0663e-02, -3.4178e-02,  5.7057e-04,  3.8771e-02, -1.8479e-02,\n",
      "         3.4330e-02,  2.3946e-02,  8.3050e-04,  1.9484e-02, -8.1931e-03,\n",
      "        -3.3266e-02, -1.0717e-02,  5.6859e-04, -2.4717e-02,  1.9662e-03,\n",
      "         1.2300e-02, -2.9758e-02,  3.0717e-02,  2.6282e-02,  1.2718e-02,\n",
      "         1.0576e-02, -2.2381e-02, -2.4229e-02,  2.9239e-02,  2.6334e-02,\n",
      "        -1.5206e-02,  1.7100e-02,  2.3403e-02, -1.9824e-02,  3.6119e-02,\n",
      "         4.4201e-04, -3.5533e-02,  2.2680e-02,  2.7533e-02, -3.3967e-03,\n",
      "         2.5771e-02, -3.5155e-02,  3.1861e-02, -8.7233e-03,  7.3481e-03,\n",
      "         1.1107e-02,  1.1738e-02,  1.9623e-02,  2.5643e-02,  1.0969e-03,\n",
      "         1.5053e-02, -1.5042e-02, -1.1546e-02, -2.2745e-02,  1.9813e-02,\n",
      "         1.8933e-02, -7.8982e-03, -1.4043e-02,  1.3550e-02,  2.1923e-02,\n",
      "         6.7255e-03, -2.4073e-02,  8.4025e-04, -7.6685e-04,  2.0121e-02,\n",
      "        -2.6024e-02, -1.9165e-02, -2.7083e-02, -2.9680e-02, -1.8938e-02,\n",
      "        -9.3477e-03,  2.1571e-02, -2.2115e-02,  2.3682e-03, -1.6781e-02,\n",
      "         1.3151e-02, -4.5459e-03,  1.4507e-02, -2.2175e-02,  2.4840e-02,\n",
      "        -5.4514e-03, -8.4552e-03,  1.4514e-02,  3.9748e-03, -2.2848e-02,\n",
      "         2.6215e-02,  9.4901e-03, -2.9091e-02, -1.4925e-02,  2.9573e-02,\n",
      "        -2.5708e-02,  1.2723e-03, -7.8757e-03, -6.0696e-03,  2.9417e-02,\n",
      "         2.8985e-02,  3.3609e-02,  2.7832e-02, -2.6490e-02,  1.9336e-04,\n",
      "         2.5887e-02, -9.0951e-03,  2.3175e-02, -1.9772e-02,  1.2605e-02,\n",
      "         9.7851e-03,  4.5328e-03, -2.4626e-02, -1.9742e-02,  1.6117e-02,\n",
      "        -1.6734e-03,  1.6714e-02, -2.9666e-02,  4.4479e-03,  2.4824e-02,\n",
      "         3.3062e-02,  1.6553e-02, -3.1407e-02,  2.2001e-02,  2.4587e-02,\n",
      "         2.8013e-02, -1.2275e-02,  3.1412e-02,  3.0962e-02,  6.6461e-03,\n",
      "         2.7241e-02,  3.4541e-02,  3.1450e-02, -7.4884e-03, -1.0530e-02,\n",
      "        -1.8937e-02, -3.6397e-02,  7.3016e-03,  1.4303e-02,  5.7484e-03,\n",
      "         6.3735e-03,  4.3044e-03,  2.5992e-02,  3.5535e-02, -2.3468e-02,\n",
      "         2.5365e-02,  2.0096e-02, -1.8787e-02,  2.9071e-02,  2.4711e-02,\n",
      "        -1.1374e-02, -1.1154e-02,  3.5301e-02, -2.7225e-02, -2.6450e-02,\n",
      "         3.3726e-02,  2.7331e-02, -1.5758e-03,  2.3927e-02, -3.6855e-03,\n",
      "        -3.0086e-02, -3.6279e-02,  7.9514e-03, -7.3358e-03,  1.8188e-03,\n",
      "         2.9849e-02, -1.8658e-02,  1.8385e-02, -3.6310e-03,  1.9497e-02,\n",
      "        -3.4201e-02,  1.8168e-02,  2.3814e-02, -1.8024e-03, -2.7913e-02,\n",
      "        -8.9350e-03,  6.8467e-04,  2.2368e-02,  3.7986e-02, -2.3333e-02,\n",
      "         2.8480e-02,  1.8220e-02, -2.8134e-02,  1.9613e-02, -2.2383e-02,\n",
      "         1.1503e-02,  6.1684e-03,  3.3738e-03, -9.6270e-03,  1.6120e-02,\n",
      "         2.6226e-02, -9.9077e-03, -1.7724e-02, -2.9454e-02,  1.3932e-03,\n",
      "        -1.3887e-02,  1.5943e-02,  1.1614e-02,  9.1565e-03,  7.2113e-03,\n",
      "         1.6717e-03, -1.3762e-02,  8.4580e-03, -2.5285e-02,  1.1806e-02,\n",
      "         2.5510e-02,  2.4366e-02,  2.6949e-03,  2.3878e-03, -7.6139e-03,\n",
      "         1.2547e-02, -5.5733e-03,  2.9174e-02, -4.9431e-03,  1.0506e-02,\n",
      "         1.1099e-02, -6.6580e-03, -6.4435e-03,  2.6558e-02, -9.3545e-03,\n",
      "        -2.6006e-02,  5.4118e-03,  3.1941e-02, -2.3838e-02, -1.7013e-02,\n",
      "         3.2366e-03, -9.0452e-03,  2.9750e-02, -1.7972e-02, -8.8481e-03,\n",
      "        -7.7971e-03, -1.1839e-02,  2.9687e-03, -1.1443e-02, -1.5221e-02,\n",
      "        -2.8425e-02, -8.4375e-04,  2.4069e-02,  6.2831e-04, -3.1947e-02,\n",
      "         1.6461e-02,  3.2303e-02,  1.7827e-02,  2.6928e-02, -7.7527e-03,\n",
      "         1.8113e-02, -1.1482e-02,  2.2411e-02,  8.5711e-04,  3.1396e-02,\n",
      "         2.8838e-02, -1.6163e-03,  3.7234e-03, -2.6119e-02,  2.0666e-02,\n",
      "         5.2280e-03, -1.1339e-02, -1.2340e-02,  3.0266e-02, -1.7305e-02,\n",
      "         5.9205e-03,  1.0644e-02,  1.0383e-02,  2.6127e-02, -1.0497e-02,\n",
      "         9.2576e-03, -2.9837e-02,  1.2071e-02, -1.5795e-02,  1.5729e-02,\n",
      "        -1.0085e-02,  3.3863e-02, -2.6172e-02,  1.9252e-02, -4.1657e-03,\n",
      "        -4.0100e-03,  3.6943e-03,  2.7675e-02,  1.4267e-02, -1.3154e-03,\n",
      "        -3.4882e-03,  3.2052e-02,  2.7069e-02,  3.0545e-02, -2.6831e-02,\n",
      "         1.7170e-02, -1.8110e-02, -1.0425e-02,  2.0602e-02, -8.8435e-03,\n",
      "        -3.3571e-02, -9.2778e-03, -4.0249e-03,  3.8933e-03, -1.3121e-02,\n",
      "         1.6497e-02, -2.1263e-02, -2.5531e-02, -3.1670e-02, -1.4764e-02,\n",
      "        -3.4465e-02,  2.4809e-02, -2.9509e-02, -2.4197e-02, -2.7523e-02,\n",
      "         1.6063e-03, -9.8763e-04, -1.9620e-02,  8.1964e-03, -1.2228e-02,\n",
      "         3.0307e-02,  8.0454e-03, -2.6529e-02,  1.2214e-02, -7.9589e-03,\n",
      "         2.0067e-02, -2.2819e-02,  1.3029e-02, -8.0304e-04, -1.4441e-02,\n",
      "        -1.3490e-02, -2.9209e-02, -2.0299e-02,  2.4623e-02,  2.3315e-02,\n",
      "        -2.5496e-02,  2.4880e-02,  1.4925e-02, -2.4660e-02,  2.2002e-02,\n",
      "         3.3283e-02,  6.8761e-03,  5.6411e-03,  1.7977e-02,  1.6078e-02,\n",
      "        -2.6897e-02, -2.0477e-02, -3.3315e-02,  3.5213e-02,  3.4475e-02])\n",
      "fc2.weight \t torch.Size([10, 1000])\n",
      "fc2.weight \t tensor([[-0.0195,  0.0006,  0.0100,  ...,  0.0241, -0.0451, -0.0907],\n",
      "        [-0.0654,  0.1105,  0.0653,  ..., -0.0439,  0.0205, -0.0064],\n",
      "        [-0.0471, -0.0803, -0.0253,  ...,  0.0279,  0.0178, -0.0140],\n",
      "        ...,\n",
      "        [-0.0892, -0.0308,  0.0981,  ..., -0.0491, -0.0717,  0.1448],\n",
      "        [ 0.0861, -0.0484, -0.1265,  ..., -0.0957, -0.0399, -0.0803],\n",
      "        [-0.0094,  0.0109,  0.0371,  ...,  0.0020, -0.0315, -0.0720]])\n",
      "fc2.bias \t torch.Size([10])\n",
      "fc2.bias \t tensor([-0.0495,  0.0138,  0.0125,  0.0118, -0.0201,  0.0038,  0.0102, -0.0494,\n",
      "         0.0655, -0.0048])\n"
     ]
    }
   ],
   "source": [
    "print(trained_nn)\n",
    "for param_tensor in trained_nn.state_dict():\n",
    "    print(param_tensor, \"\\t\", trained_nn.state_dict()[param_tensor].size())\n",
    "#     print(param_tensor, \"\\t\", len(trained_nn.state_dict()[param_tensor]))\n",
    "    print(param_tensor, \"\\t\", trained_nn.state_dict()[param_tensor])\n",
    "# print(len(trained_nn.state_dict())//2+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 1, 1000]         785,000\n",
      "            Linear-2                [-1, 1, 10]          10,010\n",
      "================================================================\n",
      "Total params: 795,010\n",
      "Trainable params: 795,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 3.03\n",
      "Estimated Total Size (MB): 3.04\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_nn.to(device)\n",
    "summary(trained_nn, input_size=(1, 28* 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the entire data and taking a subset of it\n",
    "#Getting all the batches in a single list\n",
    "batch_size=200\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before Sparsification Test set: Average loss: 0.0003, Accuracy: 9800/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "i=0\n",
    "for data, target in test_loader:\n",
    "#         print(data.shape, \" \",target.shape)\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "#         i+=1\n",
    "#     print(i)\n",
    "#         print(data.shape, \" \",target.shape)\n",
    "    net_out = trained_nn(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nBefore Sparsification Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calNonZero(trained_nn, Layers):\n",
    "    nzeros=0\n",
    "    for l in Layers:\n",
    "        for i in range(trained_nn.state_dict()[l].shape[0]):\n",
    "            for j in range(trained_nn.state_dict()[l].shape[1]):\n",
    "                if(trained_nn.state_dict()[l][i][j].item()!=0.0):\n",
    "                    nzeros+=1\n",
    "\n",
    "    return nzeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non zero entries=794000\n"
     ]
    }
   ],
   "source": [
    "Layers=[\"fc1.weight\",\"fc2.weight\"]\n",
    "print(\"Number of non zero entries={}\".format(calNonZero(trained_nn, Layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0008)\n"
     ]
    }
   ],
   "source": [
    "print((trained_nn.state_dict()[Layers[0]][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsifying layer 1, node 0\n",
      "Sparsifying layer 1, node 1\n",
      "Sparsifying layer 1, node 2\n",
      "Sparsifying layer 1, node 3\n",
      "Sparsifying layer 1, node 4\n",
      "Sparsifying layer 1, node 5\n",
      "Sparsifying layer 1, node 6\n",
      "Sparsifying layer 1, node 7\n",
      "Sparsifying layer 1, node 8\n",
      "Sparsifying layer 1, node 9\n",
      "Sparsifying layer 1, node 10\n",
      "Sparsifying layer 1, node 11\n",
      "Sparsifying layer 1, node 12\n",
      "Sparsifying layer 1, node 13\n",
      "Sparsifying layer 1, node 14\n",
      "Sparsifying layer 1, node 15\n",
      "Sparsifying layer 1, node 16\n",
      "Sparsifying layer 1, node 17\n",
      "Sparsifying layer 1, node 18\n",
      "Sparsifying layer 1, node 19\n",
      "Sparsifying layer 1, node 20\n",
      "Sparsifying layer 1, node 21\n",
      "Sparsifying layer 1, node 22\n",
      "Sparsifying layer 1, node 23\n",
      "Sparsifying layer 1, node 24\n",
      "Sparsifying layer 1, node 25\n",
      "Sparsifying layer 1, node 26\n",
      "Sparsifying layer 1, node 27\n",
      "Sparsifying layer 1, node 28\n",
      "Sparsifying layer 1, node 29\n",
      "Sparsifying layer 1, node 30\n",
      "Sparsifying layer 1, node 31\n",
      "Sparsifying layer 1, node 32\n",
      "Sparsifying layer 1, node 33\n",
      "Sparsifying layer 1, node 34\n",
      "Sparsifying layer 1, node 35\n",
      "Sparsifying layer 1, node 36\n",
      "Sparsifying layer 1, node 37\n",
      "Sparsifying layer 1, node 38\n",
      "Sparsifying layer 1, node 39\n",
      "Sparsifying layer 1, node 40\n",
      "Sparsifying layer 1, node 41\n",
      "Sparsifying layer 1, node 42\n",
      "Sparsifying layer 1, node 43\n",
      "Sparsifying layer 1, node 44\n",
      "Sparsifying layer 1, node 45\n",
      "Sparsifying layer 1, node 46\n",
      "Sparsifying layer 1, node 47\n",
      "Sparsifying layer 1, node 48\n",
      "Sparsifying layer 1, node 49\n",
      "Sparsifying layer 1, node 50\n",
      "Sparsifying layer 1, node 51\n",
      "Sparsifying layer 1, node 52\n",
      "Sparsifying layer 1, node 53\n",
      "Sparsifying layer 1, node 54\n",
      "Sparsifying layer 1, node 55\n",
      "Sparsifying layer 1, node 56\n",
      "Sparsifying layer 1, node 57\n",
      "Sparsifying layer 1, node 58\n",
      "Sparsifying layer 1, node 59\n",
      "Sparsifying layer 1, node 60\n",
      "Sparsifying layer 1, node 61\n",
      "Sparsifying layer 1, node 62\n",
      "Sparsifying layer 1, node 63\n",
      "Sparsifying layer 1, node 64\n",
      "Sparsifying layer 1, node 65\n",
      "Sparsifying layer 1, node 66\n",
      "Sparsifying layer 1, node 67\n",
      "Sparsifying layer 1, node 68\n",
      "Sparsifying layer 1, node 69\n",
      "Sparsifying layer 1, node 70\n",
      "Sparsifying layer 1, node 71\n",
      "Sparsifying layer 1, node 72\n",
      "Sparsifying layer 1, node 73\n",
      "Sparsifying layer 1, node 74\n",
      "Sparsifying layer 1, node 75\n",
      "Sparsifying layer 1, node 76\n",
      "Sparsifying layer 1, node 77\n",
      "Sparsifying layer 1, node 78\n",
      "Sparsifying layer 1, node 79\n",
      "Sparsifying layer 1, node 80\n",
      "Sparsifying layer 1, node 81\n",
      "Sparsifying layer 1, node 82\n",
      "Sparsifying layer 1, node 83\n",
      "Sparsifying layer 1, node 84\n",
      "Sparsifying layer 1, node 85\n",
      "Sparsifying layer 1, node 86\n",
      "Sparsifying layer 1, node 87\n",
      "Sparsifying layer 1, node 88\n",
      "Sparsifying layer 1, node 89\n",
      "Sparsifying layer 1, node 90\n",
      "Sparsifying layer 1, node 91\n",
      "Sparsifying layer 1, node 92\n",
      "Sparsifying layer 1, node 93\n",
      "Sparsifying layer 1, node 94\n",
      "Sparsifying layer 1, node 95\n",
      "Sparsifying layer 1, node 96\n",
      "Sparsifying layer 1, node 97\n",
      "Sparsifying layer 1, node 98\n",
      "Sparsifying layer 1, node 99\n",
      "Sparsifying layer 1, node 100\n",
      "Sparsifying layer 1, node 101\n",
      "Sparsifying layer 1, node 102\n",
      "Sparsifying layer 1, node 103\n",
      "Sparsifying layer 1, node 104\n",
      "Sparsifying layer 1, node 105\n",
      "Sparsifying layer 1, node 106\n",
      "Sparsifying layer 1, node 107\n",
      "Sparsifying layer 1, node 108\n",
      "Sparsifying layer 1, node 109\n",
      "Sparsifying layer 1, node 110\n",
      "Sparsifying layer 1, node 111\n",
      "Sparsifying layer 1, node 112\n",
      "Sparsifying layer 1, node 113\n",
      "Sparsifying layer 1, node 114\n",
      "Sparsifying layer 1, node 115\n",
      "Sparsifying layer 1, node 116\n",
      "Sparsifying layer 1, node 117\n",
      "Sparsifying layer 1, node 118\n",
      "Sparsifying layer 1, node 119\n",
      "Sparsifying layer 1, node 120\n",
      "Sparsifying layer 1, node 121\n",
      "Sparsifying layer 1, node 122\n",
      "Sparsifying layer 1, node 123\n",
      "Sparsifying layer 1, node 124\n",
      "Sparsifying layer 1, node 125\n",
      "Sparsifying layer 1, node 126\n",
      "Sparsifying layer 1, node 127\n",
      "Sparsifying layer 1, node 128\n",
      "Sparsifying layer 1, node 129\n",
      "Sparsifying layer 1, node 130\n",
      "Sparsifying layer 1, node 131\n",
      "Sparsifying layer 1, node 132\n",
      "Sparsifying layer 1, node 133\n",
      "Sparsifying layer 1, node 134\n",
      "Sparsifying layer 1, node 135\n",
      "Sparsifying layer 1, node 136\n",
      "Sparsifying layer 1, node 137\n",
      "Sparsifying layer 1, node 138\n",
      "Sparsifying layer 1, node 139\n",
      "Sparsifying layer 1, node 140\n",
      "Sparsifying layer 1, node 141\n",
      "Sparsifying layer 1, node 142\n",
      "Sparsifying layer 1, node 143\n",
      "Sparsifying layer 1, node 144\n",
      "Sparsifying layer 1, node 145\n",
      "Sparsifying layer 1, node 146\n",
      "Sparsifying layer 1, node 147\n",
      "Sparsifying layer 1, node 148\n",
      "Sparsifying layer 1, node 149\n",
      "Sparsifying layer 1, node 150\n",
      "Sparsifying layer 1, node 151\n",
      "Sparsifying layer 1, node 152\n",
      "Sparsifying layer 1, node 153\n",
      "Sparsifying layer 1, node 154\n",
      "Sparsifying layer 1, node 155\n",
      "Sparsifying layer 1, node 156\n",
      "Sparsifying layer 1, node 157\n",
      "Sparsifying layer 1, node 158\n",
      "Sparsifying layer 1, node 159\n",
      "Sparsifying layer 1, node 160\n",
      "Sparsifying layer 1, node 161\n",
      "Sparsifying layer 1, node 162\n",
      "Sparsifying layer 1, node 163\n",
      "Sparsifying layer 1, node 164\n",
      "Sparsifying layer 1, node 165\n",
      "Sparsifying layer 1, node 166\n",
      "Sparsifying layer 1, node 167\n",
      "Sparsifying layer 1, node 168\n",
      "Sparsifying layer 1, node 169\n",
      "Sparsifying layer 1, node 170\n",
      "Sparsifying layer 1, node 171\n",
      "Sparsifying layer 1, node 172\n",
      "Sparsifying layer 1, node 173\n",
      "Sparsifying layer 1, node 174\n",
      "Sparsifying layer 1, node 175\n",
      "Sparsifying layer 1, node 176\n",
      "Sparsifying layer 1, node 177\n",
      "Sparsifying layer 1, node 178\n",
      "Sparsifying layer 1, node 179\n",
      "Sparsifying layer 1, node 180\n",
      "Sparsifying layer 1, node 181\n",
      "Sparsifying layer 1, node 182\n",
      "Sparsifying layer 1, node 183\n",
      "Sparsifying layer 1, node 184\n",
      "Sparsifying layer 1, node 185\n",
      "Sparsifying layer 1, node 186\n",
      "Sparsifying layer 1, node 187\n",
      "Sparsifying layer 1, node 188\n",
      "Sparsifying layer 1, node 189\n",
      "Sparsifying layer 1, node 190\n",
      "Sparsifying layer 1, node 191\n",
      "Sparsifying layer 1, node 192\n",
      "Sparsifying layer 1, node 193\n",
      "Sparsifying layer 1, node 194\n",
      "Sparsifying layer 1, node 195\n",
      "Sparsifying layer 1, node 196\n",
      "Sparsifying layer 1, node 197\n",
      "Sparsifying layer 1, node 198\n",
      "Sparsifying layer 1, node 199\n",
      "Sparsifying layer 1, node 200\n",
      "Sparsifying layer 1, node 201\n",
      "Sparsifying layer 1, node 202\n",
      "Sparsifying layer 1, node 203\n",
      "Sparsifying layer 1, node 204\n",
      "Sparsifying layer 1, node 205\n",
      "Sparsifying layer 1, node 206\n",
      "Sparsifying layer 1, node 207\n",
      "Sparsifying layer 1, node 208\n",
      "Sparsifying layer 1, node 209\n",
      "Sparsifying layer 1, node 210\n",
      "Sparsifying layer 1, node 211\n",
      "Sparsifying layer 1, node 212\n",
      "Sparsifying layer 1, node 213\n",
      "Sparsifying layer 1, node 214\n",
      "Sparsifying layer 1, node 215\n",
      "Sparsifying layer 1, node 216\n",
      "Sparsifying layer 1, node 217\n",
      "Sparsifying layer 1, node 218\n",
      "Sparsifying layer 1, node 219\n",
      "Sparsifying layer 1, node 220\n",
      "Sparsifying layer 1, node 221\n",
      "Sparsifying layer 1, node 222\n",
      "Sparsifying layer 1, node 223\n",
      "Sparsifying layer 1, node 224\n",
      "Sparsifying layer 1, node 225\n",
      "Sparsifying layer 1, node 226\n",
      "Sparsifying layer 1, node 227\n",
      "Sparsifying layer 1, node 228\n",
      "Sparsifying layer 1, node 229\n",
      "Sparsifying layer 1, node 230\n",
      "Sparsifying layer 1, node 231\n",
      "Sparsifying layer 1, node 232\n",
      "Sparsifying layer 1, node 233\n",
      "Sparsifying layer 1, node 234\n",
      "Sparsifying layer 1, node 235\n",
      "Sparsifying layer 1, node 236\n",
      "Sparsifying layer 1, node 237\n",
      "Sparsifying layer 1, node 238\n",
      "Sparsifying layer 1, node 239\n",
      "Sparsifying layer 1, node 240\n",
      "Sparsifying layer 1, node 241\n",
      "Sparsifying layer 1, node 242\n",
      "Sparsifying layer 1, node 243\n",
      "Sparsifying layer 1, node 244\n",
      "Sparsifying layer 1, node 245\n",
      "Sparsifying layer 1, node 246\n",
      "Sparsifying layer 1, node 247\n",
      "Sparsifying layer 1, node 248\n",
      "Sparsifying layer 1, node 249\n",
      "Sparsifying layer 1, node 250\n",
      "Sparsifying layer 1, node 251\n",
      "Sparsifying layer 1, node 252\n",
      "Sparsifying layer 1, node 253\n",
      "Sparsifying layer 1, node 254\n",
      "Sparsifying layer 1, node 255\n",
      "Sparsifying layer 1, node 256\n",
      "Sparsifying layer 1, node 257\n",
      "Sparsifying layer 1, node 258\n",
      "Sparsifying layer 1, node 259\n",
      "Sparsifying layer 1, node 260\n",
      "Sparsifying layer 1, node 261\n",
      "Sparsifying layer 1, node 262\n",
      "Sparsifying layer 1, node 263\n",
      "Sparsifying layer 1, node 264\n",
      "Sparsifying layer 1, node 265\n",
      "Sparsifying layer 1, node 266\n",
      "Sparsifying layer 1, node 267\n",
      "Sparsifying layer 1, node 268\n",
      "Sparsifying layer 1, node 269\n",
      "Sparsifying layer 1, node 270\n",
      "Sparsifying layer 1, node 271\n",
      "Sparsifying layer 1, node 272\n",
      "Sparsifying layer 1, node 273\n",
      "Sparsifying layer 1, node 274\n",
      "Sparsifying layer 1, node 275\n",
      "Sparsifying layer 1, node 276\n",
      "Sparsifying layer 1, node 277\n",
      "Sparsifying layer 1, node 278\n",
      "Sparsifying layer 1, node 279\n",
      "Sparsifying layer 1, node 280\n",
      "Sparsifying layer 1, node 281\n",
      "Sparsifying layer 1, node 282\n",
      "Sparsifying layer 1, node 283\n",
      "Sparsifying layer 1, node 284\n",
      "Sparsifying layer 1, node 285\n",
      "Sparsifying layer 1, node 286\n",
      "Sparsifying layer 1, node 287\n",
      "Sparsifying layer 1, node 288\n",
      "Sparsifying layer 1, node 289\n",
      "Sparsifying layer 1, node 290\n",
      "Sparsifying layer 1, node 291\n",
      "Sparsifying layer 1, node 292\n",
      "Sparsifying layer 1, node 293\n",
      "Sparsifying layer 1, node 294\n",
      "Sparsifying layer 1, node 295\n",
      "Sparsifying layer 1, node 296\n",
      "Sparsifying layer 1, node 297\n",
      "Sparsifying layer 1, node 298\n",
      "Sparsifying layer 1, node 299\n",
      "Sparsifying layer 1, node 300\n",
      "Sparsifying layer 1, node 301\n",
      "Sparsifying layer 1, node 302\n",
      "Sparsifying layer 1, node 303\n",
      "Sparsifying layer 1, node 304\n",
      "Sparsifying layer 1, node 305\n",
      "Sparsifying layer 1, node 306\n",
      "Sparsifying layer 1, node 307\n",
      "Sparsifying layer 1, node 308\n",
      "Sparsifying layer 1, node 309\n",
      "Sparsifying layer 1, node 310\n",
      "Sparsifying layer 1, node 311\n",
      "Sparsifying layer 1, node 312\n",
      "Sparsifying layer 1, node 313\n",
      "Sparsifying layer 1, node 314\n",
      "Sparsifying layer 1, node 315\n",
      "Sparsifying layer 1, node 316\n",
      "Sparsifying layer 1, node 317\n",
      "Sparsifying layer 1, node 318\n",
      "Sparsifying layer 1, node 319\n",
      "Sparsifying layer 1, node 320\n",
      "Sparsifying layer 1, node 321\n",
      "Sparsifying layer 1, node 322\n",
      "Sparsifying layer 1, node 323\n",
      "Sparsifying layer 1, node 324\n",
      "Sparsifying layer 1, node 325\n",
      "Sparsifying layer 1, node 326\n",
      "Sparsifying layer 1, node 327\n",
      "Sparsifying layer 1, node 328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsifying layer 1, node 329\n",
      "Sparsifying layer 1, node 330\n",
      "Sparsifying layer 1, node 331\n",
      "Sparsifying layer 1, node 332\n",
      "Sparsifying layer 1, node 333\n",
      "Sparsifying layer 1, node 334\n",
      "Sparsifying layer 1, node 335\n",
      "Sparsifying layer 1, node 336\n",
      "Sparsifying layer 1, node 337\n",
      "Sparsifying layer 1, node 338\n",
      "Sparsifying layer 1, node 339\n",
      "Sparsifying layer 1, node 340\n",
      "Sparsifying layer 1, node 341\n",
      "Sparsifying layer 1, node 342\n",
      "Sparsifying layer 1, node 343\n",
      "Sparsifying layer 1, node 344\n",
      "Sparsifying layer 1, node 345\n",
      "Sparsifying layer 1, node 346\n",
      "Sparsifying layer 1, node 347\n",
      "Sparsifying layer 1, node 348\n",
      "Sparsifying layer 1, node 349\n",
      "Sparsifying layer 1, node 350\n",
      "Sparsifying layer 1, node 351\n",
      "Sparsifying layer 1, node 352\n",
      "Sparsifying layer 1, node 353\n",
      "Sparsifying layer 1, node 354\n",
      "Sparsifying layer 1, node 355\n",
      "Sparsifying layer 1, node 356\n",
      "Sparsifying layer 1, node 357\n",
      "Sparsifying layer 1, node 358\n",
      "Sparsifying layer 1, node 359\n",
      "Sparsifying layer 1, node 360\n",
      "Sparsifying layer 1, node 361\n",
      "Sparsifying layer 1, node 362\n",
      "Sparsifying layer 1, node 363\n",
      "Sparsifying layer 1, node 364\n",
      "Sparsifying layer 1, node 365\n",
      "Sparsifying layer 1, node 366\n",
      "Sparsifying layer 1, node 367\n",
      "Sparsifying layer 1, node 368\n",
      "Sparsifying layer 1, node 369\n",
      "Sparsifying layer 1, node 370\n",
      "Sparsifying layer 1, node 371\n",
      "Sparsifying layer 1, node 372\n",
      "Sparsifying layer 1, node 373\n",
      "Sparsifying layer 1, node 374\n",
      "Sparsifying layer 1, node 375\n",
      "Sparsifying layer 1, node 376\n",
      "Sparsifying layer 1, node 377\n",
      "Sparsifying layer 1, node 378\n",
      "Sparsifying layer 1, node 379\n",
      "Sparsifying layer 1, node 380\n",
      "Sparsifying layer 1, node 381\n",
      "Sparsifying layer 1, node 382\n",
      "Sparsifying layer 1, node 383\n",
      "Sparsifying layer 1, node 384\n",
      "Sparsifying layer 1, node 385\n",
      "Sparsifying layer 1, node 386\n",
      "Sparsifying layer 1, node 387\n",
      "Sparsifying layer 1, node 388\n",
      "Sparsifying layer 1, node 389\n",
      "Sparsifying layer 1, node 390\n",
      "Sparsifying layer 1, node 391\n",
      "Sparsifying layer 1, node 392\n",
      "Sparsifying layer 1, node 393\n",
      "Sparsifying layer 1, node 394\n",
      "Sparsifying layer 1, node 395\n",
      "Sparsifying layer 1, node 396\n",
      "Sparsifying layer 1, node 397\n",
      "Sparsifying layer 1, node 398\n",
      "Sparsifying layer 1, node 399\n",
      "Sparsifying layer 1, node 400\n",
      "Sparsifying layer 1, node 401\n",
      "Sparsifying layer 1, node 402\n",
      "Sparsifying layer 1, node 403\n",
      "Sparsifying layer 1, node 404\n",
      "Sparsifying layer 1, node 405\n",
      "Sparsifying layer 1, node 406\n",
      "Sparsifying layer 1, node 407\n",
      "Sparsifying layer 1, node 408\n",
      "Sparsifying layer 1, node 409\n",
      "Sparsifying layer 1, node 410\n",
      "Sparsifying layer 1, node 411\n",
      "Sparsifying layer 1, node 412\n",
      "Sparsifying layer 1, node 413\n",
      "Sparsifying layer 1, node 414\n",
      "Sparsifying layer 1, node 415\n",
      "Sparsifying layer 1, node 416\n",
      "Sparsifying layer 1, node 417\n",
      "Sparsifying layer 1, node 418\n",
      "Sparsifying layer 1, node 419\n",
      "Sparsifying layer 1, node 420\n",
      "Sparsifying layer 1, node 421\n",
      "Sparsifying layer 1, node 422\n",
      "Sparsifying layer 1, node 423\n",
      "Sparsifying layer 1, node 424\n",
      "Sparsifying layer 1, node 425\n",
      "Sparsifying layer 1, node 426\n",
      "Sparsifying layer 1, node 427\n",
      "Sparsifying layer 1, node 428\n",
      "Sparsifying layer 1, node 429\n",
      "Sparsifying layer 1, node 430\n",
      "Sparsifying layer 1, node 431\n",
      "Sparsifying layer 1, node 432\n",
      "Sparsifying layer 1, node 433\n",
      "Sparsifying layer 1, node 434\n",
      "Sparsifying layer 1, node 435\n",
      "Sparsifying layer 1, node 436\n",
      "Sparsifying layer 1, node 437\n",
      "Sparsifying layer 1, node 438\n",
      "Sparsifying layer 1, node 439\n",
      "Sparsifying layer 1, node 440\n",
      "Sparsifying layer 1, node 441\n",
      "Sparsifying layer 1, node 442\n",
      "Sparsifying layer 1, node 443\n",
      "Sparsifying layer 1, node 444\n",
      "Sparsifying layer 1, node 445\n",
      "Sparsifying layer 1, node 446\n",
      "Sparsifying layer 1, node 447\n",
      "Sparsifying layer 1, node 448\n",
      "Sparsifying layer 1, node 449\n",
      "Sparsifying layer 1, node 450\n",
      "Sparsifying layer 1, node 451\n",
      "Sparsifying layer 1, node 452\n",
      "Sparsifying layer 1, node 453\n",
      "Sparsifying layer 1, node 454\n",
      "Sparsifying layer 1, node 455\n",
      "Sparsifying layer 1, node 456\n",
      "Sparsifying layer 1, node 457\n",
      "Sparsifying layer 1, node 458\n",
      "Sparsifying layer 1, node 459\n",
      "Sparsifying layer 1, node 460\n",
      "Sparsifying layer 1, node 461\n",
      "Sparsifying layer 1, node 462\n",
      "Sparsifying layer 1, node 463\n",
      "Sparsifying layer 1, node 464\n",
      "Sparsifying layer 1, node 465\n",
      "Sparsifying layer 1, node 466\n",
      "Sparsifying layer 1, node 467\n",
      "Sparsifying layer 1, node 468\n",
      "Sparsifying layer 1, node 469\n",
      "Sparsifying layer 1, node 470\n",
      "Sparsifying layer 1, node 471\n",
      "Sparsifying layer 1, node 472\n",
      "Sparsifying layer 1, node 473\n",
      "Sparsifying layer 1, node 474\n",
      "Sparsifying layer 1, node 475\n",
      "Sparsifying layer 1, node 476\n",
      "Sparsifying layer 1, node 477\n",
      "Sparsifying layer 1, node 478\n",
      "Sparsifying layer 1, node 479\n",
      "Sparsifying layer 1, node 480\n",
      "Sparsifying layer 1, node 481\n",
      "Sparsifying layer 1, node 482\n",
      "Sparsifying layer 1, node 483\n",
      "Sparsifying layer 1, node 484\n",
      "Sparsifying layer 1, node 485\n",
      "Sparsifying layer 1, node 486\n",
      "Sparsifying layer 1, node 487\n",
      "Sparsifying layer 1, node 488\n",
      "Sparsifying layer 1, node 489\n",
      "Sparsifying layer 1, node 490\n",
      "Sparsifying layer 1, node 491\n",
      "Sparsifying layer 1, node 492\n",
      "Sparsifying layer 1, node 493\n",
      "Sparsifying layer 1, node 494\n",
      "Sparsifying layer 1, node 495\n",
      "Sparsifying layer 1, node 496\n",
      "Sparsifying layer 1, node 497\n",
      "Sparsifying layer 1, node 498\n",
      "Sparsifying layer 1, node 499\n",
      "Sparsifying layer 1, node 500\n",
      "Sparsifying layer 1, node 501\n",
      "Sparsifying layer 1, node 502\n",
      "Sparsifying layer 1, node 503\n",
      "Sparsifying layer 1, node 504\n",
      "Sparsifying layer 1, node 505\n",
      "Sparsifying layer 1, node 506\n",
      "Sparsifying layer 1, node 507\n",
      "Sparsifying layer 1, node 508\n",
      "Sparsifying layer 1, node 509\n",
      "Sparsifying layer 1, node 510\n",
      "Sparsifying layer 1, node 511\n",
      "Sparsifying layer 1, node 512\n",
      "Sparsifying layer 1, node 513\n",
      "Sparsifying layer 1, node 514\n",
      "Sparsifying layer 1, node 515\n",
      "Sparsifying layer 1, node 516\n",
      "Sparsifying layer 1, node 517\n",
      "Sparsifying layer 1, node 518\n",
      "Sparsifying layer 1, node 519\n",
      "Sparsifying layer 1, node 520\n",
      "Sparsifying layer 1, node 521\n",
      "Sparsifying layer 1, node 522\n",
      "Sparsifying layer 1, node 523\n",
      "Sparsifying layer 1, node 524\n",
      "Sparsifying layer 1, node 525\n",
      "Sparsifying layer 1, node 526\n",
      "Sparsifying layer 1, node 527\n",
      "Sparsifying layer 1, node 528\n",
      "Sparsifying layer 1, node 529\n",
      "Sparsifying layer 1, node 530\n",
      "Sparsifying layer 1, node 531\n",
      "Sparsifying layer 1, node 532\n",
      "Sparsifying layer 1, node 533\n",
      "Sparsifying layer 1, node 534\n",
      "Sparsifying layer 1, node 535\n",
      "Sparsifying layer 1, node 536\n",
      "Sparsifying layer 1, node 537\n",
      "Sparsifying layer 1, node 538\n",
      "Sparsifying layer 1, node 539\n",
      "Sparsifying layer 1, node 540\n",
      "Sparsifying layer 1, node 541\n",
      "Sparsifying layer 1, node 542\n",
      "Sparsifying layer 1, node 543\n",
      "Sparsifying layer 1, node 544\n",
      "Sparsifying layer 1, node 545\n",
      "Sparsifying layer 1, node 546\n",
      "Sparsifying layer 1, node 547\n",
      "Sparsifying layer 1, node 548\n",
      "Sparsifying layer 1, node 549\n",
      "Sparsifying layer 1, node 550\n",
      "Sparsifying layer 1, node 551\n",
      "Sparsifying layer 1, node 552\n",
      "Sparsifying layer 1, node 553\n",
      "Sparsifying layer 1, node 554\n",
      "Sparsifying layer 1, node 555\n",
      "Sparsifying layer 1, node 556\n",
      "Sparsifying layer 1, node 557\n",
      "Sparsifying layer 1, node 558\n",
      "Sparsifying layer 1, node 559\n",
      "Sparsifying layer 1, node 560\n",
      "Sparsifying layer 1, node 561\n",
      "Sparsifying layer 1, node 562\n",
      "Sparsifying layer 1, node 563\n",
      "Sparsifying layer 1, node 564\n",
      "Sparsifying layer 1, node 565\n",
      "Sparsifying layer 1, node 566\n",
      "Sparsifying layer 1, node 567\n",
      "Sparsifying layer 1, node 568\n",
      "Sparsifying layer 1, node 569\n",
      "Sparsifying layer 1, node 570\n",
      "Sparsifying layer 1, node 571\n",
      "Sparsifying layer 1, node 572\n",
      "Sparsifying layer 1, node 573\n",
      "Sparsifying layer 1, node 574\n",
      "Sparsifying layer 1, node 575\n",
      "Sparsifying layer 1, node 576\n",
      "Sparsifying layer 1, node 577\n",
      "Sparsifying layer 1, node 578\n",
      "Sparsifying layer 1, node 579\n",
      "Sparsifying layer 1, node 580\n",
      "Sparsifying layer 1, node 581\n",
      "Sparsifying layer 1, node 582\n",
      "Sparsifying layer 1, node 583\n",
      "Sparsifying layer 1, node 584\n",
      "Sparsifying layer 1, node 585\n",
      "Sparsifying layer 1, node 586\n",
      "Sparsifying layer 1, node 587\n",
      "Sparsifying layer 1, node 588\n",
      "Sparsifying layer 1, node 589\n",
      "Sparsifying layer 1, node 590\n",
      "Sparsifying layer 1, node 591\n",
      "Sparsifying layer 1, node 592\n",
      "Sparsifying layer 1, node 593\n",
      "Sparsifying layer 1, node 594\n",
      "Sparsifying layer 1, node 595\n",
      "Sparsifying layer 1, node 596\n",
      "Sparsifying layer 1, node 597\n",
      "Sparsifying layer 1, node 598\n",
      "Sparsifying layer 1, node 599\n",
      "Sparsifying layer 1, node 600\n",
      "Sparsifying layer 1, node 601\n",
      "Sparsifying layer 1, node 602\n",
      "Sparsifying layer 1, node 603\n",
      "Sparsifying layer 1, node 604\n",
      "Sparsifying layer 1, node 605\n",
      "Sparsifying layer 1, node 606\n",
      "Sparsifying layer 1, node 607\n",
      "Sparsifying layer 1, node 608\n",
      "Sparsifying layer 1, node 609\n",
      "Sparsifying layer 1, node 610\n",
      "Sparsifying layer 1, node 611\n",
      "Sparsifying layer 1, node 612\n",
      "Sparsifying layer 1, node 613\n",
      "Sparsifying layer 1, node 614\n",
      "Sparsifying layer 1, node 615\n",
      "Sparsifying layer 1, node 616\n",
      "Sparsifying layer 1, node 617\n",
      "Sparsifying layer 1, node 618\n",
      "Sparsifying layer 1, node 619\n",
      "Sparsifying layer 1, node 620\n",
      "Sparsifying layer 1, node 621\n",
      "Sparsifying layer 1, node 622\n",
      "Sparsifying layer 1, node 623\n",
      "Sparsifying layer 1, node 624\n",
      "Sparsifying layer 1, node 625\n",
      "Sparsifying layer 1, node 626\n",
      "Sparsifying layer 1, node 627\n",
      "Sparsifying layer 1, node 628\n",
      "Sparsifying layer 1, node 629\n",
      "Sparsifying layer 1, node 630\n",
      "Sparsifying layer 1, node 631\n",
      "Sparsifying layer 1, node 632\n",
      "Sparsifying layer 1, node 633\n",
      "Sparsifying layer 1, node 634\n",
      "Sparsifying layer 1, node 635\n",
      "Sparsifying layer 1, node 636\n",
      "Sparsifying layer 1, node 637\n",
      "Sparsifying layer 1, node 638\n",
      "Sparsifying layer 1, node 639\n",
      "Sparsifying layer 1, node 640\n",
      "Sparsifying layer 1, node 641\n",
      "Sparsifying layer 1, node 642\n",
      "Sparsifying layer 1, node 643\n",
      "Sparsifying layer 1, node 644\n",
      "Sparsifying layer 1, node 645\n",
      "Sparsifying layer 1, node 646\n",
      "Sparsifying layer 1, node 647\n",
      "Sparsifying layer 1, node 648\n",
      "Sparsifying layer 1, node 649\n",
      "Sparsifying layer 1, node 650\n",
      "Sparsifying layer 1, node 651\n",
      "Sparsifying layer 1, node 652\n",
      "Sparsifying layer 1, node 653\n",
      "Sparsifying layer 1, node 654\n",
      "Sparsifying layer 1, node 655\n",
      "Sparsifying layer 1, node 656\n",
      "Sparsifying layer 1, node 657\n",
      "Sparsifying layer 1, node 658\n",
      "Sparsifying layer 1, node 659\n",
      "Sparsifying layer 1, node 660\n",
      "Sparsifying layer 1, node 661\n",
      "Sparsifying layer 1, node 662\n",
      "Sparsifying layer 1, node 663\n",
      "Sparsifying layer 1, node 664\n",
      "Sparsifying layer 1, node 665\n",
      "Sparsifying layer 1, node 666\n",
      "Sparsifying layer 1, node 667\n",
      "Sparsifying layer 1, node 668\n",
      "Sparsifying layer 1, node 669\n",
      "Sparsifying layer 1, node 670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsifying layer 1, node 671\n",
      "Sparsifying layer 1, node 672\n",
      "Sparsifying layer 1, node 673\n",
      "Sparsifying layer 1, node 674\n",
      "Sparsifying layer 1, node 675\n",
      "Sparsifying layer 1, node 676\n",
      "Sparsifying layer 1, node 677\n",
      "Sparsifying layer 1, node 678\n",
      "Sparsifying layer 1, node 679\n",
      "Sparsifying layer 1, node 680\n",
      "Sparsifying layer 1, node 681\n",
      "Sparsifying layer 1, node 682\n",
      "Sparsifying layer 1, node 683\n",
      "Sparsifying layer 1, node 684\n",
      "Sparsifying layer 1, node 685\n",
      "Sparsifying layer 1, node 686\n",
      "Sparsifying layer 1, node 687\n",
      "Sparsifying layer 1, node 688\n",
      "Sparsifying layer 1, node 689\n",
      "Sparsifying layer 1, node 690\n",
      "Sparsifying layer 1, node 691\n",
      "Sparsifying layer 1, node 692\n",
      "Sparsifying layer 1, node 693\n",
      "Sparsifying layer 1, node 694\n",
      "Sparsifying layer 1, node 695\n",
      "Sparsifying layer 1, node 696\n",
      "Sparsifying layer 1, node 697\n",
      "Sparsifying layer 1, node 698\n",
      "Sparsifying layer 1, node 699\n",
      "Sparsifying layer 1, node 700\n",
      "Sparsifying layer 1, node 701\n",
      "Sparsifying layer 1, node 702\n",
      "Sparsifying layer 1, node 703\n",
      "Sparsifying layer 1, node 704\n",
      "Sparsifying layer 1, node 705\n",
      "Sparsifying layer 1, node 706\n",
      "Sparsifying layer 1, node 707\n",
      "Sparsifying layer 1, node 708\n",
      "Sparsifying layer 1, node 709\n",
      "Sparsifying layer 1, node 710\n",
      "Sparsifying layer 1, node 711\n",
      "Sparsifying layer 1, node 712\n",
      "Sparsifying layer 1, node 713\n",
      "Sparsifying layer 1, node 714\n",
      "Sparsifying layer 1, node 715\n",
      "Sparsifying layer 1, node 716\n",
      "Sparsifying layer 1, node 717\n",
      "Sparsifying layer 1, node 718\n",
      "Sparsifying layer 1, node 719\n",
      "Sparsifying layer 1, node 720\n",
      "Sparsifying layer 1, node 721\n",
      "Sparsifying layer 1, node 722\n",
      "Sparsifying layer 1, node 723\n",
      "Sparsifying layer 1, node 724\n",
      "Sparsifying layer 1, node 725\n",
      "Sparsifying layer 1, node 726\n",
      "Sparsifying layer 1, node 727\n",
      "Sparsifying layer 1, node 728\n",
      "Sparsifying layer 1, node 729\n",
      "Sparsifying layer 1, node 730\n",
      "Sparsifying layer 1, node 731\n",
      "Sparsifying layer 1, node 732\n",
      "Sparsifying layer 1, node 733\n",
      "Sparsifying layer 1, node 734\n",
      "Sparsifying layer 1, node 735\n",
      "Sparsifying layer 1, node 736\n",
      "Sparsifying layer 1, node 737\n",
      "Sparsifying layer 1, node 738\n",
      "Sparsifying layer 1, node 739\n",
      "Sparsifying layer 1, node 740\n",
      "Sparsifying layer 1, node 741\n",
      "Sparsifying layer 1, node 742\n",
      "Sparsifying layer 1, node 743\n",
      "Sparsifying layer 1, node 744\n",
      "Sparsifying layer 1, node 745\n",
      "Sparsifying layer 1, node 746\n",
      "Sparsifying layer 1, node 747\n",
      "Sparsifying layer 1, node 748\n",
      "Sparsifying layer 1, node 749\n",
      "Sparsifying layer 1, node 750\n",
      "Sparsifying layer 1, node 751\n",
      "Sparsifying layer 1, node 752\n",
      "Sparsifying layer 1, node 753\n",
      "Sparsifying layer 1, node 754\n",
      "Sparsifying layer 1, node 755\n",
      "Sparsifying layer 1, node 756\n",
      "Sparsifying layer 1, node 757\n",
      "Sparsifying layer 1, node 758\n",
      "Sparsifying layer 1, node 759\n",
      "Sparsifying layer 1, node 760\n",
      "Sparsifying layer 1, node 761\n",
      "Sparsifying layer 1, node 762\n",
      "Sparsifying layer 1, node 763\n",
      "Sparsifying layer 1, node 764\n",
      "Sparsifying layer 1, node 765\n",
      "Sparsifying layer 1, node 766\n",
      "Sparsifying layer 1, node 767\n",
      "Sparsifying layer 1, node 768\n",
      "Sparsifying layer 1, node 769\n",
      "Sparsifying layer 1, node 770\n",
      "Sparsifying layer 1, node 771\n",
      "Sparsifying layer 1, node 772\n",
      "Sparsifying layer 1, node 773\n",
      "Sparsifying layer 1, node 774\n",
      "Sparsifying layer 1, node 775\n",
      "Sparsifying layer 1, node 776\n",
      "Sparsifying layer 1, node 777\n",
      "Sparsifying layer 1, node 778\n",
      "Sparsifying layer 1, node 779\n",
      "Sparsifying layer 1, node 780\n",
      "Sparsifying layer 1, node 781\n",
      "Sparsifying layer 1, node 782\n",
      "Sparsifying layer 1, node 783\n",
      "Sparsifying layer 1, node 784\n",
      "Sparsifying layer 1, node 785\n",
      "Sparsifying layer 1, node 786\n",
      "Sparsifying layer 1, node 787\n",
      "Sparsifying layer 1, node 788\n",
      "Sparsifying layer 1, node 789\n",
      "Sparsifying layer 1, node 790\n",
      "Sparsifying layer 1, node 791\n",
      "Sparsifying layer 1, node 792\n",
      "Sparsifying layer 1, node 793\n",
      "Sparsifying layer 1, node 794\n",
      "Sparsifying layer 1, node 795\n",
      "Sparsifying layer 1, node 796\n",
      "Sparsifying layer 1, node 797\n",
      "Sparsifying layer 1, node 798\n",
      "Sparsifying layer 1, node 799\n",
      "Sparsifying layer 1, node 800\n",
      "Sparsifying layer 1, node 801\n",
      "Sparsifying layer 1, node 802\n",
      "Sparsifying layer 1, node 803\n",
      "Sparsifying layer 1, node 804\n",
      "Sparsifying layer 1, node 805\n",
      "Sparsifying layer 1, node 806\n",
      "Sparsifying layer 1, node 807\n",
      "Sparsifying layer 1, node 808\n",
      "Sparsifying layer 1, node 809\n",
      "Sparsifying layer 1, node 810\n",
      "Sparsifying layer 1, node 811\n",
      "Sparsifying layer 1, node 812\n",
      "Sparsifying layer 1, node 813\n",
      "Sparsifying layer 1, node 814\n",
      "Sparsifying layer 1, node 815\n",
      "Sparsifying layer 1, node 816\n",
      "Sparsifying layer 1, node 817\n",
      "Sparsifying layer 1, node 818\n",
      "Sparsifying layer 1, node 819\n",
      "Sparsifying layer 1, node 820\n",
      "Sparsifying layer 1, node 821\n",
      "Sparsifying layer 1, node 822\n",
      "Sparsifying layer 1, node 823\n",
      "Sparsifying layer 1, node 824\n",
      "Sparsifying layer 1, node 825\n",
      "Sparsifying layer 1, node 826\n",
      "Sparsifying layer 1, node 827\n",
      "Sparsifying layer 1, node 828\n",
      "Sparsifying layer 1, node 829\n",
      "Sparsifying layer 1, node 830\n",
      "Sparsifying layer 1, node 831\n",
      "Sparsifying layer 1, node 832\n",
      "Sparsifying layer 1, node 833\n",
      "Sparsifying layer 1, node 834\n",
      "Sparsifying layer 1, node 835\n",
      "Sparsifying layer 1, node 836\n",
      "Sparsifying layer 1, node 837\n",
      "Sparsifying layer 1, node 838\n",
      "Sparsifying layer 1, node 839\n",
      "Sparsifying layer 1, node 840\n",
      "Sparsifying layer 1, node 841\n",
      "Sparsifying layer 1, node 842\n",
      "Sparsifying layer 1, node 843\n",
      "Sparsifying layer 1, node 844\n",
      "Sparsifying layer 1, node 845\n",
      "Sparsifying layer 1, node 846\n",
      "Sparsifying layer 1, node 847\n",
      "Sparsifying layer 1, node 848\n",
      "Sparsifying layer 1, node 849\n",
      "Sparsifying layer 1, node 850\n",
      "Sparsifying layer 1, node 851\n",
      "Sparsifying layer 1, node 852\n",
      "Sparsifying layer 1, node 853\n",
      "Sparsifying layer 1, node 854\n",
      "Sparsifying layer 1, node 855\n",
      "Sparsifying layer 1, node 856\n",
      "Sparsifying layer 1, node 857\n",
      "Sparsifying layer 1, node 858\n",
      "Sparsifying layer 1, node 859\n",
      "Sparsifying layer 1, node 860\n",
      "Sparsifying layer 1, node 861\n",
      "Sparsifying layer 1, node 862\n",
      "Sparsifying layer 1, node 863\n",
      "Sparsifying layer 1, node 864\n",
      "Sparsifying layer 1, node 865\n",
      "Sparsifying layer 1, node 866\n",
      "Sparsifying layer 1, node 867\n",
      "Sparsifying layer 1, node 868\n",
      "Sparsifying layer 1, node 869\n",
      "Sparsifying layer 1, node 870\n",
      "Sparsifying layer 1, node 871\n",
      "Sparsifying layer 1, node 872\n",
      "Sparsifying layer 1, node 873\n",
      "Sparsifying layer 1, node 874\n",
      "Sparsifying layer 1, node 875\n",
      "Sparsifying layer 1, node 876\n",
      "Sparsifying layer 1, node 877\n",
      "Sparsifying layer 1, node 878\n",
      "Sparsifying layer 1, node 879\n",
      "Sparsifying layer 1, node 880\n",
      "Sparsifying layer 1, node 881\n",
      "Sparsifying layer 1, node 882\n",
      "Sparsifying layer 1, node 883\n",
      "Sparsifying layer 1, node 884\n",
      "Sparsifying layer 1, node 885\n",
      "Sparsifying layer 1, node 886\n",
      "Sparsifying layer 1, node 887\n",
      "Sparsifying layer 1, node 888\n",
      "Sparsifying layer 1, node 889\n",
      "Sparsifying layer 1, node 890\n",
      "Sparsifying layer 1, node 891\n",
      "Sparsifying layer 1, node 892\n",
      "Sparsifying layer 1, node 893\n",
      "Sparsifying layer 1, node 894\n",
      "Sparsifying layer 1, node 895\n",
      "Sparsifying layer 1, node 896\n",
      "Sparsifying layer 1, node 897\n",
      "Sparsifying layer 1, node 898\n",
      "Sparsifying layer 1, node 899\n",
      "Sparsifying layer 1, node 900\n",
      "Sparsifying layer 1, node 901\n",
      "Sparsifying layer 1, node 902\n",
      "Sparsifying layer 1, node 903\n",
      "Sparsifying layer 1, node 904\n",
      "Sparsifying layer 1, node 905\n",
      "Sparsifying layer 1, node 906\n",
      "Sparsifying layer 1, node 907\n",
      "Sparsifying layer 1, node 908\n",
      "Sparsifying layer 1, node 909\n",
      "Sparsifying layer 1, node 910\n",
      "Sparsifying layer 1, node 911\n",
      "Sparsifying layer 1, node 912\n",
      "Sparsifying layer 1, node 913\n",
      "Sparsifying layer 1, node 914\n",
      "Sparsifying layer 1, node 915\n",
      "Sparsifying layer 1, node 916\n",
      "Sparsifying layer 1, node 917\n",
      "Sparsifying layer 1, node 918\n",
      "Sparsifying layer 1, node 919\n",
      "Sparsifying layer 1, node 920\n",
      "Sparsifying layer 1, node 921\n",
      "Sparsifying layer 1, node 922\n",
      "Sparsifying layer 1, node 923\n",
      "Sparsifying layer 1, node 924\n",
      "Sparsifying layer 1, node 925\n",
      "Sparsifying layer 1, node 926\n",
      "Sparsifying layer 1, node 927\n",
      "Sparsifying layer 1, node 928\n",
      "Sparsifying layer 1, node 929\n",
      "Sparsifying layer 1, node 930\n",
      "Sparsifying layer 1, node 931\n",
      "Sparsifying layer 1, node 932\n",
      "Sparsifying layer 1, node 933\n",
      "Sparsifying layer 1, node 934\n",
      "Sparsifying layer 1, node 935\n",
      "Sparsifying layer 1, node 936\n",
      "Sparsifying layer 1, node 937\n",
      "Sparsifying layer 1, node 938\n",
      "Sparsifying layer 1, node 939\n",
      "Sparsifying layer 1, node 940\n",
      "Sparsifying layer 1, node 941\n",
      "Sparsifying layer 1, node 942\n",
      "Sparsifying layer 1, node 943\n",
      "Sparsifying layer 1, node 944\n",
      "Sparsifying layer 1, node 945\n",
      "Sparsifying layer 1, node 946\n",
      "Sparsifying layer 1, node 947\n",
      "Sparsifying layer 1, node 948\n",
      "Sparsifying layer 1, node 949\n",
      "Sparsifying layer 1, node 950\n",
      "Sparsifying layer 1, node 951\n",
      "Sparsifying layer 1, node 952\n",
      "Sparsifying layer 1, node 953\n",
      "Sparsifying layer 1, node 954\n",
      "Sparsifying layer 1, node 955\n",
      "Sparsifying layer 1, node 956\n",
      "Sparsifying layer 1, node 957\n",
      "Sparsifying layer 1, node 958\n",
      "Sparsifying layer 1, node 959\n",
      "Sparsifying layer 1, node 960\n",
      "Sparsifying layer 1, node 961\n",
      "Sparsifying layer 1, node 962\n",
      "Sparsifying layer 1, node 963\n",
      "Sparsifying layer 1, node 964\n",
      "Sparsifying layer 1, node 965\n",
      "Sparsifying layer 1, node 966\n",
      "Sparsifying layer 1, node 967\n",
      "Sparsifying layer 1, node 968\n",
      "Sparsifying layer 1, node 969\n",
      "Sparsifying layer 1, node 970\n",
      "Sparsifying layer 1, node 971\n",
      "Sparsifying layer 1, node 972\n",
      "Sparsifying layer 1, node 973\n",
      "Sparsifying layer 1, node 974\n",
      "Sparsifying layer 1, node 975\n",
      "Sparsifying layer 1, node 976\n",
      "Sparsifying layer 1, node 977\n",
      "Sparsifying layer 1, node 978\n",
      "Sparsifying layer 1, node 979\n",
      "Sparsifying layer 1, node 980\n",
      "Sparsifying layer 1, node 981\n",
      "Sparsifying layer 1, node 982\n",
      "Sparsifying layer 1, node 983\n",
      "Sparsifying layer 1, node 984\n",
      "Sparsifying layer 1, node 985\n",
      "Sparsifying layer 1, node 986\n",
      "Sparsifying layer 1, node 987\n",
      "Sparsifying layer 1, node 988\n",
      "Sparsifying layer 1, node 989\n",
      "Sparsifying layer 1, node 990\n",
      "Sparsifying layer 1, node 991\n",
      "Sparsifying layer 1, node 992\n",
      "Sparsifying layer 1, node 993\n",
      "Sparsifying layer 1, node 994\n",
      "Sparsifying layer 1, node 995\n",
      "Sparsifying layer 1, node 996\n",
      "Sparsifying layer 1, node 997\n",
      "Sparsifying layer 1, node 998\n",
      "Sparsifying layer 1, node 999\n",
      "Sparsifying layer 2, node 0\n",
      "Sparsifying layer 2, node 1\n",
      "Sparsifying layer 2, node 2\n",
      "Sparsifying layer 2, node 3\n",
      "Sparsifying layer 2, node 4\n",
      "Sparsifying layer 2, node 5\n",
      "Sparsifying layer 2, node 6\n",
      "Sparsifying layer 2, node 7\n",
      "Sparsifying layer 2, node 8\n",
      "Sparsifying layer 2, node 9\n",
      "Total time taken=1.2099623680114746\n"
     ]
    }
   ],
   "source": [
    "trained_nn=Net()\n",
    "trained_nn.load_state_dict(torch.load(\"Trained_MNIST_model\"))\n",
    "Layers=[\"fc1.weight\",\"fc2.weight\"]\n",
    "\n",
    "import time\n",
    "start_time=time.time()\n",
    "\n",
    "for l in range(len(Layers)): # l will point to one layer at a time\n",
    "    prev_layer_nodes_count=len(trained_nn.state_dict()[Layers[l]][0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #****************************************************************************************************************\n",
    "    #****************************************************************************************************************\n",
    "    #****************************************************************************************************************\n",
    "    #               Here in the below line we are choosing what percentage of nonzero weights we will keep\n",
    "    #we are choosing the percentage of nnz() to be kept here in the below line\n",
    "    \n",
    "    percent_to_keep = 0.1\n",
    "    m=math.ceil(percent_to_keep*(prev_layer_nodes_count))\n",
    "                     \n",
    "                     \n",
    "    for i in range(len(trained_nn.state_dict()[Layers[l]])): # i will point to one node of the current layer at a time\n",
    "        print(\"Sparsifying layer {}, node {}\".format(l+1, i))\n",
    "        w_small=trained_nn.state_dict()[Layers[l]][i]\n",
    "        w_small_cap=torch.zeros([prev_layer_nodes_count], dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #****************************************************************************************************************\n",
    "        #****************************************************************************************************************\n",
    "        #****************************************************************************************************************\n",
    "        #               Here in the below line we are choosing with replacement or without\n",
    "    \n",
    "        C=np.random.choice(prev_layer_nodes_count, m, replace=True)\n",
    "        \n",
    "        \n",
    "        q_j=1/prev_layer_nodes_count\n",
    "        \n",
    "        for j in C:\n",
    "            w_small_cap[j]+=(w_small[j])/(m*q_j)\n",
    "            \n",
    "        trained_nn.state_dict()[Layers[l]][i]=w_small_cap\n",
    "        \n",
    "        \n",
    "end_time=time.time()\n",
    "print(\"Total time taken={}\".format(end_time-start_time))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n",
      "fc1.weight \t torch.Size([1000, 784])\n",
      "fc1.weight \t tensor([[ 0.0082,  0.0000,  0.0000,  ...,  0.0000,  0.0713,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.2164,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000, -0.1935,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "fc1.bias \t torch.Size([1000])\n",
      "fc1.bias \t tensor([-1.6859e-02,  1.3460e-02, -7.7439e-05, -1.3749e-02,  2.0384e-02,\n",
      "        -7.0187e-03, -2.0665e-02, -8.3884e-04, -3.3272e-02,  3.1202e-02,\n",
      "        -5.3926e-03,  1.7053e-02, -8.7766e-03, -6.1230e-03, -3.0541e-02,\n",
      "        -1.5238e-02,  1.1690e-02, -1.2237e-02, -2.2264e-02, -1.4238e-02,\n",
      "         2.8548e-02,  1.5772e-02, -3.2383e-02, -2.9137e-03,  1.1673e-02,\n",
      "         1.2503e-02,  3.5169e-02,  3.1178e-02,  1.2307e-03,  6.8792e-03,\n",
      "         1.4583e-02, -2.8054e-02,  1.1891e-02, -2.0236e-02, -2.8651e-02,\n",
      "        -3.3944e-02,  2.3523e-02, -7.7312e-03, -1.5719e-03, -3.7004e-03,\n",
      "        -4.5317e-03,  3.9485e-02,  1.1039e-02, -3.2398e-02, -6.7338e-03,\n",
      "        -3.3201e-02,  3.9959e-02,  2.7649e-02,  1.2438e-02,  2.4172e-02,\n",
      "         2.9244e-02, -3.9412e-03,  1.8805e-02,  9.3704e-03,  5.3456e-03,\n",
      "         1.4089e-02,  2.5796e-02,  2.5679e-03,  3.2045e-02, -2.0643e-02,\n",
      "        -2.8041e-02,  1.8608e-02,  1.0463e-02,  2.1094e-02,  2.8603e-02,\n",
      "        -2.8920e-02, -1.4576e-02, -9.7922e-03,  4.4662e-03,  1.8838e-02,\n",
      "         2.0700e-02, -2.8283e-02, -2.5483e-02,  2.1332e-02, -1.3758e-02,\n",
      "        -1.4950e-02, -2.0177e-02, -1.6998e-02,  4.7761e-04, -2.7114e-02,\n",
      "        -3.0164e-02, -1.8631e-02,  3.3790e-02,  1.4187e-03,  6.7427e-03,\n",
      "        -4.0292e-03, -2.5829e-02, -3.5074e-03, -2.7933e-03, -8.9478e-03,\n",
      "        -3.3876e-03, -2.7487e-02, -2.6416e-02,  3.1508e-02,  1.1353e-02,\n",
      "        -2.1050e-02,  1.3648e-02, -1.8206e-02, -2.2400e-02,  3.3383e-02,\n",
      "         2.3278e-02, -1.2263e-02, -3.7707e-02, -7.5198e-03, -4.0924e-03,\n",
      "         2.4632e-02, -1.9105e-02,  1.4256e-02,  2.8673e-02,  3.0765e-02,\n",
      "        -2.8913e-02,  3.5141e-02,  2.1355e-02, -2.4891e-02,  2.4797e-02,\n",
      "        -4.6514e-03, -2.3235e-02, -1.7997e-04, -1.2624e-02, -1.4760e-02,\n",
      "        -3.3273e-02, -7.0536e-03, -6.0006e-03,  4.1829e-03, -8.1847e-03,\n",
      "         8.2242e-03,  2.8102e-02, -5.6066e-03, -3.2138e-02,  3.2810e-02,\n",
      "        -3.7577e-02, -1.6275e-02, -1.2826e-02,  2.7613e-02, -1.9695e-02,\n",
      "        -2.9579e-02,  2.6522e-02,  1.8609e-03, -2.9071e-02, -3.6716e-02,\n",
      "        -3.4031e-02,  3.1426e-02,  7.9376e-03,  3.0186e-02,  2.8508e-02,\n",
      "         8.1561e-03,  3.0805e-02, -1.5377e-02, -1.9315e-02,  1.0547e-02,\n",
      "        -9.9232e-03, -2.8792e-03, -3.1791e-02,  1.1566e-02, -1.8887e-02,\n",
      "        -9.3176e-03,  3.4055e-02, -1.0936e-02, -3.3840e-02, -3.2402e-02,\n",
      "         1.5378e-03, -1.0825e-02,  3.2235e-03, -4.4056e-03, -1.9644e-02,\n",
      "         9.3230e-03, -1.8223e-02,  7.1267e-03, -3.1743e-02, -1.4983e-02,\n",
      "        -6.6903e-03,  1.1397e-02, -1.4394e-04, -1.7996e-02,  2.0776e-02,\n",
      "        -2.4328e-02,  3.8775e-02,  2.4633e-02, -2.2444e-02,  1.8341e-02,\n",
      "        -9.7510e-03,  1.3932e-04,  2.5034e-02, -4.3560e-03, -6.5161e-03,\n",
      "        -1.8475e-02,  3.1807e-02,  9.6688e-04, -2.8615e-02, -1.0130e-02,\n",
      "        -9.4076e-03, -1.8219e-02,  3.1576e-03,  3.1776e-02,  2.4489e-02,\n",
      "         1.0728e-02, -1.9516e-02,  1.3161e-02,  1.5823e-06, -9.0653e-03,\n",
      "         1.9601e-02, -1.2640e-02,  2.1566e-02, -1.3782e-02, -1.7915e-03,\n",
      "        -3.5319e-03, -7.8002e-04,  1.1874e-02,  3.7000e-02, -4.3763e-03,\n",
      "        -8.4175e-03, -3.1844e-02,  1.1120e-02,  2.5106e-02, -2.9080e-03,\n",
      "         2.9848e-03,  2.7033e-02, -6.6570e-03, -9.5792e-03,  5.8102e-04,\n",
      "         4.6327e-03,  1.8638e-02, -3.3958e-02, -3.7467e-03,  4.2578e-03,\n",
      "         5.9965e-04,  3.6407e-02,  1.8836e-02,  3.7980e-02, -6.7629e-03,\n",
      "         2.3661e-02,  2.6956e-04,  1.8455e-02, -2.2148e-02,  1.5825e-02,\n",
      "         1.5997e-02,  2.7452e-02, -1.1604e-02, -1.2023e-02, -2.5411e-02,\n",
      "         3.6011e-02,  3.2747e-02,  1.6033e-03,  2.4941e-02, -3.0610e-02,\n",
      "         2.1953e-02, -2.7360e-02, -2.7770e-02,  1.0782e-02, -3.3479e-02,\n",
      "        -2.7198e-02,  2.6573e-02, -2.5046e-02,  7.9420e-03,  3.2401e-02,\n",
      "         6.4453e-03,  1.5683e-02,  1.6029e-02, -2.2536e-02, -2.9484e-02,\n",
      "        -1.9416e-02,  1.7478e-02,  7.0941e-03,  1.8817e-02, -1.8567e-02,\n",
      "        -1.2144e-02, -1.6478e-02, -2.5853e-02,  2.3879e-02, -3.2549e-02,\n",
      "         1.2450e-02,  2.6691e-02, -2.9704e-02,  9.3801e-03, -3.3202e-02,\n",
      "        -2.6845e-02,  7.3531e-03,  2.8514e-02,  3.4737e-03,  2.1142e-02,\n",
      "        -6.7180e-04,  1.2243e-03, -2.6879e-02, -3.9832e-02, -2.1644e-02,\n",
      "         2.7655e-03, -1.7850e-02, -5.7381e-03, -2.4169e-03, -4.7464e-03,\n",
      "        -1.7254e-02, -2.9879e-02, -2.0908e-02,  1.2306e-02, -3.4759e-02,\n",
      "         2.5776e-02, -2.5007e-02,  1.7903e-02, -7.8976e-03, -3.4851e-02,\n",
      "         5.2703e-03, -1.2936e-02, -3.2879e-02,  9.8203e-03,  3.5353e-02,\n",
      "         3.1044e-02, -6.9357e-03, -1.0654e-02,  3.1226e-04, -2.6211e-02,\n",
      "        -2.5304e-02, -2.9473e-02,  1.2499e-02, -1.7286e-03, -6.7698e-03,\n",
      "         2.8735e-02,  2.2573e-02, -1.7346e-02, -3.7478e-03,  3.2612e-02,\n",
      "         1.2030e-02, -2.3887e-03, -4.0103e-03,  9.8798e-03, -3.7048e-02,\n",
      "         1.8162e-02, -8.9492e-03, -1.8441e-02,  3.5023e-02, -1.1574e-02,\n",
      "         5.9705e-03,  1.1949e-02, -1.3999e-02, -2.2407e-02, -1.9676e-03,\n",
      "         2.4447e-02,  2.1150e-02, -2.3664e-02, -6.8058e-03,  3.1489e-02,\n",
      "        -3.1358e-02, -8.5244e-03, -1.4621e-02,  4.6497e-03,  2.0578e-02,\n",
      "        -3.4926e-02, -3.6361e-02, -2.2970e-02,  3.9044e-02,  2.4049e-03,\n",
      "        -2.4684e-02,  1.4924e-02, -4.2351e-03,  3.4321e-02,  3.2509e-02,\n",
      "        -1.9783e-02,  1.0241e-02, -2.6026e-02, -2.5239e-02, -3.6797e-02,\n",
      "         2.2707e-02,  2.6848e-03,  3.4329e-02,  3.5793e-02, -1.5908e-02,\n",
      "         1.6972e-02, -2.2831e-02, -3.5532e-02,  3.9488e-03, -1.8188e-02,\n",
      "        -4.9939e-03, -3.3090e-02,  2.6901e-02, -6.0767e-04,  3.1327e-02,\n",
      "         1.0583e-02, -2.0820e-02, -2.4919e-02,  9.1142e-03,  1.1495e-02,\n",
      "        -2.9925e-02,  1.9034e-02, -2.2784e-02, -1.7611e-02, -1.1200e-02,\n",
      "        -3.2053e-03,  1.5466e-02, -1.8533e-02,  2.3724e-03,  3.5427e-02,\n",
      "         1.2065e-02, -2.1362e-02, -1.9177e-02, -2.9866e-02,  2.5494e-02,\n",
      "        -8.1749e-03, -1.8604e-02,  2.9149e-02,  2.9746e-02,  8.7027e-03,\n",
      "         2.9048e-02, -9.0787e-03, -1.6617e-02,  2.5716e-02, -5.2803e-03,\n",
      "         3.0528e-02, -3.4420e-02,  2.9412e-02, -2.2859e-02, -1.3110e-02,\n",
      "        -1.0653e-02,  3.9468e-03,  2.2499e-02, -3.2003e-03,  2.2555e-02,\n",
      "        -1.7004e-02,  2.7007e-02, -1.6955e-02, -1.7248e-02,  3.1350e-02,\n",
      "         2.0928e-02, -5.5677e-03, -5.4412e-03,  5.1529e-03, -4.9034e-03,\n",
      "         7.5332e-03,  2.3226e-02, -2.4154e-02,  1.6646e-02, -1.2026e-02,\n",
      "         1.6394e-02,  5.3271e-04,  3.3855e-02,  1.9687e-02,  1.0370e-02,\n",
      "        -1.3894e-02, -1.3655e-02,  9.3923e-03,  4.8331e-03, -1.7260e-02,\n",
      "         1.7349e-02,  2.4862e-02, -3.1586e-02,  2.9275e-02, -6.5938e-03,\n",
      "         5.6527e-03,  1.6568e-02, -2.7076e-02, -2.0198e-03, -2.2374e-02,\n",
      "        -1.8052e-02,  1.9144e-02,  2.7664e-02, -8.4953e-04,  2.6757e-02,\n",
      "        -4.5504e-03,  1.9892e-02, -1.4306e-02,  7.1105e-03,  3.7762e-02,\n",
      "         2.3847e-02,  2.4965e-02, -2.1920e-02,  4.0312e-02, -1.3173e-02,\n",
      "        -5.2861e-03, -3.5363e-02, -2.3227e-02, -3.4698e-03, -2.1368e-02,\n",
      "         3.4045e-02,  1.3994e-03, -1.5896e-02, -1.5125e-02, -1.3789e-02,\n",
      "        -7.3017e-03,  7.6252e-03,  8.0640e-03,  1.5374e-02,  3.4634e-02,\n",
      "        -3.1866e-02,  2.5007e-03,  7.6619e-03,  6.0747e-03,  1.8626e-02,\n",
      "        -2.8042e-03,  3.5003e-02,  2.9216e-02,  1.7812e-02, -1.8380e-02,\n",
      "         2.8252e-02, -1.4372e-02, -1.4691e-02, -2.6816e-02, -3.0506e-02,\n",
      "         5.6135e-03,  2.9473e-02, -9.0064e-03, -1.6374e-02, -1.1331e-02,\n",
      "        -1.0481e-02, -2.7522e-02,  1.3542e-02,  2.0611e-02, -6.2828e-03,\n",
      "         2.6816e-02,  3.3725e-02,  3.3997e-04, -2.0489e-02,  5.3857e-03,\n",
      "        -2.5742e-02,  1.9148e-02,  1.2377e-02,  7.2313e-03, -3.7452e-02,\n",
      "         2.2885e-02,  8.8947e-03, -3.0348e-02, -3.3915e-02, -2.7327e-02,\n",
      "         3.5974e-02, -9.0178e-03,  1.8018e-02, -2.1616e-02, -1.0715e-02,\n",
      "         1.4519e-02, -3.8186e-03,  2.9952e-03, -8.4410e-05, -1.8107e-02,\n",
      "         1.5353e-02, -8.8395e-03, -1.4761e-02,  2.0320e-02,  1.6429e-02,\n",
      "        -5.4936e-03,  2.8459e-02,  3.0983e-02,  3.1644e-02, -2.9516e-02,\n",
      "        -8.5194e-03, -2.3411e-02, -2.2855e-03, -9.5404e-03,  7.9828e-03,\n",
      "         2.1277e-02, -5.4410e-03,  2.0390e-02, -3.2354e-02, -1.1641e-02,\n",
      "        -5.3108e-03, -2.4734e-02, -1.7587e-02, -2.0852e-02,  3.1554e-02,\n",
      "         3.2099e-02, -1.1361e-02, -1.7204e-02, -3.1524e-03, -2.7776e-02,\n",
      "        -1.0160e-02,  2.4097e-02, -1.1787e-02, -1.9031e-02,  3.2482e-02,\n",
      "        -2.0089e-02, -1.4207e-02, -1.6822e-02,  2.6661e-02,  2.9094e-02,\n",
      "        -7.0493e-04,  3.0894e-02,  3.4258e-02, -1.2379e-02, -3.8460e-02,\n",
      "        -3.3450e-02,  1.6074e-02,  7.3245e-03,  2.0296e-02,  7.8330e-03,\n",
      "         2.0292e-02, -2.2893e-02,  1.7325e-02, -1.7516e-02, -3.6251e-03,\n",
      "         2.3599e-02, -1.0717e-02,  1.2064e-02, -3.3235e-02, -3.1049e-02,\n",
      "         9.9755e-03, -3.4804e-03, -9.2139e-03,  2.8075e-02,  2.7660e-02,\n",
      "        -2.8269e-02,  1.8722e-02, -3.1726e-02, -3.3104e-02,  3.4362e-02,\n",
      "        -2.5876e-02,  6.7439e-03, -1.8409e-02,  1.4823e-02,  2.6240e-03,\n",
      "         3.0980e-02,  2.8447e-02, -2.4241e-02,  2.0235e-02, -1.3473e-02,\n",
      "        -3.3415e-02,  1.8221e-02, -2.7005e-02, -1.8740e-02,  2.0403e-02,\n",
      "         2.9499e-02,  8.4409e-03, -2.3346e-03, -2.4521e-02, -1.7434e-02,\n",
      "        -1.4484e-02, -3.0119e-03, -2.5416e-02,  3.6909e-02, -2.7169e-02,\n",
      "         2.3431e-02,  3.7313e-02, -3.4502e-02, -2.5467e-02,  3.0231e-02,\n",
      "         2.7142e-02,  1.4485e-02,  1.2127e-02, -2.5843e-02, -2.2134e-02,\n",
      "        -8.6641e-03, -2.5088e-02, -1.7523e-02,  1.7729e-02, -1.8295e-02,\n",
      "        -2.0717e-02, -8.6116e-03, -5.8242e-03, -3.3423e-02, -1.1817e-02,\n",
      "         1.1313e-02,  1.0824e-02,  5.2580e-03, -2.6815e-02, -1.0093e-02,\n",
      "        -2.6711e-02,  2.6179e-02, -6.4594e-03, -3.4464e-02,  1.8325e-02,\n",
      "         2.3164e-02, -6.2270e-03, -3.1191e-02,  1.0330e-03, -1.9309e-02,\n",
      "         3.2902e-02,  8.1725e-03, -2.2966e-02,  1.9716e-02, -1.9503e-02,\n",
      "        -6.0171e-03, -1.4511e-02,  8.9028e-03,  9.5911e-03, -1.8292e-03,\n",
      "         3.0663e-02, -3.4178e-02,  5.7057e-04,  3.8771e-02, -1.8479e-02,\n",
      "         3.4330e-02,  2.3946e-02,  8.3050e-04,  1.9484e-02, -8.1931e-03,\n",
      "        -3.3266e-02, -1.0717e-02,  5.6859e-04, -2.4717e-02,  1.9662e-03,\n",
      "         1.2300e-02, -2.9758e-02,  3.0717e-02,  2.6282e-02,  1.2718e-02,\n",
      "         1.0576e-02, -2.2381e-02, -2.4229e-02,  2.9239e-02,  2.6334e-02,\n",
      "        -1.5206e-02,  1.7100e-02,  2.3403e-02, -1.9824e-02,  3.6119e-02,\n",
      "         4.4201e-04, -3.5533e-02,  2.2680e-02,  2.7533e-02, -3.3967e-03,\n",
      "         2.5771e-02, -3.5155e-02,  3.1861e-02, -8.7233e-03,  7.3481e-03,\n",
      "         1.1107e-02,  1.1738e-02,  1.9623e-02,  2.5643e-02,  1.0969e-03,\n",
      "         1.5053e-02, -1.5042e-02, -1.1546e-02, -2.2745e-02,  1.9813e-02,\n",
      "         1.8933e-02, -7.8982e-03, -1.4043e-02,  1.3550e-02,  2.1923e-02,\n",
      "         6.7255e-03, -2.4073e-02,  8.4025e-04, -7.6685e-04,  2.0121e-02,\n",
      "        -2.6024e-02, -1.9165e-02, -2.7083e-02, -2.9680e-02, -1.8938e-02,\n",
      "        -9.3477e-03,  2.1571e-02, -2.2115e-02,  2.3682e-03, -1.6781e-02,\n",
      "         1.3151e-02, -4.5459e-03,  1.4507e-02, -2.2175e-02,  2.4840e-02,\n",
      "        -5.4514e-03, -8.4552e-03,  1.4514e-02,  3.9748e-03, -2.2848e-02,\n",
      "         2.6215e-02,  9.4901e-03, -2.9091e-02, -1.4925e-02,  2.9573e-02,\n",
      "        -2.5708e-02,  1.2723e-03, -7.8757e-03, -6.0696e-03,  2.9417e-02,\n",
      "         2.8985e-02,  3.3609e-02,  2.7832e-02, -2.6490e-02,  1.9336e-04,\n",
      "         2.5887e-02, -9.0951e-03,  2.3175e-02, -1.9772e-02,  1.2605e-02,\n",
      "         9.7851e-03,  4.5328e-03, -2.4626e-02, -1.9742e-02,  1.6117e-02,\n",
      "        -1.6734e-03,  1.6714e-02, -2.9666e-02,  4.4479e-03,  2.4824e-02,\n",
      "         3.3062e-02,  1.6553e-02, -3.1407e-02,  2.2001e-02,  2.4587e-02,\n",
      "         2.8013e-02, -1.2275e-02,  3.1412e-02,  3.0962e-02,  6.6461e-03,\n",
      "         2.7241e-02,  3.4541e-02,  3.1450e-02, -7.4884e-03, -1.0530e-02,\n",
      "        -1.8937e-02, -3.6397e-02,  7.3016e-03,  1.4303e-02,  5.7484e-03,\n",
      "         6.3735e-03,  4.3044e-03,  2.5992e-02,  3.5535e-02, -2.3468e-02,\n",
      "         2.5365e-02,  2.0096e-02, -1.8787e-02,  2.9071e-02,  2.4711e-02,\n",
      "        -1.1374e-02, -1.1154e-02,  3.5301e-02, -2.7225e-02, -2.6450e-02,\n",
      "         3.3726e-02,  2.7331e-02, -1.5758e-03,  2.3927e-02, -3.6855e-03,\n",
      "        -3.0086e-02, -3.6279e-02,  7.9514e-03, -7.3358e-03,  1.8188e-03,\n",
      "         2.9849e-02, -1.8658e-02,  1.8385e-02, -3.6310e-03,  1.9497e-02,\n",
      "        -3.4201e-02,  1.8168e-02,  2.3814e-02, -1.8024e-03, -2.7913e-02,\n",
      "        -8.9350e-03,  6.8467e-04,  2.2368e-02,  3.7986e-02, -2.3333e-02,\n",
      "         2.8480e-02,  1.8220e-02, -2.8134e-02,  1.9613e-02, -2.2383e-02,\n",
      "         1.1503e-02,  6.1684e-03,  3.3738e-03, -9.6270e-03,  1.6120e-02,\n",
      "         2.6226e-02, -9.9077e-03, -1.7724e-02, -2.9454e-02,  1.3932e-03,\n",
      "        -1.3887e-02,  1.5943e-02,  1.1614e-02,  9.1565e-03,  7.2113e-03,\n",
      "         1.6717e-03, -1.3762e-02,  8.4580e-03, -2.5285e-02,  1.1806e-02,\n",
      "         2.5510e-02,  2.4366e-02,  2.6949e-03,  2.3878e-03, -7.6139e-03,\n",
      "         1.2547e-02, -5.5733e-03,  2.9174e-02, -4.9431e-03,  1.0506e-02,\n",
      "         1.1099e-02, -6.6580e-03, -6.4435e-03,  2.6558e-02, -9.3545e-03,\n",
      "        -2.6006e-02,  5.4118e-03,  3.1941e-02, -2.3838e-02, -1.7013e-02,\n",
      "         3.2366e-03, -9.0452e-03,  2.9750e-02, -1.7972e-02, -8.8481e-03,\n",
      "        -7.7971e-03, -1.1839e-02,  2.9687e-03, -1.1443e-02, -1.5221e-02,\n",
      "        -2.8425e-02, -8.4375e-04,  2.4069e-02,  6.2831e-04, -3.1947e-02,\n",
      "         1.6461e-02,  3.2303e-02,  1.7827e-02,  2.6928e-02, -7.7527e-03,\n",
      "         1.8113e-02, -1.1482e-02,  2.2411e-02,  8.5711e-04,  3.1396e-02,\n",
      "         2.8838e-02, -1.6163e-03,  3.7234e-03, -2.6119e-02,  2.0666e-02,\n",
      "         5.2280e-03, -1.1339e-02, -1.2340e-02,  3.0266e-02, -1.7305e-02,\n",
      "         5.9205e-03,  1.0644e-02,  1.0383e-02,  2.6127e-02, -1.0497e-02,\n",
      "         9.2576e-03, -2.9837e-02,  1.2071e-02, -1.5795e-02,  1.5729e-02,\n",
      "        -1.0085e-02,  3.3863e-02, -2.6172e-02,  1.9252e-02, -4.1657e-03,\n",
      "        -4.0100e-03,  3.6943e-03,  2.7675e-02,  1.4267e-02, -1.3154e-03,\n",
      "        -3.4882e-03,  3.2052e-02,  2.7069e-02,  3.0545e-02, -2.6831e-02,\n",
      "         1.7170e-02, -1.8110e-02, -1.0425e-02,  2.0602e-02, -8.8435e-03,\n",
      "        -3.3571e-02, -9.2778e-03, -4.0249e-03,  3.8933e-03, -1.3121e-02,\n",
      "         1.6497e-02, -2.1263e-02, -2.5531e-02, -3.1670e-02, -1.4764e-02,\n",
      "        -3.4465e-02,  2.4809e-02, -2.9509e-02, -2.4197e-02, -2.7523e-02,\n",
      "         1.6063e-03, -9.8763e-04, -1.9620e-02,  8.1964e-03, -1.2228e-02,\n",
      "         3.0307e-02,  8.0454e-03, -2.6529e-02,  1.2214e-02, -7.9589e-03,\n",
      "         2.0067e-02, -2.2819e-02,  1.3029e-02, -8.0304e-04, -1.4441e-02,\n",
      "        -1.3490e-02, -2.9209e-02, -2.0299e-02,  2.4623e-02,  2.3315e-02,\n",
      "        -2.5496e-02,  2.4880e-02,  1.4925e-02, -2.4660e-02,  2.2002e-02,\n",
      "         3.3283e-02,  6.8761e-03,  5.6411e-03,  1.7977e-02,  1.6078e-02,\n",
      "        -2.6897e-02, -2.0477e-02, -3.3315e-02,  3.5213e-02,  3.4475e-02])\n",
      "fc2.weight \t torch.Size([10, 1000])\n",
      "fc2.weight \t tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.1054,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.1093,  0.0000,  ...,  0.0201,  0.0000, -0.7198]])\n",
      "fc2.bias \t torch.Size([10])\n",
      "fc2.bias \t tensor([-0.0495,  0.0138,  0.0125,  0.0118, -0.0201,  0.0038,  0.0102, -0.0494,\n",
      "         0.0655, -0.0048])\n"
     ]
    }
   ],
   "source": [
    "print(trained_nn)\n",
    "for param_tensor in trained_nn.state_dict():\n",
    "    print(param_tensor, \"\\t\", trained_nn.state_dict()[param_tensor].size())\n",
    "#     print(param_tensor, \"\\t\", len(trained_nn.state_dict()[param_tensor]))\n",
    "    print(param_tensor, \"\\t\", trained_nn.state_dict()[param_tensor])\n",
    "# print(len(trained_nn.state_dict())//2+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non zero entries after saparsifying=76174\n"
     ]
    }
   ],
   "source": [
    "number_non_zero=calNonZero(trained_nn, Layers)\n",
    "print(\"Number of non zero entries after saparsifying={}\".format(number_non_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09593702770780857\n"
     ]
    }
   ],
   "source": [
    "print(number_non_zero/794000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 1, 1000]         785,000\n",
      "            Linear-2                [-1, 1, 10]          10,010\n",
      "================================================================\n",
      "Total params: 795,010\n",
      "Trainable params: 795,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 3.03\n",
      "Estimated Total Size (MB): 3.04\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "summary(trained_nn, input_size=(1, 28* 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After sparsification Test set: Average loss: 0.0615, Accuracy: 2495/10000 (24%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "i=0\n",
    "for data, target in test_loader:\n",
    "#         print(data.shape, \" \",target.shape)\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "#         i+=1\n",
    "#     print(i)\n",
    "#         print(data.shape, \" \",target.shape)\n",
    "    net_out = trained_nn(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nAfter sparsification Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "[1] https://github.com/adventuresinML/adventures-in-ml-code/blob/master/pytorch_nn.py [28.08.2019]\n",
    "\n",
    "[2] https://towardsdatascience.com/model-summary-in-pytorch-b5a1e4b64d25 [02.09.2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
