{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(\"LeNET_300_100_MNIST_Model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 300]          77,100\n",
      "            Linear-6                  [-1, 100]          30,100\n",
      "            Linear-7                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 110,782\n",
      "Trainable params: 110,782\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 0.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device=torch.device(\"cpu\")\n",
    "model=Net().to(device)\n",
    "summary(model, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([6, 1, 5, 5])\n",
      "conv1.weight \t tensor([[[[-0.0192,  0.2511,  0.2183,  0.0210,  0.0190],\n",
      "          [-0.0522,  0.3645,  0.3561,  0.5513,  0.3440],\n",
      "          [ 0.0598,  0.5070,  0.6116,  0.6220,  0.2953],\n",
      "          [ 0.2985,  0.1301,  0.4567,  0.1638,  0.2149],\n",
      "          [-0.0246,  0.3206,  0.1249,  0.1288, -0.1632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1775, -0.0977,  0.0400,  0.2623,  0.2931],\n",
      "          [ 0.0318, -0.1988, -0.1956,  0.1583,  0.1678],\n",
      "          [-0.2439, -0.2983, -0.0792,  0.2235,  0.3461],\n",
      "          [-0.0280, -0.2769, -0.1121,  0.0926,  0.3560],\n",
      "          [-0.1970, -0.1645,  0.1938,  0.2115,  0.1824]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1362,  0.2118, -0.2154, -0.2076, -0.2334],\n",
      "          [ 0.2467, -0.0821, -0.1670, -0.0738,  0.2066],\n",
      "          [ 0.2564,  0.1952,  0.2048,  0.3864,  0.0963],\n",
      "          [ 0.0786,  0.3546,  0.3418, -0.0168,  0.1037],\n",
      "          [ 0.0178, -0.2039, -0.1759, -0.0146, -0.0048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299,  0.1694, -0.0029, -0.1827, -0.0718],\n",
      "          [-0.1909, -0.1638,  0.1044, -0.2096,  0.0405],\n",
      "          [ 0.0873, -0.1777, -0.1820, -0.0258, -0.1887],\n",
      "          [ 0.0190, -0.1462, -0.0486,  0.0288, -0.2104],\n",
      "          [ 0.1727, -0.1630, -0.0454, -0.2031, -0.1915]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1378,  0.0087, -0.1978, -0.2727,  0.0345],\n",
      "          [ 0.2285, -0.0154, -0.1342, -0.2998, -0.0739],\n",
      "          [ 0.2801,  0.2191,  0.1079,  0.2947,  0.1320],\n",
      "          [ 0.3344,  0.4029,  0.3628,  0.5148,  0.0400],\n",
      "          [ 0.0682,  0.0300,  0.3382,  0.2907,  0.3091]]],\n",
      "\n",
      "\n",
      "        [[[-0.2584, -0.2308, -0.1856,  0.0330,  0.0465],\n",
      "          [-0.2978, -0.1939, -0.2057,  0.0400,  0.1692],\n",
      "          [ 0.0883, -0.1518,  0.0861, -0.1625,  0.0874],\n",
      "          [ 0.1464,  0.2543,  0.0292,  0.0581, -0.0674],\n",
      "          [ 0.3818,  0.3657,  0.2362, -0.0540,  0.1097]]]])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv1.bias \t tensor([ 0.2434, -0.0632,  0.2198,  0.1060,  0.1912,  0.0086])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.weight \t tensor([[[[-0.0109, -0.0548, -0.0268,  0.0555,  0.0529],\n",
      "          [-0.0747, -0.0093, -0.0871,  0.0756,  0.0290],\n",
      "          [ 0.0085, -0.0744, -0.1693, -0.1346, -0.1455],\n",
      "          [ 0.0382, -0.0446, -0.1455, -0.0654, -0.0830],\n",
      "          [ 0.0522,  0.1614,  0.1331,  0.1559,  0.0276]],\n",
      "\n",
      "         [[-0.0135,  0.0531,  0.0238,  0.0102,  0.0312],\n",
      "          [ 0.0628, -0.0327,  0.0206, -0.0319,  0.0428],\n",
      "          [-0.0834, -0.1235, -0.0557,  0.0262, -0.0169],\n",
      "          [ 0.0244, -0.0334,  0.0382, -0.0140,  0.0345],\n",
      "          [ 0.1239, -0.0369, -0.0981, -0.0913, -0.0969]],\n",
      "\n",
      "         [[-0.0242, -0.0831,  0.0583,  0.0632, -0.0662],\n",
      "          [-0.0261, -0.0701,  0.0334, -0.0154,  0.0068],\n",
      "          [-0.0244,  0.0824, -0.1057, -0.1036,  0.0335],\n",
      "          [-0.0140,  0.1386,  0.0705,  0.0171, -0.0072],\n",
      "          [ 0.0018,  0.1739,  0.1223,  0.0542,  0.0006]],\n",
      "\n",
      "         [[ 0.0685,  0.0419, -0.0685, -0.0128, -0.0514],\n",
      "          [ 0.0531,  0.0907, -0.0022,  0.0279,  0.0706],\n",
      "          [ 0.0644,  0.0581,  0.0447,  0.0862, -0.0299],\n",
      "          [-0.0563, -0.0664,  0.0204, -0.0461, -0.0049],\n",
      "          [-0.0201, -0.0669, -0.0425, -0.0638,  0.0590]],\n",
      "\n",
      "         [[-0.0472,  0.0201, -0.0225, -0.0141, -0.0058],\n",
      "          [ 0.0624,  0.0067,  0.0042,  0.0046,  0.0419],\n",
      "          [ 0.0394,  0.0676, -0.0732, -0.1638, -0.0343],\n",
      "          [ 0.1362,  0.1999,  0.0978,  0.1239,  0.0379],\n",
      "          [ 0.0652,  0.1395,  0.1929,  0.1191,  0.0623]],\n",
      "\n",
      "         [[ 0.0314, -0.0123, -0.0633,  0.0803,  0.0407],\n",
      "          [-0.0420,  0.0859,  0.0217, -0.0667,  0.0375],\n",
      "          [ 0.0298,  0.0806,  0.0096,  0.0557, -0.0537],\n",
      "          [ 0.0555, -0.0591,  0.0920,  0.1185,  0.0150],\n",
      "          [-0.0733, -0.1120, -0.0059,  0.1112,  0.0869]]],\n",
      "\n",
      "\n",
      "        [[[-0.0222, -0.0632,  0.0230,  0.0518,  0.0471],\n",
      "          [-0.0131, -0.0666, -0.0177, -0.0039,  0.0601],\n",
      "          [ 0.0307,  0.0119,  0.1052, -0.0188,  0.0316],\n",
      "          [-0.0572, -0.0849, -0.0043,  0.1368,  0.0699],\n",
      "          [-0.0658, -0.1288, -0.1282, -0.0940, -0.0316]],\n",
      "\n",
      "         [[ 0.0440,  0.0498,  0.0480,  0.0980,  0.0044],\n",
      "          [-0.0486,  0.0267,  0.0349, -0.0436,  0.0280],\n",
      "          [-0.0089,  0.0524,  0.0789, -0.0944, -0.0586],\n",
      "          [-0.0659,  0.0035,  0.0432, -0.0714, -0.0228],\n",
      "          [ 0.0241, -0.0759, -0.0205,  0.0458, -0.0653]],\n",
      "\n",
      "         [[-0.0281,  0.0291, -0.0404,  0.0430, -0.0491],\n",
      "          [ 0.0727, -0.0770, -0.0016,  0.0005,  0.0026],\n",
      "          [-0.0638,  0.0402,  0.0268,  0.0278, -0.0408],\n",
      "          [-0.0593, -0.0857, -0.0497,  0.1150,  0.1128],\n",
      "          [-0.0520, -0.0732, -0.0377, -0.0443, -0.0022]],\n",
      "\n",
      "         [[-0.0036, -0.0737,  0.0009,  0.0342,  0.0625],\n",
      "          [ 0.0713, -0.0301, -0.0545, -0.0360, -0.0155],\n",
      "          [ 0.0323,  0.0786, -0.0489, -0.0150, -0.0732],\n",
      "          [-0.0087,  0.0101,  0.0736, -0.0299,  0.0564],\n",
      "          [ 0.0291, -0.0514,  0.0165, -0.0648, -0.0488]],\n",
      "\n",
      "         [[-0.0781,  0.0070, -0.0439,  0.0522,  0.0101],\n",
      "          [-0.0655, -0.0328,  0.0275, -0.0029, -0.0499],\n",
      "          [-0.0462,  0.0460,  0.0361,  0.1371,  0.0840],\n",
      "          [ 0.0280, -0.0768,  0.0399,  0.0632,  0.1553],\n",
      "          [ 0.0167,  0.0032, -0.0908, -0.1111, -0.0763]],\n",
      "\n",
      "         [[ 0.0044,  0.0608, -0.0532, -0.0601,  0.0587],\n",
      "          [-0.0058, -0.0142, -0.0047, -0.0417, -0.0031],\n",
      "          [-0.0824, -0.0439, -0.0758, -0.0655, -0.0323],\n",
      "          [ 0.0141, -0.0282,  0.0304,  0.0188, -0.0538],\n",
      "          [-0.0402,  0.0069,  0.0732,  0.0188, -0.0666]]],\n",
      "\n",
      "\n",
      "        [[[-0.0290, -0.0331, -0.0362, -0.0128, -0.1624],\n",
      "          [ 0.0264,  0.0704,  0.0559,  0.0057, -0.1444],\n",
      "          [ 0.0334, -0.0197,  0.0786,  0.1296, -0.0665],\n",
      "          [-0.1325, -0.0002, -0.0102,  0.0673,  0.1488],\n",
      "          [ 0.0281,  0.0752,  0.1247,  0.0654,  0.0795]],\n",
      "\n",
      "         [[ 0.0432,  0.0453, -0.0163, -0.0221, -0.0871],\n",
      "          [ 0.0297,  0.0078, -0.0213, -0.0102,  0.0096],\n",
      "          [-0.0610,  0.0428,  0.1683,  0.1447,  0.1145],\n",
      "          [ 0.0129,  0.1065,  0.1422,  0.0659,  0.0251],\n",
      "          [-0.0014,  0.0767,  0.0355,  0.0775,  0.0778]],\n",
      "\n",
      "         [[-0.0408,  0.0344,  0.0030, -0.0213, -0.0086],\n",
      "          [ 0.0332,  0.0192,  0.0254,  0.0032,  0.0026],\n",
      "          [-0.0381, -0.0148,  0.0117,  0.0037,  0.0669],\n",
      "          [ 0.0096, -0.0044,  0.0353, -0.0138,  0.0783],\n",
      "          [ 0.0304,  0.0076,  0.0405,  0.0741,  0.0048]],\n",
      "\n",
      "         [[-0.0272,  0.0014, -0.0568,  0.0614,  0.0953],\n",
      "          [ 0.0689, -0.0018,  0.0250,  0.0122,  0.0488],\n",
      "          [ 0.0131, -0.0347,  0.0192,  0.0197, -0.0680],\n",
      "          [ 0.0553, -0.0440, -0.0922, -0.0444,  0.0591],\n",
      "          [ 0.0409, -0.0933,  0.0049, -0.0250,  0.0385]],\n",
      "\n",
      "         [[ 0.0040,  0.0977,  0.0278,  0.0440, -0.1256],\n",
      "          [ 0.0426,  0.0925,  0.1221,  0.1534, -0.0139],\n",
      "          [-0.0277,  0.0244, -0.0303,  0.1574,  0.0935],\n",
      "          [-0.0314,  0.0749,  0.0602,  0.0665,  0.0982],\n",
      "          [ 0.0176,  0.0285,  0.0090,  0.0930, -0.0757]],\n",
      "\n",
      "         [[-0.0138,  0.1097,  0.0643,  0.1291,  0.1020],\n",
      "          [ 0.0214,  0.0248, -0.0334,  0.1013,  0.0531],\n",
      "          [-0.0008, -0.0636,  0.0248, -0.0394, -0.0282],\n",
      "          [ 0.1024,  0.0172,  0.0456, -0.0545,  0.0182],\n",
      "          [-0.0197,  0.1342,  0.0421,  0.0906,  0.0427]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0875,  0.0322,  0.0375,  0.0408,  0.0054],\n",
      "          [-0.0460, -0.0633, -0.0649,  0.0760, -0.0567],\n",
      "          [ 0.0322, -0.0105, -0.0606, -0.0797, -0.0679],\n",
      "          [-0.0232, -0.0553, -0.0724,  0.0120, -0.0809],\n",
      "          [ 0.0408, -0.0291, -0.0568, -0.0558, -0.0341]],\n",
      "\n",
      "         [[-0.0568, -0.0631,  0.0572, -0.0491, -0.0536],\n",
      "          [-0.0138,  0.0440,  0.0787,  0.0738,  0.0220],\n",
      "          [-0.0163,  0.0168, -0.0073,  0.0200, -0.0652],\n",
      "          [ 0.0009,  0.0010, -0.0487, -0.0715,  0.0072],\n",
      "          [-0.0719, -0.0316, -0.0134, -0.0032,  0.0724]],\n",
      "\n",
      "         [[ 0.0582, -0.0085,  0.0132, -0.0703,  0.0159],\n",
      "          [ 0.0671, -0.0448,  0.0393, -0.0356,  0.0394],\n",
      "          [ 0.0450,  0.0304, -0.0662,  0.0135,  0.0431],\n",
      "          [-0.0309,  0.0268, -0.0411, -0.0587,  0.0662],\n",
      "          [ 0.0126,  0.0711, -0.0226, -0.0170,  0.0568]],\n",
      "\n",
      "         [[ 0.0282, -0.0763, -0.0282, -0.0218, -0.0370],\n",
      "          [ 0.0586, -0.0726, -0.0211, -0.0571, -0.0420],\n",
      "          [ 0.0335,  0.0127,  0.0681,  0.0412,  0.0192],\n",
      "          [-0.0766, -0.0016, -0.0073,  0.0633, -0.0200],\n",
      "          [-0.0027, -0.0093,  0.0077, -0.0436,  0.0106]],\n",
      "\n",
      "         [[-0.0392,  0.0598, -0.0669,  0.0659, -0.0636],\n",
      "          [-0.0772, -0.0788, -0.0499, -0.0290,  0.0244],\n",
      "          [-0.0204, -0.0283,  0.0083, -0.0568,  0.0490],\n",
      "          [ 0.0234, -0.0123, -0.0521,  0.0399, -0.0175],\n",
      "          [ 0.0072, -0.0435, -0.0295, -0.0564, -0.0306]],\n",
      "\n",
      "         [[ 0.0560, -0.0579, -0.0484,  0.0134,  0.0548],\n",
      "          [-0.0322,  0.0032,  0.0593, -0.0421, -0.0552],\n",
      "          [ 0.0751,  0.0288, -0.0811, -0.0192, -0.0289],\n",
      "          [-0.0103,  0.0337, -0.0256, -0.0466,  0.0394],\n",
      "          [ 0.0175,  0.0412, -0.0659, -0.0206,  0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340,  0.0957,  0.0862, -0.0030,  0.0326],\n",
      "          [ 0.0625,  0.1338,  0.0669, -0.0977,  0.0272],\n",
      "          [ 0.1258,  0.2758,  0.0864, -0.1109, -0.1041],\n",
      "          [ 0.1510,  0.1942,  0.0006, -0.1443, -0.1476],\n",
      "          [ 0.1227,  0.0747, -0.0599, -0.0265, -0.0703]],\n",
      "\n",
      "         [[ 0.0696,  0.0604,  0.0966,  0.0341, -0.0003],\n",
      "          [ 0.2600,  0.1970,  0.0654, -0.0413,  0.0700],\n",
      "          [ 0.2760,  0.1238,  0.0613,  0.0552, -0.0365],\n",
      "          [ 0.2264,  0.0426,  0.0530, -0.0057,  0.0397],\n",
      "          [ 0.0955,  0.0771,  0.0420, -0.0585, -0.0408]],\n",
      "\n",
      "         [[-0.0805, -0.0571,  0.0686,  0.0679,  0.0019],\n",
      "          [ 0.0002, -0.0732,  0.0858,  0.0114, -0.0765],\n",
      "          [-0.0982,  0.0699,  0.1299,  0.0603, -0.0301],\n",
      "          [-0.0205,  0.0617,  0.0674,  0.0328,  0.0286],\n",
      "          [-0.0963,  0.0384,  0.1049,  0.1011,  0.0627]],\n",
      "\n",
      "         [[ 0.0357,  0.0340,  0.0522,  0.0495,  0.0708],\n",
      "          [-0.0845, -0.0224, -0.0304, -0.0551, -0.0190],\n",
      "          [-0.0668,  0.0409, -0.0619, -0.0562,  0.0009],\n",
      "          [ 0.0180, -0.0264,  0.0483,  0.0653,  0.0173],\n",
      "          [ 0.0295,  0.0553,  0.0585, -0.0805,  0.0243]],\n",
      "\n",
      "         [[ 0.1076, -0.0023, -0.0163, -0.0140,  0.0564],\n",
      "          [ 0.0750,  0.0792, -0.0273,  0.0257, -0.0842],\n",
      "          [ 0.0222,  0.0533, -0.0139, -0.0419, -0.0244],\n",
      "          [ 0.0319,  0.0712,  0.0382,  0.0173,  0.0096],\n",
      "          [-0.0934,  0.0405,  0.1524,  0.0470,  0.0904]],\n",
      "\n",
      "         [[-0.0850,  0.0575,  0.0297,  0.0397,  0.0481],\n",
      "          [-0.0801, -0.0908, -0.0910, -0.0494,  0.0574],\n",
      "          [ 0.1137, -0.0567, -0.1489, -0.0140,  0.0657],\n",
      "          [ 0.0604, -0.0574, -0.0979,  0.0109,  0.0994],\n",
      "          [ 0.0521, -0.0905,  0.0108,  0.0659,  0.0968]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1651,  0.1899,  0.1213,  0.0844,  0.0381],\n",
      "          [ 0.0872,  0.0726,  0.1388,  0.1072,  0.0381],\n",
      "          [-0.0822, -0.0235, -0.0253,  0.0575,  0.0126],\n",
      "          [-0.1143, -0.0686,  0.0064,  0.0676,  0.0519],\n",
      "          [-0.0414, -0.0143, -0.0119,  0.0260,  0.0315]],\n",
      "\n",
      "         [[-0.0832, -0.0186,  0.0401, -0.0030,  0.0215],\n",
      "          [-0.0418,  0.0453,  0.0292,  0.0065,  0.0488],\n",
      "          [ 0.0127, -0.1047,  0.0074,  0.0770,  0.0699],\n",
      "          [-0.1040,  0.0111, -0.0547,  0.0259,  0.0403],\n",
      "          [-0.0534, -0.0387, -0.0023,  0.0301, -0.0905]],\n",
      "\n",
      "         [[ 0.0518,  0.1405,  0.1145,  0.0954, -0.0037],\n",
      "          [-0.0601,  0.0425, -0.0251,  0.0921, -0.0411],\n",
      "          [-0.0104, -0.0991, -0.0232, -0.0040,  0.0175],\n",
      "          [ 0.0217, -0.0569, -0.0210, -0.0877,  0.0530],\n",
      "          [ 0.0366, -0.0513,  0.0025, -0.0891,  0.0799]],\n",
      "\n",
      "         [[ 0.0528,  0.0358, -0.0843,  0.0293,  0.0443],\n",
      "          [ 0.0218,  0.0180, -0.0699, -0.0587,  0.0115],\n",
      "          [ 0.0040,  0.0050,  0.0047,  0.0408,  0.0669],\n",
      "          [ 0.0741, -0.0620,  0.0226,  0.0472,  0.0682],\n",
      "          [ 0.0717,  0.0853, -0.0000, -0.0112, -0.0270]],\n",
      "\n",
      "         [[ 0.1038,  0.1426,  0.0998,  0.0320,  0.1111],\n",
      "          [-0.1177, -0.0672,  0.0677,  0.1034, -0.0451],\n",
      "          [-0.0871, -0.1239, -0.0099, -0.0290, -0.0613],\n",
      "          [-0.0878, -0.0596, -0.0144, -0.0210,  0.0641],\n",
      "          [-0.0289,  0.0519, -0.0234,  0.0174,  0.0056]],\n",
      "\n",
      "         [[-0.0465,  0.0461,  0.0285,  0.0367,  0.0337],\n",
      "          [ 0.0520, -0.0826, -0.0852, -0.0947, -0.1000],\n",
      "          [ 0.0594,  0.0626,  0.0371,  0.0097, -0.0623],\n",
      "          [ 0.0788, -0.0128,  0.0158, -0.0668, -0.0158],\n",
      "          [-0.0014,  0.0584, -0.0293, -0.0480, -0.0613]]]])\n",
      "conv2.bias \t torch.Size([16])\n",
      "conv2.bias \t tensor([ 0.0730,  0.0296,  0.0032,  0.0657,  0.0553,  0.0152, -0.0441, -0.0667,\n",
      "         0.0277,  0.0222, -0.0358,  0.0246,  0.0430, -0.0688, -0.0241,  0.0027])\n",
      "fc1.weight \t torch.Size([300, 256])\n",
      "fc1.weight \t tensor([[ 0.0272,  0.0675,  0.0587,  ...,  0.0256, -0.0110, -0.0366],\n",
      "        [ 0.0592,  0.0532,  0.0201,  ..., -0.0201, -0.0062,  0.0536],\n",
      "        [-0.0039, -0.0355, -0.0470,  ...,  0.0263, -0.0019, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0400,  0.0265, -0.0372,  ..., -0.0495,  0.0229,  0.0028],\n",
      "        [-0.0405,  0.0220, -0.0051,  ...,  0.0582, -0.0126, -0.0565],\n",
      "        [ 0.0613,  0.0368, -0.0311,  ..., -0.0378, -0.0014,  0.0345]])\n",
      "fc1.bias \t torch.Size([300])\n",
      "fc1.bias \t tensor([ 0.0125,  0.0496,  0.0088,  0.0085,  0.0095, -0.0318, -0.0080,  0.0076,\n",
      "         0.0534, -0.0552, -0.0182, -0.0311, -0.0108, -0.0078, -0.0341,  0.0239,\n",
      "        -0.0408, -0.0497, -0.0399, -0.0287,  0.0010, -0.0028,  0.0587,  0.0291,\n",
      "        -0.0397,  0.0408, -0.0342,  0.0057,  0.0011,  0.0218, -0.0139,  0.0099,\n",
      "        -0.0637, -0.0040,  0.0283, -0.0100, -0.0033,  0.0194,  0.0250,  0.0024,\n",
      "        -0.0618,  0.0368, -0.0075, -0.0286, -0.0415,  0.0529,  0.0308, -0.0437,\n",
      "         0.0435,  0.0390, -0.0456, -0.0025,  0.0445, -0.0515,  0.0301, -0.0621,\n",
      "        -0.0068,  0.0022, -0.0306,  0.0367, -0.0544, -0.0037, -0.0045,  0.0001,\n",
      "         0.0466,  0.0304,  0.0454,  0.0361,  0.0541, -0.0258, -0.0537, -0.0621,\n",
      "        -0.0620,  0.0021,  0.0354,  0.0299,  0.0376,  0.0373, -0.0519,  0.0416,\n",
      "         0.0261, -0.0177, -0.0317,  0.0305, -0.0425,  0.0313,  0.0406,  0.0041,\n",
      "         0.0194,  0.0046,  0.0000, -0.0381,  0.0463,  0.0631,  0.0508, -0.0166,\n",
      "        -0.0199, -0.0257, -0.0614, -0.0028,  0.0009, -0.0030, -0.0501, -0.0544,\n",
      "         0.0413, -0.0580,  0.0460,  0.0103,  0.0018,  0.0094,  0.0230,  0.0387,\n",
      "        -0.0001,  0.0430, -0.0575,  0.0499, -0.0169,  0.0482,  0.0148,  0.0434,\n",
      "         0.0406, -0.0329, -0.0347, -0.0067,  0.0056,  0.0384,  0.0232,  0.0367,\n",
      "        -0.0032,  0.0292, -0.0203, -0.0486,  0.0126, -0.0531,  0.0207,  0.0398,\n",
      "         0.0525, -0.0629,  0.0590, -0.0428, -0.0445, -0.0290, -0.0070, -0.0509,\n",
      "         0.0348,  0.0296, -0.0262, -0.0463,  0.0530, -0.0203,  0.0406, -0.0518,\n",
      "         0.0414,  0.0028,  0.0606,  0.0647, -0.0044,  0.0148, -0.0470,  0.0146,\n",
      "        -0.0530, -0.0190,  0.0125,  0.0080, -0.0501,  0.0331, -0.0305, -0.0425,\n",
      "         0.0532, -0.0100,  0.0302, -0.0257,  0.0368,  0.0359,  0.0030, -0.0122,\n",
      "        -0.0206, -0.0331,  0.0122, -0.0050,  0.0362, -0.0105,  0.0027, -0.0145,\n",
      "        -0.0539,  0.0082,  0.0559, -0.0274, -0.0482, -0.0504, -0.0238,  0.0578,\n",
      "         0.0111, -0.0489,  0.0005,  0.0606, -0.0305,  0.0339,  0.0619,  0.0450,\n",
      "        -0.0125,  0.0277, -0.0592, -0.0471, -0.0499, -0.0444, -0.0058,  0.0355,\n",
      "         0.0439, -0.0299, -0.0286, -0.0260, -0.0086,  0.0275,  0.0235,  0.0480,\n",
      "         0.0576, -0.0442, -0.0399,  0.0315,  0.0131, -0.0080,  0.0349,  0.0340,\n",
      "        -0.0147, -0.0120, -0.0187,  0.0264,  0.0293,  0.0585,  0.0324, -0.0166,\n",
      "        -0.0555,  0.0493, -0.0615, -0.0557,  0.0566, -0.0102,  0.0540, -0.0159,\n",
      "        -0.0254, -0.0540, -0.0359, -0.0339,  0.0491, -0.0252,  0.0568,  0.0199,\n",
      "        -0.0021, -0.0235, -0.0565,  0.0607,  0.0233, -0.0354,  0.0298,  0.0329,\n",
      "         0.0625, -0.0587,  0.0386, -0.0303,  0.0186, -0.0189,  0.0408, -0.0094,\n",
      "         0.0109, -0.0002, -0.0109, -0.0081,  0.0345, -0.0408, -0.0037, -0.0351,\n",
      "         0.0441,  0.0274, -0.0352,  0.0627, -0.0417,  0.0531, -0.0357,  0.0050,\n",
      "         0.0328, -0.0356,  0.0408, -0.0267,  0.0032, -0.0200,  0.0475, -0.0275,\n",
      "        -0.0526,  0.0087,  0.0170,  0.0063,  0.0208, -0.0253, -0.0119,  0.0515,\n",
      "        -0.0609, -0.0366,  0.0094,  0.0507])\n",
      "fc2.weight \t torch.Size([100, 300])\n",
      "fc2.weight \t tensor([[ 0.0402, -0.0434,  0.0285,  ..., -0.0308, -0.0236,  0.0146],\n",
      "        [ 0.0027,  0.0438, -0.0360,  ...,  0.0140,  0.0560,  0.0263],\n",
      "        [ 0.0155, -0.0348,  0.0268,  ...,  0.0200,  0.0367,  0.0531],\n",
      "        ...,\n",
      "        [ 0.0570, -0.0697,  0.0352,  ..., -0.0380, -0.0513, -0.0107],\n",
      "        [-0.0099,  0.0150,  0.0478,  ...,  0.0631, -0.0225,  0.0330],\n",
      "        [-0.0361,  0.0464, -0.0538,  ...,  0.0255, -0.0425, -0.0736]])\n",
      "fc2.bias \t torch.Size([100])\n",
      "fc2.bias \t tensor([ 0.0397, -0.0087, -0.0203,  0.0181,  0.0399, -0.0314, -0.0457,  0.0263,\n",
      "        -0.0335,  0.0092, -0.0015, -0.0373,  0.0526,  0.0600,  0.0099, -0.0135,\n",
      "         0.0540,  0.0312,  0.0291,  0.0140,  0.0042,  0.0081, -0.0266,  0.0049,\n",
      "         0.0078,  0.0362, -0.0094,  0.0693,  0.0406,  0.0248,  0.0119, -0.0411,\n",
      "         0.0599, -0.0495, -0.0417, -0.0585, -0.0123,  0.0157, -0.0193, -0.0215,\n",
      "         0.0462, -0.0107, -0.0284,  0.0102, -0.0230,  0.0080, -0.0122, -0.0338,\n",
      "         0.0525, -0.0359,  0.0041,  0.0639,  0.0477,  0.0078,  0.0393,  0.0211,\n",
      "        -0.0103, -0.0084, -0.0399, -0.0345, -0.0042, -0.0272,  0.0004,  0.0318,\n",
      "        -0.0283,  0.0067, -0.0566,  0.0388,  0.0067,  0.0128, -0.0366, -0.0135,\n",
      "        -0.0255, -0.0400,  0.0588, -0.0205,  0.0412,  0.0009,  0.0443,  0.0374,\n",
      "         0.0142,  0.0051,  0.0115,  0.0211,  0.0548,  0.0299,  0.0294, -0.0427,\n",
      "        -0.0211, -0.0392,  0.0204,  0.0366, -0.0392, -0.0465, -0.0501, -0.0336,\n",
      "        -0.0278,  0.0443,  0.0489, -0.0206])\n",
      "fc3.weight \t torch.Size([10, 100])\n",
      "fc3.weight \t tensor([[-0.0799, -0.0514,  0.0154,  0.0339, -0.0871, -0.0275,  0.0830, -0.0554,\n",
      "         -0.0946, -0.0866, -0.1241,  0.0442, -0.0847,  0.1099, -0.0807, -0.0870,\n",
      "         -0.0052,  0.0141, -0.1187,  0.0135, -0.0574,  0.0641,  0.0971,  0.1102,\n",
      "         -0.0481, -0.1465, -0.0263, -0.1137, -0.0913,  0.1041,  0.0754, -0.0701,\n",
      "         -0.0155,  0.1341, -0.0931, -0.0810,  0.0836,  0.0484,  0.1427, -0.1458,\n",
      "         -0.1428,  0.0642,  0.0834, -0.0122,  0.0128, -0.0866,  0.1009, -0.0585,\n",
      "          0.1132, -0.0641,  0.0977,  0.0277,  0.0624, -0.0949, -0.0213,  0.1714,\n",
      "         -0.1392, -0.0815, -0.0149,  0.1095, -0.0326, -0.0129,  0.1646,  0.0889,\n",
      "         -0.1588, -0.0397, -0.0864,  0.0607,  0.0431,  0.0588, -0.0666,  0.0588,\n",
      "          0.0749,  0.0891, -0.0775,  0.0067, -0.0723,  0.0007, -0.1620, -0.0965,\n",
      "         -0.1363,  0.0115,  0.0796,  0.0139,  0.0489, -0.0417, -0.0408,  0.1485,\n",
      "          0.0496,  0.1629, -0.0059, -0.0451, -0.0528,  0.1137, -0.0703,  0.1330,\n",
      "         -0.0167, -0.0833, -0.0043,  0.0967],\n",
      "        [ 0.0605, -0.0714, -0.1559, -0.0261, -0.0907, -0.0269,  0.0549, -0.1048,\n",
      "          0.1263, -0.1483,  0.1830, -0.1227,  0.1579,  0.0976, -0.0630, -0.0642,\n",
      "          0.0642,  0.0171,  0.0748, -0.0418, -0.0341,  0.1174,  0.1188, -0.1357,\n",
      "         -0.0569,  0.1670,  0.1514,  0.1882, -0.0166, -0.0848, -0.0153,  0.0541,\n",
      "          0.1883, -0.0972, -0.0972, -0.0510, -0.1072, -0.0235, -0.0642,  0.1352,\n",
      "          0.0150, -0.0173, -0.1752,  0.1126, -0.0674,  0.0020,  0.0317, -0.0967,\n",
      "          0.1348, -0.1442, -0.0774, -0.1404,  0.1202, -0.0748, -0.1379,  0.0576,\n",
      "          0.1114, -0.0876,  0.1180, -0.1181,  0.1069,  0.1197,  0.1634, -0.0131,\n",
      "          0.1384, -0.0552,  0.0004, -0.0252, -0.0005, -0.0646, -0.0370, -0.0633,\n",
      "         -0.0284,  0.0869, -0.1731,  0.1347, -0.1184, -0.1579, -0.0766,  0.0333,\n",
      "          0.0633, -0.0766, -0.0020,  0.0931, -0.0060, -0.0381,  0.0882,  0.0496,\n",
      "         -0.0383, -0.0410,  0.0548, -0.0701, -0.0373, -0.1803,  0.1029, -0.0843,\n",
      "         -0.0158, -0.1036,  0.0492, -0.0733],\n",
      "        [ 0.1163,  0.0529,  0.0872, -0.0351, -0.0968,  0.1147,  0.0832, -0.0014,\n",
      "         -0.0788,  0.0679,  0.0367,  0.0653,  0.2598, -0.0424, -0.0725,  0.0351,\n",
      "          0.0587,  0.0893, -0.1037, -0.0686, -0.0598, -0.0415,  0.1333,  0.0031,\n",
      "          0.0858, -0.0589, -0.0039, -0.0715,  0.0809, -0.0254,  0.0300, -0.1061,\n",
      "          0.1893,  0.0260, -0.0539,  0.0239,  0.2075, -0.0787, -0.1141,  0.0259,\n",
      "          0.0277, -0.0784, -0.0361, -0.0954,  0.1004, -0.0387, -0.0915, -0.1756,\n",
      "          0.0812,  0.0002,  0.0163,  0.1229,  0.0020, -0.0592,  0.0071,  0.0194,\n",
      "         -0.0185, -0.1658, -0.0266, -0.0476, -0.0040, -0.1298,  0.0797, -0.0132,\n",
      "         -0.0125, -0.0713,  0.0886,  0.0654, -0.1828,  0.1266, -0.0312, -0.1565,\n",
      "         -0.0008,  0.0433, -0.0532, -0.0615,  0.0067,  0.2707, -0.1773, -0.0664,\n",
      "          0.0780,  0.1225, -0.0906, -0.0346, -0.1364, -0.1227, -0.0573,  0.0918,\n",
      "         -0.1081,  0.1082, -0.0558, -0.0893,  0.0545, -0.0031,  0.0543, -0.1526,\n",
      "         -0.1887,  0.0908, -0.0388,  0.1185],\n",
      "        [-0.1174, -0.0956, -0.0778, -0.0839, -0.1300,  0.1048, -0.0927,  0.0063,\n",
      "         -0.0726, -0.1231,  0.0053,  0.0911, -0.0725, -0.1458, -0.0792,  0.0743,\n",
      "          0.0184,  0.0899,  0.2005, -0.1634, -0.0627, -0.0126, -0.0801, -0.0196,\n",
      "         -0.1274, -0.1908, -0.0943,  0.0627, -0.0222,  0.0798,  0.0219, -0.1379,\n",
      "          0.1264,  0.0201,  0.1231, -0.0776, -0.0175,  0.0741,  0.2067,  0.1731,\n",
      "          0.1828,  0.0337, -0.0757,  0.1103,  0.0470, -0.0247,  0.0363, -0.1289,\n",
      "         -0.1138,  0.1339,  0.0701,  0.0968, -0.1750, -0.0275, -0.1629, -0.1297,\n",
      "          0.1120, -0.0651, -0.0119, -0.0819, -0.0517,  0.1247, -0.0554,  0.0212,\n",
      "         -0.1113,  0.0962, -0.1201, -0.0136,  0.1305, -0.0559, -0.1068, -0.1020,\n",
      "         -0.0739,  0.0870,  0.1114, -0.0510, -0.0069,  0.1589, -0.0137, -0.0609,\n",
      "          0.0888,  0.0635, -0.0432, -0.0050, -0.0781,  0.0995, -0.0672,  0.0887,\n",
      "         -0.0514, -0.0577, -0.1168,  0.0129, -0.0206,  0.0918,  0.0148, -0.0447,\n",
      "          0.0651,  0.0980, -0.0730,  0.1064],\n",
      "        [ 0.1344, -0.0527, -0.1721,  0.0228,  0.0103, -0.0212,  0.0978, -0.0764,\n",
      "         -0.0590,  0.0773,  0.0813,  0.1136,  0.0045, -0.0662,  0.0648, -0.1574,\n",
      "          0.0122, -0.0855, -0.0298, -0.1251,  0.0770,  0.1594,  0.1165, -0.0861,\n",
      "         -0.1141, -0.0640, -0.1431, -0.0167,  0.0222, -0.0586, -0.1808,  0.0198,\n",
      "         -0.1278,  0.0701, -0.1002,  0.0631, -0.0676, -0.0541,  0.0505, -0.0883,\n",
      "         -0.0518,  0.0285,  0.0013, -0.1970,  0.0006,  0.0546, -0.0142,  0.0711,\n",
      "         -0.0467, -0.0935, -0.0555, -0.0608,  0.0216,  0.0222,  0.0968, -0.0574,\n",
      "          0.0568,  0.0914,  0.1699,  0.1728,  0.2329, -0.0513,  0.0553,  0.0158,\n",
      "          0.0237, -0.1188,  0.1711, -0.0002,  0.0112,  0.0978,  0.0454,  0.0862,\n",
      "          0.0530,  0.0608,  0.0068, -0.1118,  0.0083, -0.1468,  0.1673,  0.0777,\n",
      "          0.0660,  0.0521,  0.0632, -0.0286,  0.1892, -0.1432,  0.0500,  0.0170,\n",
      "          0.0163,  0.0035, -0.1290,  0.0328, -0.0114, -0.1939,  0.1007,  0.0200,\n",
      "         -0.0886, -0.0748, -0.0414, -0.0459],\n",
      "        [-0.0702,  0.0972, -0.1089, -0.0164,  0.1510,  0.1673,  0.0355, -0.0335,\n",
      "          0.0098, -0.0335, -0.0313,  0.0474, -0.2167,  0.0931,  0.0591,  0.1774,\n",
      "          0.0661, -0.1510, -0.0437,  0.0292,  0.0459,  0.0118, -0.1072,  0.0775,\n",
      "          0.1941,  0.0081, -0.0666,  0.1146,  0.0869, -0.0447,  0.0142,  0.1406,\n",
      "         -0.0295,  0.0406,  0.1028,  0.1352,  0.0855, -0.0151, -0.0241, -0.0670,\n",
      "          0.0759,  0.0260, -0.1360,  0.1547, -0.0115, -0.0346,  0.1122,  0.0071,\n",
      "         -0.1181, -0.0383, -0.0050,  0.0827,  0.0534,  0.0518, -0.1492,  0.0719,\n",
      "         -0.0114,  0.1788, -0.0322,  0.0634, -0.1965,  0.0355, -0.1877,  0.0430,\n",
      "         -0.0153,  0.1204, -0.0848, -0.0072, -0.0121,  0.0730, -0.0186,  0.0754,\n",
      "          0.0164,  0.0924,  0.1594, -0.0360,  0.0770, -0.0528,  0.0936, -0.0763,\n",
      "         -0.0707, -0.0935, -0.0783, -0.0139, -0.0352,  0.0623, -0.0253, -0.1390,\n",
      "          0.0381, -0.1633,  0.1316, -0.1226, -0.0330,  0.1635, -0.1238, -0.0575,\n",
      "          0.2134, -0.1288, -0.0973, -0.1462],\n",
      "        [-0.1267,  0.0608,  0.0945, -0.0855,  0.0137, -0.1502, -0.0231, -0.0381,\n",
      "          0.0615,  0.1791,  0.0682, -0.0547, -0.1165,  0.0160, -0.0677,  0.1215,\n",
      "          0.1270,  0.0552,  0.0252,  0.0310, -0.0521,  0.0416, -0.0192, -0.0318,\n",
      "          0.1169, -0.0378, -0.0670, -0.1821,  0.0983, -0.0484, -0.0966,  0.0173,\n",
      "         -0.1199, -0.0362, -0.0371, -0.0377, -0.0871, -0.0873, -0.0467, -0.1680,\n",
      "         -0.0357,  0.0793, -0.1178, -0.0236,  0.0737, -0.0153, -0.0381, -0.0225,\n",
      "          0.0352, -0.2038,  0.0576,  0.0369, -0.0268,  0.0086,  0.0528,  0.2330,\n",
      "         -0.0982,  0.0023, -0.0252,  0.0119, -0.1204, -0.0712, -0.1209,  0.0804,\n",
      "          0.0809,  0.0293,  0.0905, -0.0259, -0.0375, -0.0021,  0.1119,  0.0360,\n",
      "         -0.0401,  0.0578, -0.1174, -0.0666, -0.1238, -0.0625,  0.0602, -0.0826,\n",
      "         -0.0951, -0.1556, -0.0814,  0.0590, -0.0276,  0.1907,  0.1193,  0.1279,\n",
      "          0.0191,  0.0608, -0.0323, -0.2167, -0.0499, -0.0226, -0.0062,  0.2766,\n",
      "          0.0613,  0.0723, -0.1194,  0.0362],\n",
      "        [ 0.1253, -0.0567, -0.0470,  0.0057, -0.0911,  0.1846, -0.0189,  0.0406,\n",
      "          0.0422, -0.0780, -0.0071, -0.2156, -0.0201, -0.0144, -0.0221, -0.0102,\n",
      "         -0.0019, -0.1399,  0.1202, -0.0799, -0.0560, -0.0376,  0.0086, -0.1423,\n",
      "         -0.1784,  0.1010,  0.0851,  0.0469, -0.0839, -0.0062, -0.1095,  0.1140,\n",
      "          0.0282,  0.0649, -0.0995, -0.0791,  0.1417, -0.0001, -0.0900,  0.0731,\n",
      "         -0.1580,  0.0366,  0.1625, -0.1173,  0.0578,  0.0885,  0.0795, -0.0279,\n",
      "          0.0966,  0.2265, -0.0082, -0.0968,  0.1544, -0.0177, -0.0833, -0.0294,\n",
      "          0.0502, -0.1520,  0.1117,  0.0786, -0.0282, -0.1254, -0.0582,  0.0808,\n",
      "         -0.1701, -0.0076, -0.1343, -0.0174, -0.1298, -0.0760, -0.0093, -0.0526,\n",
      "          0.0641,  0.0873,  0.0920,  0.0170,  0.0591, -0.0053,  0.0125,  0.0697,\n",
      "          0.0351, -0.1147, -0.1147,  0.0011, -0.0274, -0.1307, -0.1444,  0.0165,\n",
      "         -0.0701,  0.1172,  0.0456,  0.0969,  0.0502,  0.1024,  0.1399, -0.1740,\n",
      "         -0.1500,  0.0137, -0.0495,  0.0711],\n",
      "        [ 0.0179,  0.0685,  0.0873,  0.0371,  0.0590,  0.0197,  0.1005,  0.0666,\n",
      "         -0.1361,  0.1357, -0.0054, -0.0601,  0.1132, -0.1546, -0.0455, -0.0954,\n",
      "         -0.0512,  0.1773, -0.1578, -0.0489, -0.0059, -0.0846, -0.0196,  0.1207,\n",
      "          0.0869, -0.0223, -0.0498, -0.0834, -0.0735,  0.0632,  0.0749,  0.0071,\n",
      "          0.0989, -0.0011, -0.0934, -0.0903, -0.0573,  0.0495,  0.0562, -0.0934,\n",
      "         -0.0274,  0.0376,  0.2146,  0.0413, -0.0074, -0.0715,  0.0373,  0.1217,\n",
      "         -0.0909,  0.1196,  0.0167,  0.0224,  0.0212, -0.0289,  0.0734, -0.1166,\n",
      "          0.0324,  0.1181, -0.1991, -0.1180,  0.0931, -0.0650, -0.0690, -0.0341,\n",
      "          0.0157,  0.1252, -0.0985,  0.0414,  0.1322, -0.1907, -0.0653, -0.0911,\n",
      "         -0.0773,  0.0222, -0.1590,  0.0777, -0.0225,  0.1439,  0.0272,  0.0664,\n",
      "         -0.0481, -0.1818,  0.0939,  0.0641, -0.1797,  0.0371,  0.1511, -0.0627,\n",
      "          0.0379, -0.0997, -0.1519,  0.0007,  0.0910, -0.0139,  0.0055, -0.0742,\n",
      "         -0.0375,  0.1489, -0.0454, -0.1793],\n",
      "        [ 0.0146, -0.0842,  0.0056,  0.0951, -0.0240,  0.0560,  0.0690,  0.0636,\n",
      "         -0.0916,  0.0020, -0.0134,  0.0796, -0.0850, -0.0028,  0.0361, -0.0669,\n",
      "         -0.3067,  0.1429, -0.0458,  0.0598,  0.0754,  0.0671, -0.0477,  0.1118,\n",
      "         -0.0051, -0.0064, -0.0240,  0.1009,  0.0136, -0.0055,  0.0861,  0.0048,\n",
      "         -0.2677, -0.1203, -0.0678, -0.0073, -0.1324,  0.0625, -0.0334,  0.0507,\n",
      "          0.0945,  0.0802, -0.1084, -0.0306, -0.0056,  0.0800, -0.1123,  0.0358,\n",
      "         -0.0649,  0.1931,  0.0560,  0.1092, -0.1128, -0.0712,  0.1549, -0.1540,\n",
      "          0.0716,  0.1764,  0.0486, -0.0538,  0.0456,  0.1606, -0.0910, -0.0560,\n",
      "          0.0678,  0.1936,  0.0395, -0.0298, -0.0671,  0.0566, -0.1658,  0.0905,\n",
      "          0.0633, -0.0620,  0.1562,  0.0469, -0.1265, -0.2520,  0.0131, -0.0754,\n",
      "         -0.2041,  0.1002,  0.0500, -0.0082,  0.1458, -0.2233, -0.0389,  0.0779,\n",
      "          0.0896,  0.1174,  0.0230,  0.1004,  0.0039,  0.0297,  0.0805, -0.0051,\n",
      "          0.0482, -0.0716,  0.0410,  0.1311]])\n",
      "fc3.bias \t torch.Size([10])\n",
      "fc3.bias \t tensor([-0.0838,  0.0204, -0.0172,  0.0138,  0.0053, -0.0699,  0.0305,  0.0152,\n",
      "        -0.0514, -0.0417])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   torch.Size([6, 1, 5, 5]) \n",
      "  Parameter containing:\n",
      "tensor([[[[-0.0192,  0.2511,  0.2183,  0.0210,  0.0190],\n",
      "          [-0.0522,  0.3645,  0.3561,  0.5513,  0.3440],\n",
      "          [ 0.0598,  0.5070,  0.6116,  0.6220,  0.2953],\n",
      "          [ 0.2985,  0.1301,  0.4567,  0.1638,  0.2149],\n",
      "          [-0.0246,  0.3206,  0.1249,  0.1288, -0.1632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1775, -0.0977,  0.0400,  0.2623,  0.2931],\n",
      "          [ 0.0318, -0.1988, -0.1956,  0.1583,  0.1678],\n",
      "          [-0.2439, -0.2983, -0.0792,  0.2235,  0.3461],\n",
      "          [-0.0280, -0.2769, -0.1121,  0.0926,  0.3560],\n",
      "          [-0.1970, -0.1645,  0.1938,  0.2115,  0.1824]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1362,  0.2118, -0.2154, -0.2076, -0.2334],\n",
      "          [ 0.2467, -0.0821, -0.1670, -0.0738,  0.2066],\n",
      "          [ 0.2564,  0.1952,  0.2048,  0.3864,  0.0963],\n",
      "          [ 0.0786,  0.3546,  0.3418, -0.0168,  0.1037],\n",
      "          [ 0.0178, -0.2039, -0.1759, -0.0146, -0.0048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299,  0.1694, -0.0029, -0.1827, -0.0718],\n",
      "          [-0.1909, -0.1638,  0.1044, -0.2096,  0.0405],\n",
      "          [ 0.0873, -0.1777, -0.1820, -0.0258, -0.1887],\n",
      "          [ 0.0190, -0.1462, -0.0486,  0.0288, -0.2104],\n",
      "          [ 0.1727, -0.1630, -0.0454, -0.2031, -0.1915]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1378,  0.0087, -0.1978, -0.2727,  0.0345],\n",
      "          [ 0.2285, -0.0154, -0.1342, -0.2998, -0.0739],\n",
      "          [ 0.2801,  0.2191,  0.1079,  0.2947,  0.1320],\n",
      "          [ 0.3344,  0.4029,  0.3628,  0.5148,  0.0400],\n",
      "          [ 0.0682,  0.0300,  0.3382,  0.2907,  0.3091]]],\n",
      "\n",
      "\n",
      "        [[[-0.2584, -0.2308, -0.1856,  0.0330,  0.0465],\n",
      "          [-0.2978, -0.1939, -0.2057,  0.0400,  0.1692],\n",
      "          [ 0.0883, -0.1518,  0.0861, -0.1625,  0.0874],\n",
      "          [ 0.1464,  0.2543,  0.0292,  0.0581, -0.0674],\n",
      "          [ 0.3818,  0.3657,  0.2362, -0.0540,  0.1097]]]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "2   torch.Size([6]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.2434, -0.0632,  0.2198,  0.1060,  0.1912,  0.0086], requires_grad=True) \n",
      "\n",
      "\n",
      "3   torch.Size([16, 6, 5, 5]) \n",
      "  Parameter containing:\n",
      "tensor([[[[-0.0109, -0.0548, -0.0268,  0.0555,  0.0529],\n",
      "          [-0.0747, -0.0093, -0.0871,  0.0756,  0.0290],\n",
      "          [ 0.0085, -0.0744, -0.1693, -0.1346, -0.1455],\n",
      "          [ 0.0382, -0.0446, -0.1455, -0.0654, -0.0830],\n",
      "          [ 0.0522,  0.1614,  0.1331,  0.1559,  0.0276]],\n",
      "\n",
      "         [[-0.0135,  0.0531,  0.0238,  0.0102,  0.0312],\n",
      "          [ 0.0628, -0.0327,  0.0206, -0.0319,  0.0428],\n",
      "          [-0.0834, -0.1235, -0.0557,  0.0262, -0.0169],\n",
      "          [ 0.0244, -0.0334,  0.0382, -0.0140,  0.0345],\n",
      "          [ 0.1239, -0.0369, -0.0981, -0.0913, -0.0969]],\n",
      "\n",
      "         [[-0.0242, -0.0831,  0.0583,  0.0632, -0.0662],\n",
      "          [-0.0261, -0.0701,  0.0334, -0.0154,  0.0068],\n",
      "          [-0.0244,  0.0824, -0.1057, -0.1036,  0.0335],\n",
      "          [-0.0140,  0.1386,  0.0705,  0.0171, -0.0072],\n",
      "          [ 0.0018,  0.1739,  0.1223,  0.0542,  0.0006]],\n",
      "\n",
      "         [[ 0.0685,  0.0419, -0.0685, -0.0128, -0.0514],\n",
      "          [ 0.0531,  0.0907, -0.0022,  0.0279,  0.0706],\n",
      "          [ 0.0644,  0.0581,  0.0447,  0.0862, -0.0299],\n",
      "          [-0.0563, -0.0664,  0.0204, -0.0461, -0.0049],\n",
      "          [-0.0201, -0.0669, -0.0425, -0.0638,  0.0590]],\n",
      "\n",
      "         [[-0.0472,  0.0201, -0.0225, -0.0141, -0.0058],\n",
      "          [ 0.0624,  0.0067,  0.0042,  0.0046,  0.0419],\n",
      "          [ 0.0394,  0.0676, -0.0732, -0.1638, -0.0343],\n",
      "          [ 0.1362,  0.1999,  0.0978,  0.1239,  0.0379],\n",
      "          [ 0.0652,  0.1395,  0.1929,  0.1191,  0.0623]],\n",
      "\n",
      "         [[ 0.0314, -0.0123, -0.0633,  0.0803,  0.0407],\n",
      "          [-0.0420,  0.0859,  0.0217, -0.0667,  0.0375],\n",
      "          [ 0.0298,  0.0806,  0.0096,  0.0557, -0.0537],\n",
      "          [ 0.0555, -0.0591,  0.0920,  0.1185,  0.0150],\n",
      "          [-0.0733, -0.1120, -0.0059,  0.1112,  0.0869]]],\n",
      "\n",
      "\n",
      "        [[[-0.0222, -0.0632,  0.0230,  0.0518,  0.0471],\n",
      "          [-0.0131, -0.0666, -0.0177, -0.0039,  0.0601],\n",
      "          [ 0.0307,  0.0119,  0.1052, -0.0188,  0.0316],\n",
      "          [-0.0572, -0.0849, -0.0043,  0.1368,  0.0699],\n",
      "          [-0.0658, -0.1288, -0.1282, -0.0940, -0.0316]],\n",
      "\n",
      "         [[ 0.0440,  0.0498,  0.0480,  0.0980,  0.0044],\n",
      "          [-0.0486,  0.0267,  0.0349, -0.0436,  0.0280],\n",
      "          [-0.0089,  0.0524,  0.0789, -0.0944, -0.0586],\n",
      "          [-0.0659,  0.0035,  0.0432, -0.0714, -0.0228],\n",
      "          [ 0.0241, -0.0759, -0.0205,  0.0458, -0.0653]],\n",
      "\n",
      "         [[-0.0281,  0.0291, -0.0404,  0.0430, -0.0491],\n",
      "          [ 0.0727, -0.0770, -0.0016,  0.0005,  0.0026],\n",
      "          [-0.0638,  0.0402,  0.0268,  0.0278, -0.0408],\n",
      "          [-0.0593, -0.0857, -0.0497,  0.1150,  0.1128],\n",
      "          [-0.0520, -0.0732, -0.0377, -0.0443, -0.0022]],\n",
      "\n",
      "         [[-0.0036, -0.0737,  0.0009,  0.0342,  0.0625],\n",
      "          [ 0.0713, -0.0301, -0.0545, -0.0360, -0.0155],\n",
      "          [ 0.0323,  0.0786, -0.0489, -0.0150, -0.0732],\n",
      "          [-0.0087,  0.0101,  0.0736, -0.0299,  0.0564],\n",
      "          [ 0.0291, -0.0514,  0.0165, -0.0648, -0.0488]],\n",
      "\n",
      "         [[-0.0781,  0.0070, -0.0439,  0.0522,  0.0101],\n",
      "          [-0.0655, -0.0328,  0.0275, -0.0029, -0.0499],\n",
      "          [-0.0462,  0.0460,  0.0361,  0.1371,  0.0840],\n",
      "          [ 0.0280, -0.0768,  0.0399,  0.0632,  0.1553],\n",
      "          [ 0.0167,  0.0032, -0.0908, -0.1111, -0.0763]],\n",
      "\n",
      "         [[ 0.0044,  0.0608, -0.0532, -0.0601,  0.0587],\n",
      "          [-0.0058, -0.0142, -0.0047, -0.0417, -0.0031],\n",
      "          [-0.0824, -0.0439, -0.0758, -0.0655, -0.0323],\n",
      "          [ 0.0141, -0.0282,  0.0304,  0.0188, -0.0538],\n",
      "          [-0.0402,  0.0069,  0.0732,  0.0188, -0.0666]]],\n",
      "\n",
      "\n",
      "        [[[-0.0290, -0.0331, -0.0362, -0.0128, -0.1624],\n",
      "          [ 0.0264,  0.0704,  0.0559,  0.0057, -0.1444],\n",
      "          [ 0.0334, -0.0197,  0.0786,  0.1296, -0.0665],\n",
      "          [-0.1325, -0.0002, -0.0102,  0.0673,  0.1488],\n",
      "          [ 0.0281,  0.0752,  0.1247,  0.0654,  0.0795]],\n",
      "\n",
      "         [[ 0.0432,  0.0453, -0.0163, -0.0221, -0.0871],\n",
      "          [ 0.0297,  0.0078, -0.0213, -0.0102,  0.0096],\n",
      "          [-0.0610,  0.0428,  0.1683,  0.1447,  0.1145],\n",
      "          [ 0.0129,  0.1065,  0.1422,  0.0659,  0.0251],\n",
      "          [-0.0014,  0.0767,  0.0355,  0.0775,  0.0778]],\n",
      "\n",
      "         [[-0.0408,  0.0344,  0.0030, -0.0213, -0.0086],\n",
      "          [ 0.0332,  0.0192,  0.0254,  0.0032,  0.0026],\n",
      "          [-0.0381, -0.0148,  0.0117,  0.0037,  0.0669],\n",
      "          [ 0.0096, -0.0044,  0.0353, -0.0138,  0.0783],\n",
      "          [ 0.0304,  0.0076,  0.0405,  0.0741,  0.0048]],\n",
      "\n",
      "         [[-0.0272,  0.0014, -0.0568,  0.0614,  0.0953],\n",
      "          [ 0.0689, -0.0018,  0.0250,  0.0122,  0.0488],\n",
      "          [ 0.0131, -0.0347,  0.0192,  0.0197, -0.0680],\n",
      "          [ 0.0553, -0.0440, -0.0922, -0.0444,  0.0591],\n",
      "          [ 0.0409, -0.0933,  0.0049, -0.0250,  0.0385]],\n",
      "\n",
      "         [[ 0.0040,  0.0977,  0.0278,  0.0440, -0.1256],\n",
      "          [ 0.0426,  0.0925,  0.1221,  0.1534, -0.0139],\n",
      "          [-0.0277,  0.0244, -0.0303,  0.1574,  0.0935],\n",
      "          [-0.0314,  0.0749,  0.0602,  0.0665,  0.0982],\n",
      "          [ 0.0176,  0.0285,  0.0090,  0.0930, -0.0757]],\n",
      "\n",
      "         [[-0.0138,  0.1097,  0.0643,  0.1291,  0.1020],\n",
      "          [ 0.0214,  0.0248, -0.0334,  0.1013,  0.0531],\n",
      "          [-0.0008, -0.0636,  0.0248, -0.0394, -0.0282],\n",
      "          [ 0.1024,  0.0172,  0.0456, -0.0545,  0.0182],\n",
      "          [-0.0197,  0.1342,  0.0421,  0.0906,  0.0427]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0875,  0.0322,  0.0375,  0.0408,  0.0054],\n",
      "          [-0.0460, -0.0633, -0.0649,  0.0760, -0.0567],\n",
      "          [ 0.0322, -0.0105, -0.0606, -0.0797, -0.0679],\n",
      "          [-0.0232, -0.0553, -0.0724,  0.0120, -0.0809],\n",
      "          [ 0.0408, -0.0291, -0.0568, -0.0558, -0.0341]],\n",
      "\n",
      "         [[-0.0568, -0.0631,  0.0572, -0.0491, -0.0536],\n",
      "          [-0.0138,  0.0440,  0.0787,  0.0738,  0.0220],\n",
      "          [-0.0163,  0.0168, -0.0073,  0.0200, -0.0652],\n",
      "          [ 0.0009,  0.0010, -0.0487, -0.0715,  0.0072],\n",
      "          [-0.0719, -0.0316, -0.0134, -0.0032,  0.0724]],\n",
      "\n",
      "         [[ 0.0582, -0.0085,  0.0132, -0.0703,  0.0159],\n",
      "          [ 0.0671, -0.0448,  0.0393, -0.0356,  0.0394],\n",
      "          [ 0.0450,  0.0304, -0.0662,  0.0135,  0.0431],\n",
      "          [-0.0309,  0.0268, -0.0411, -0.0587,  0.0662],\n",
      "          [ 0.0126,  0.0711, -0.0226, -0.0170,  0.0568]],\n",
      "\n",
      "         [[ 0.0282, -0.0763, -0.0282, -0.0218, -0.0370],\n",
      "          [ 0.0586, -0.0726, -0.0211, -0.0571, -0.0420],\n",
      "          [ 0.0335,  0.0127,  0.0681,  0.0412,  0.0192],\n",
      "          [-0.0766, -0.0016, -0.0073,  0.0633, -0.0200],\n",
      "          [-0.0027, -0.0093,  0.0077, -0.0436,  0.0106]],\n",
      "\n",
      "         [[-0.0392,  0.0598, -0.0669,  0.0659, -0.0636],\n",
      "          [-0.0772, -0.0788, -0.0499, -0.0290,  0.0244],\n",
      "          [-0.0204, -0.0283,  0.0083, -0.0568,  0.0490],\n",
      "          [ 0.0234, -0.0123, -0.0521,  0.0399, -0.0175],\n",
      "          [ 0.0072, -0.0435, -0.0295, -0.0564, -0.0306]],\n",
      "\n",
      "         [[ 0.0560, -0.0579, -0.0484,  0.0134,  0.0548],\n",
      "          [-0.0322,  0.0032,  0.0593, -0.0421, -0.0552],\n",
      "          [ 0.0751,  0.0288, -0.0811, -0.0192, -0.0289],\n",
      "          [-0.0103,  0.0337, -0.0256, -0.0466,  0.0394],\n",
      "          [ 0.0175,  0.0412, -0.0659, -0.0206,  0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340,  0.0957,  0.0862, -0.0030,  0.0326],\n",
      "          [ 0.0625,  0.1338,  0.0669, -0.0977,  0.0272],\n",
      "          [ 0.1258,  0.2758,  0.0864, -0.1109, -0.1041],\n",
      "          [ 0.1510,  0.1942,  0.0006, -0.1443, -0.1476],\n",
      "          [ 0.1227,  0.0747, -0.0599, -0.0265, -0.0703]],\n",
      "\n",
      "         [[ 0.0696,  0.0604,  0.0966,  0.0341, -0.0003],\n",
      "          [ 0.2600,  0.1970,  0.0654, -0.0413,  0.0700],\n",
      "          [ 0.2760,  0.1238,  0.0613,  0.0552, -0.0365],\n",
      "          [ 0.2264,  0.0426,  0.0530, -0.0057,  0.0397],\n",
      "          [ 0.0955,  0.0771,  0.0420, -0.0585, -0.0408]],\n",
      "\n",
      "         [[-0.0805, -0.0571,  0.0686,  0.0679,  0.0019],\n",
      "          [ 0.0002, -0.0732,  0.0858,  0.0114, -0.0765],\n",
      "          [-0.0982,  0.0699,  0.1299,  0.0603, -0.0301],\n",
      "          [-0.0205,  0.0617,  0.0674,  0.0328,  0.0286],\n",
      "          [-0.0963,  0.0384,  0.1049,  0.1011,  0.0627]],\n",
      "\n",
      "         [[ 0.0357,  0.0340,  0.0522,  0.0495,  0.0708],\n",
      "          [-0.0845, -0.0224, -0.0304, -0.0551, -0.0190],\n",
      "          [-0.0668,  0.0409, -0.0619, -0.0562,  0.0009],\n",
      "          [ 0.0180, -0.0264,  0.0483,  0.0653,  0.0173],\n",
      "          [ 0.0295,  0.0553,  0.0585, -0.0805,  0.0243]],\n",
      "\n",
      "         [[ 0.1076, -0.0023, -0.0163, -0.0140,  0.0564],\n",
      "          [ 0.0750,  0.0792, -0.0273,  0.0257, -0.0842],\n",
      "          [ 0.0222,  0.0533, -0.0139, -0.0419, -0.0244],\n",
      "          [ 0.0319,  0.0712,  0.0382,  0.0173,  0.0096],\n",
      "          [-0.0934,  0.0405,  0.1524,  0.0470,  0.0904]],\n",
      "\n",
      "         [[-0.0850,  0.0575,  0.0297,  0.0397,  0.0481],\n",
      "          [-0.0801, -0.0908, -0.0910, -0.0494,  0.0574],\n",
      "          [ 0.1137, -0.0567, -0.1489, -0.0140,  0.0657],\n",
      "          [ 0.0604, -0.0574, -0.0979,  0.0109,  0.0994],\n",
      "          [ 0.0521, -0.0905,  0.0108,  0.0659,  0.0968]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1651,  0.1899,  0.1213,  0.0844,  0.0381],\n",
      "          [ 0.0872,  0.0726,  0.1388,  0.1072,  0.0381],\n",
      "          [-0.0822, -0.0235, -0.0253,  0.0575,  0.0126],\n",
      "          [-0.1143, -0.0686,  0.0064,  0.0676,  0.0519],\n",
      "          [-0.0414, -0.0143, -0.0119,  0.0260,  0.0315]],\n",
      "\n",
      "         [[-0.0832, -0.0186,  0.0401, -0.0030,  0.0215],\n",
      "          [-0.0418,  0.0453,  0.0292,  0.0065,  0.0488],\n",
      "          [ 0.0127, -0.1047,  0.0074,  0.0770,  0.0699],\n",
      "          [-0.1040,  0.0111, -0.0547,  0.0259,  0.0403],\n",
      "          [-0.0534, -0.0387, -0.0023,  0.0301, -0.0905]],\n",
      "\n",
      "         [[ 0.0518,  0.1405,  0.1145,  0.0954, -0.0037],\n",
      "          [-0.0601,  0.0425, -0.0251,  0.0921, -0.0411],\n",
      "          [-0.0104, -0.0991, -0.0232, -0.0040,  0.0175],\n",
      "          [ 0.0217, -0.0569, -0.0210, -0.0877,  0.0530],\n",
      "          [ 0.0366, -0.0513,  0.0025, -0.0891,  0.0799]],\n",
      "\n",
      "         [[ 0.0528,  0.0358, -0.0843,  0.0293,  0.0443],\n",
      "          [ 0.0218,  0.0180, -0.0699, -0.0587,  0.0115],\n",
      "          [ 0.0040,  0.0050,  0.0047,  0.0408,  0.0669],\n",
      "          [ 0.0741, -0.0620,  0.0226,  0.0472,  0.0682],\n",
      "          [ 0.0717,  0.0853, -0.0000, -0.0112, -0.0270]],\n",
      "\n",
      "         [[ 0.1038,  0.1426,  0.0998,  0.0320,  0.1111],\n",
      "          [-0.1177, -0.0672,  0.0677,  0.1034, -0.0451],\n",
      "          [-0.0871, -0.1239, -0.0099, -0.0290, -0.0613],\n",
      "          [-0.0878, -0.0596, -0.0144, -0.0210,  0.0641],\n",
      "          [-0.0289,  0.0519, -0.0234,  0.0174,  0.0056]],\n",
      "\n",
      "         [[-0.0465,  0.0461,  0.0285,  0.0367,  0.0337],\n",
      "          [ 0.0520, -0.0826, -0.0852, -0.0947, -0.1000],\n",
      "          [ 0.0594,  0.0626,  0.0371,  0.0097, -0.0623],\n",
      "          [ 0.0788, -0.0128,  0.0158, -0.0668, -0.0158],\n",
      "          [-0.0014,  0.0584, -0.0293, -0.0480, -0.0613]]]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "4   torch.Size([16]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0730,  0.0296,  0.0032,  0.0657,  0.0553,  0.0152, -0.0441, -0.0667,\n",
      "         0.0277,  0.0222, -0.0358,  0.0246,  0.0430, -0.0688, -0.0241,  0.0027],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "5   torch.Size([300, 256]) \n",
      "  Parameter containing:\n",
      "tensor([[ 0.0272,  0.0675,  0.0587,  ...,  0.0256, -0.0110, -0.0366],\n",
      "        [ 0.0592,  0.0532,  0.0201,  ..., -0.0201, -0.0062,  0.0536],\n",
      "        [-0.0039, -0.0355, -0.0470,  ...,  0.0263, -0.0019, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0400,  0.0265, -0.0372,  ..., -0.0495,  0.0229,  0.0028],\n",
      "        [-0.0405,  0.0220, -0.0051,  ...,  0.0582, -0.0126, -0.0565],\n",
      "        [ 0.0613,  0.0368, -0.0311,  ..., -0.0378, -0.0014,  0.0345]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "6   torch.Size([300]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0125,  0.0496,  0.0088,  0.0085,  0.0095, -0.0318, -0.0080,  0.0076,\n",
      "         0.0534, -0.0552, -0.0182, -0.0311, -0.0108, -0.0078, -0.0341,  0.0239,\n",
      "        -0.0408, -0.0497, -0.0399, -0.0287,  0.0010, -0.0028,  0.0587,  0.0291,\n",
      "        -0.0397,  0.0408, -0.0342,  0.0057,  0.0011,  0.0218, -0.0139,  0.0099,\n",
      "        -0.0637, -0.0040,  0.0283, -0.0100, -0.0033,  0.0194,  0.0250,  0.0024,\n",
      "        -0.0618,  0.0368, -0.0075, -0.0286, -0.0415,  0.0529,  0.0308, -0.0437,\n",
      "         0.0435,  0.0390, -0.0456, -0.0025,  0.0445, -0.0515,  0.0301, -0.0621,\n",
      "        -0.0068,  0.0022, -0.0306,  0.0367, -0.0544, -0.0037, -0.0045,  0.0001,\n",
      "         0.0466,  0.0304,  0.0454,  0.0361,  0.0541, -0.0258, -0.0537, -0.0621,\n",
      "        -0.0620,  0.0021,  0.0354,  0.0299,  0.0376,  0.0373, -0.0519,  0.0416,\n",
      "         0.0261, -0.0177, -0.0317,  0.0305, -0.0425,  0.0313,  0.0406,  0.0041,\n",
      "         0.0194,  0.0046,  0.0000, -0.0381,  0.0463,  0.0631,  0.0508, -0.0166,\n",
      "        -0.0199, -0.0257, -0.0614, -0.0028,  0.0009, -0.0030, -0.0501, -0.0544,\n",
      "         0.0413, -0.0580,  0.0460,  0.0103,  0.0018,  0.0094,  0.0230,  0.0387,\n",
      "        -0.0001,  0.0430, -0.0575,  0.0499, -0.0169,  0.0482,  0.0148,  0.0434,\n",
      "         0.0406, -0.0329, -0.0347, -0.0067,  0.0056,  0.0384,  0.0232,  0.0367,\n",
      "        -0.0032,  0.0292, -0.0203, -0.0486,  0.0126, -0.0531,  0.0207,  0.0398,\n",
      "         0.0525, -0.0629,  0.0590, -0.0428, -0.0445, -0.0290, -0.0070, -0.0509,\n",
      "         0.0348,  0.0296, -0.0262, -0.0463,  0.0530, -0.0203,  0.0406, -0.0518,\n",
      "         0.0414,  0.0028,  0.0606,  0.0647, -0.0044,  0.0148, -0.0470,  0.0146,\n",
      "        -0.0530, -0.0190,  0.0125,  0.0080, -0.0501,  0.0331, -0.0305, -0.0425,\n",
      "         0.0532, -0.0100,  0.0302, -0.0257,  0.0368,  0.0359,  0.0030, -0.0122,\n",
      "        -0.0206, -0.0331,  0.0122, -0.0050,  0.0362, -0.0105,  0.0027, -0.0145,\n",
      "        -0.0539,  0.0082,  0.0559, -0.0274, -0.0482, -0.0504, -0.0238,  0.0578,\n",
      "         0.0111, -0.0489,  0.0005,  0.0606, -0.0305,  0.0339,  0.0619,  0.0450,\n",
      "        -0.0125,  0.0277, -0.0592, -0.0471, -0.0499, -0.0444, -0.0058,  0.0355,\n",
      "         0.0439, -0.0299, -0.0286, -0.0260, -0.0086,  0.0275,  0.0235,  0.0480,\n",
      "         0.0576, -0.0442, -0.0399,  0.0315,  0.0131, -0.0080,  0.0349,  0.0340,\n",
      "        -0.0147, -0.0120, -0.0187,  0.0264,  0.0293,  0.0585,  0.0324, -0.0166,\n",
      "        -0.0555,  0.0493, -0.0615, -0.0557,  0.0566, -0.0102,  0.0540, -0.0159,\n",
      "        -0.0254, -0.0540, -0.0359, -0.0339,  0.0491, -0.0252,  0.0568,  0.0199,\n",
      "        -0.0021, -0.0235, -0.0565,  0.0607,  0.0233, -0.0354,  0.0298,  0.0329,\n",
      "         0.0625, -0.0587,  0.0386, -0.0303,  0.0186, -0.0189,  0.0408, -0.0094,\n",
      "         0.0109, -0.0002, -0.0109, -0.0081,  0.0345, -0.0408, -0.0037, -0.0351,\n",
      "         0.0441,  0.0274, -0.0352,  0.0627, -0.0417,  0.0531, -0.0357,  0.0050,\n",
      "         0.0328, -0.0356,  0.0408, -0.0267,  0.0032, -0.0200,  0.0475, -0.0275,\n",
      "        -0.0526,  0.0087,  0.0170,  0.0063,  0.0208, -0.0253, -0.0119,  0.0515,\n",
      "        -0.0609, -0.0366,  0.0094,  0.0507], requires_grad=True) \n",
      "\n",
      "\n",
      "7   torch.Size([100, 300]) \n",
      "  Parameter containing:\n",
      "tensor([[ 0.0402, -0.0434,  0.0285,  ..., -0.0308, -0.0236,  0.0146],\n",
      "        [ 0.0027,  0.0438, -0.0360,  ...,  0.0140,  0.0560,  0.0263],\n",
      "        [ 0.0155, -0.0348,  0.0268,  ...,  0.0200,  0.0367,  0.0531],\n",
      "        ...,\n",
      "        [ 0.0570, -0.0697,  0.0352,  ..., -0.0380, -0.0513, -0.0107],\n",
      "        [-0.0099,  0.0150,  0.0478,  ...,  0.0631, -0.0225,  0.0330],\n",
      "        [-0.0361,  0.0464, -0.0538,  ...,  0.0255, -0.0425, -0.0736]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "8   torch.Size([100]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0397, -0.0087, -0.0203,  0.0181,  0.0399, -0.0314, -0.0457,  0.0263,\n",
      "        -0.0335,  0.0092, -0.0015, -0.0373,  0.0526,  0.0600,  0.0099, -0.0135,\n",
      "         0.0540,  0.0312,  0.0291,  0.0140,  0.0042,  0.0081, -0.0266,  0.0049,\n",
      "         0.0078,  0.0362, -0.0094,  0.0693,  0.0406,  0.0248,  0.0119, -0.0411,\n",
      "         0.0599, -0.0495, -0.0417, -0.0585, -0.0123,  0.0157, -0.0193, -0.0215,\n",
      "         0.0462, -0.0107, -0.0284,  0.0102, -0.0230,  0.0080, -0.0122, -0.0338,\n",
      "         0.0525, -0.0359,  0.0041,  0.0639,  0.0477,  0.0078,  0.0393,  0.0211,\n",
      "        -0.0103, -0.0084, -0.0399, -0.0345, -0.0042, -0.0272,  0.0004,  0.0318,\n",
      "        -0.0283,  0.0067, -0.0566,  0.0388,  0.0067,  0.0128, -0.0366, -0.0135,\n",
      "        -0.0255, -0.0400,  0.0588, -0.0205,  0.0412,  0.0009,  0.0443,  0.0374,\n",
      "         0.0142,  0.0051,  0.0115,  0.0211,  0.0548,  0.0299,  0.0294, -0.0427,\n",
      "        -0.0211, -0.0392,  0.0204,  0.0366, -0.0392, -0.0465, -0.0501, -0.0336,\n",
      "        -0.0278,  0.0443,  0.0489, -0.0206], requires_grad=True) \n",
      "\n",
      "\n",
      "9   torch.Size([10, 100]) \n",
      "  Parameter containing:\n",
      "tensor([[-0.0799, -0.0514,  0.0154,  0.0339, -0.0871, -0.0275,  0.0830, -0.0554,\n",
      "         -0.0946, -0.0866, -0.1241,  0.0442, -0.0847,  0.1099, -0.0807, -0.0870,\n",
      "         -0.0052,  0.0141, -0.1187,  0.0135, -0.0574,  0.0641,  0.0971,  0.1102,\n",
      "         -0.0481, -0.1465, -0.0263, -0.1137, -0.0913,  0.1041,  0.0754, -0.0701,\n",
      "         -0.0155,  0.1341, -0.0931, -0.0810,  0.0836,  0.0484,  0.1427, -0.1458,\n",
      "         -0.1428,  0.0642,  0.0834, -0.0122,  0.0128, -0.0866,  0.1009, -0.0585,\n",
      "          0.1132, -0.0641,  0.0977,  0.0277,  0.0624, -0.0949, -0.0213,  0.1714,\n",
      "         -0.1392, -0.0815, -0.0149,  0.1095, -0.0326, -0.0129,  0.1646,  0.0889,\n",
      "         -0.1588, -0.0397, -0.0864,  0.0607,  0.0431,  0.0588, -0.0666,  0.0588,\n",
      "          0.0749,  0.0891, -0.0775,  0.0067, -0.0723,  0.0007, -0.1620, -0.0965,\n",
      "         -0.1363,  0.0115,  0.0796,  0.0139,  0.0489, -0.0417, -0.0408,  0.1485,\n",
      "          0.0496,  0.1629, -0.0059, -0.0451, -0.0528,  0.1137, -0.0703,  0.1330,\n",
      "         -0.0167, -0.0833, -0.0043,  0.0967],\n",
      "        [ 0.0605, -0.0714, -0.1559, -0.0261, -0.0907, -0.0269,  0.0549, -0.1048,\n",
      "          0.1263, -0.1483,  0.1830, -0.1227,  0.1579,  0.0976, -0.0630, -0.0642,\n",
      "          0.0642,  0.0171,  0.0748, -0.0418, -0.0341,  0.1174,  0.1188, -0.1357,\n",
      "         -0.0569,  0.1670,  0.1514,  0.1882, -0.0166, -0.0848, -0.0153,  0.0541,\n",
      "          0.1883, -0.0972, -0.0972, -0.0510, -0.1072, -0.0235, -0.0642,  0.1352,\n",
      "          0.0150, -0.0173, -0.1752,  0.1126, -0.0674,  0.0020,  0.0317, -0.0967,\n",
      "          0.1348, -0.1442, -0.0774, -0.1404,  0.1202, -0.0748, -0.1379,  0.0576,\n",
      "          0.1114, -0.0876,  0.1180, -0.1181,  0.1069,  0.1197,  0.1634, -0.0131,\n",
      "          0.1384, -0.0552,  0.0004, -0.0252, -0.0005, -0.0646, -0.0370, -0.0633,\n",
      "         -0.0284,  0.0869, -0.1731,  0.1347, -0.1184, -0.1579, -0.0766,  0.0333,\n",
      "          0.0633, -0.0766, -0.0020,  0.0931, -0.0060, -0.0381,  0.0882,  0.0496,\n",
      "         -0.0383, -0.0410,  0.0548, -0.0701, -0.0373, -0.1803,  0.1029, -0.0843,\n",
      "         -0.0158, -0.1036,  0.0492, -0.0733],\n",
      "        [ 0.1163,  0.0529,  0.0872, -0.0351, -0.0968,  0.1147,  0.0832, -0.0014,\n",
      "         -0.0788,  0.0679,  0.0367,  0.0653,  0.2598, -0.0424, -0.0725,  0.0351,\n",
      "          0.0587,  0.0893, -0.1037, -0.0686, -0.0598, -0.0415,  0.1333,  0.0031,\n",
      "          0.0858, -0.0589, -0.0039, -0.0715,  0.0809, -0.0254,  0.0300, -0.1061,\n",
      "          0.1893,  0.0260, -0.0539,  0.0239,  0.2075, -0.0787, -0.1141,  0.0259,\n",
      "          0.0277, -0.0784, -0.0361, -0.0954,  0.1004, -0.0387, -0.0915, -0.1756,\n",
      "          0.0812,  0.0002,  0.0163,  0.1229,  0.0020, -0.0592,  0.0071,  0.0194,\n",
      "         -0.0185, -0.1658, -0.0266, -0.0476, -0.0040, -0.1298,  0.0797, -0.0132,\n",
      "         -0.0125, -0.0713,  0.0886,  0.0654, -0.1828,  0.1266, -0.0312, -0.1565,\n",
      "         -0.0008,  0.0433, -0.0532, -0.0615,  0.0067,  0.2707, -0.1773, -0.0664,\n",
      "          0.0780,  0.1225, -0.0906, -0.0346, -0.1364, -0.1227, -0.0573,  0.0918,\n",
      "         -0.1081,  0.1082, -0.0558, -0.0893,  0.0545, -0.0031,  0.0543, -0.1526,\n",
      "         -0.1887,  0.0908, -0.0388,  0.1185],\n",
      "        [-0.1174, -0.0956, -0.0778, -0.0839, -0.1300,  0.1048, -0.0927,  0.0063,\n",
      "         -0.0726, -0.1231,  0.0053,  0.0911, -0.0725, -0.1458, -0.0792,  0.0743,\n",
      "          0.0184,  0.0899,  0.2005, -0.1634, -0.0627, -0.0126, -0.0801, -0.0196,\n",
      "         -0.1274, -0.1908, -0.0943,  0.0627, -0.0222,  0.0798,  0.0219, -0.1379,\n",
      "          0.1264,  0.0201,  0.1231, -0.0776, -0.0175,  0.0741,  0.2067,  0.1731,\n",
      "          0.1828,  0.0337, -0.0757,  0.1103,  0.0470, -0.0247,  0.0363, -0.1289,\n",
      "         -0.1138,  0.1339,  0.0701,  0.0968, -0.1750, -0.0275, -0.1629, -0.1297,\n",
      "          0.1120, -0.0651, -0.0119, -0.0819, -0.0517,  0.1247, -0.0554,  0.0212,\n",
      "         -0.1113,  0.0962, -0.1201, -0.0136,  0.1305, -0.0559, -0.1068, -0.1020,\n",
      "         -0.0739,  0.0870,  0.1114, -0.0510, -0.0069,  0.1589, -0.0137, -0.0609,\n",
      "          0.0888,  0.0635, -0.0432, -0.0050, -0.0781,  0.0995, -0.0672,  0.0887,\n",
      "         -0.0514, -0.0577, -0.1168,  0.0129, -0.0206,  0.0918,  0.0148, -0.0447,\n",
      "          0.0651,  0.0980, -0.0730,  0.1064],\n",
      "        [ 0.1344, -0.0527, -0.1721,  0.0228,  0.0103, -0.0212,  0.0978, -0.0764,\n",
      "         -0.0590,  0.0773,  0.0813,  0.1136,  0.0045, -0.0662,  0.0648, -0.1574,\n",
      "          0.0122, -0.0855, -0.0298, -0.1251,  0.0770,  0.1594,  0.1165, -0.0861,\n",
      "         -0.1141, -0.0640, -0.1431, -0.0167,  0.0222, -0.0586, -0.1808,  0.0198,\n",
      "         -0.1278,  0.0701, -0.1002,  0.0631, -0.0676, -0.0541,  0.0505, -0.0883,\n",
      "         -0.0518,  0.0285,  0.0013, -0.1970,  0.0006,  0.0546, -0.0142,  0.0711,\n",
      "         -0.0467, -0.0935, -0.0555, -0.0608,  0.0216,  0.0222,  0.0968, -0.0574,\n",
      "          0.0568,  0.0914,  0.1699,  0.1728,  0.2329, -0.0513,  0.0553,  0.0158,\n",
      "          0.0237, -0.1188,  0.1711, -0.0002,  0.0112,  0.0978,  0.0454,  0.0862,\n",
      "          0.0530,  0.0608,  0.0068, -0.1118,  0.0083, -0.1468,  0.1673,  0.0777,\n",
      "          0.0660,  0.0521,  0.0632, -0.0286,  0.1892, -0.1432,  0.0500,  0.0170,\n",
      "          0.0163,  0.0035, -0.1290,  0.0328, -0.0114, -0.1939,  0.1007,  0.0200,\n",
      "         -0.0886, -0.0748, -0.0414, -0.0459],\n",
      "        [-0.0702,  0.0972, -0.1089, -0.0164,  0.1510,  0.1673,  0.0355, -0.0335,\n",
      "          0.0098, -0.0335, -0.0313,  0.0474, -0.2167,  0.0931,  0.0591,  0.1774,\n",
      "          0.0661, -0.1510, -0.0437,  0.0292,  0.0459,  0.0118, -0.1072,  0.0775,\n",
      "          0.1941,  0.0081, -0.0666,  0.1146,  0.0869, -0.0447,  0.0142,  0.1406,\n",
      "         -0.0295,  0.0406,  0.1028,  0.1352,  0.0855, -0.0151, -0.0241, -0.0670,\n",
      "          0.0759,  0.0260, -0.1360,  0.1547, -0.0115, -0.0346,  0.1122,  0.0071,\n",
      "         -0.1181, -0.0383, -0.0050,  0.0827,  0.0534,  0.0518, -0.1492,  0.0719,\n",
      "         -0.0114,  0.1788, -0.0322,  0.0634, -0.1965,  0.0355, -0.1877,  0.0430,\n",
      "         -0.0153,  0.1204, -0.0848, -0.0072, -0.0121,  0.0730, -0.0186,  0.0754,\n",
      "          0.0164,  0.0924,  0.1594, -0.0360,  0.0770, -0.0528,  0.0936, -0.0763,\n",
      "         -0.0707, -0.0935, -0.0783, -0.0139, -0.0352,  0.0623, -0.0253, -0.1390,\n",
      "          0.0381, -0.1633,  0.1316, -0.1226, -0.0330,  0.1635, -0.1238, -0.0575,\n",
      "          0.2134, -0.1288, -0.0973, -0.1462],\n",
      "        [-0.1267,  0.0608,  0.0945, -0.0855,  0.0137, -0.1502, -0.0231, -0.0381,\n",
      "          0.0615,  0.1791,  0.0682, -0.0547, -0.1165,  0.0160, -0.0677,  0.1215,\n",
      "          0.1270,  0.0552,  0.0252,  0.0310, -0.0521,  0.0416, -0.0192, -0.0318,\n",
      "          0.1169, -0.0378, -0.0670, -0.1821,  0.0983, -0.0484, -0.0966,  0.0173,\n",
      "         -0.1199, -0.0362, -0.0371, -0.0377, -0.0871, -0.0873, -0.0467, -0.1680,\n",
      "         -0.0357,  0.0793, -0.1178, -0.0236,  0.0737, -0.0153, -0.0381, -0.0225,\n",
      "          0.0352, -0.2038,  0.0576,  0.0369, -0.0268,  0.0086,  0.0528,  0.2330,\n",
      "         -0.0982,  0.0023, -0.0252,  0.0119, -0.1204, -0.0712, -0.1209,  0.0804,\n",
      "          0.0809,  0.0293,  0.0905, -0.0259, -0.0375, -0.0021,  0.1119,  0.0360,\n",
      "         -0.0401,  0.0578, -0.1174, -0.0666, -0.1238, -0.0625,  0.0602, -0.0826,\n",
      "         -0.0951, -0.1556, -0.0814,  0.0590, -0.0276,  0.1907,  0.1193,  0.1279,\n",
      "          0.0191,  0.0608, -0.0323, -0.2167, -0.0499, -0.0226, -0.0062,  0.2766,\n",
      "          0.0613,  0.0723, -0.1194,  0.0362],\n",
      "        [ 0.1253, -0.0567, -0.0470,  0.0057, -0.0911,  0.1846, -0.0189,  0.0406,\n",
      "          0.0422, -0.0780, -0.0071, -0.2156, -0.0201, -0.0144, -0.0221, -0.0102,\n",
      "         -0.0019, -0.1399,  0.1202, -0.0799, -0.0560, -0.0376,  0.0086, -0.1423,\n",
      "         -0.1784,  0.1010,  0.0851,  0.0469, -0.0839, -0.0062, -0.1095,  0.1140,\n",
      "          0.0282,  0.0649, -0.0995, -0.0791,  0.1417, -0.0001, -0.0900,  0.0731,\n",
      "         -0.1580,  0.0366,  0.1625, -0.1173,  0.0578,  0.0885,  0.0795, -0.0279,\n",
      "          0.0966,  0.2265, -0.0082, -0.0968,  0.1544, -0.0177, -0.0833, -0.0294,\n",
      "          0.0502, -0.1520,  0.1117,  0.0786, -0.0282, -0.1254, -0.0582,  0.0808,\n",
      "         -0.1701, -0.0076, -0.1343, -0.0174, -0.1298, -0.0760, -0.0093, -0.0526,\n",
      "          0.0641,  0.0873,  0.0920,  0.0170,  0.0591, -0.0053,  0.0125,  0.0697,\n",
      "          0.0351, -0.1147, -0.1147,  0.0011, -0.0274, -0.1307, -0.1444,  0.0165,\n",
      "         -0.0701,  0.1172,  0.0456,  0.0969,  0.0502,  0.1024,  0.1399, -0.1740,\n",
      "         -0.1500,  0.0137, -0.0495,  0.0711],\n",
      "        [ 0.0179,  0.0685,  0.0873,  0.0371,  0.0590,  0.0197,  0.1005,  0.0666,\n",
      "         -0.1361,  0.1357, -0.0054, -0.0601,  0.1132, -0.1546, -0.0455, -0.0954,\n",
      "         -0.0512,  0.1773, -0.1578, -0.0489, -0.0059, -0.0846, -0.0196,  0.1207,\n",
      "          0.0869, -0.0223, -0.0498, -0.0834, -0.0735,  0.0632,  0.0749,  0.0071,\n",
      "          0.0989, -0.0011, -0.0934, -0.0903, -0.0573,  0.0495,  0.0562, -0.0934,\n",
      "         -0.0274,  0.0376,  0.2146,  0.0413, -0.0074, -0.0715,  0.0373,  0.1217,\n",
      "         -0.0909,  0.1196,  0.0167,  0.0224,  0.0212, -0.0289,  0.0734, -0.1166,\n",
      "          0.0324,  0.1181, -0.1991, -0.1180,  0.0931, -0.0650, -0.0690, -0.0341,\n",
      "          0.0157,  0.1252, -0.0985,  0.0414,  0.1322, -0.1907, -0.0653, -0.0911,\n",
      "         -0.0773,  0.0222, -0.1590,  0.0777, -0.0225,  0.1439,  0.0272,  0.0664,\n",
      "         -0.0481, -0.1818,  0.0939,  0.0641, -0.1797,  0.0371,  0.1511, -0.0627,\n",
      "          0.0379, -0.0997, -0.1519,  0.0007,  0.0910, -0.0139,  0.0055, -0.0742,\n",
      "         -0.0375,  0.1489, -0.0454, -0.1793],\n",
      "        [ 0.0146, -0.0842,  0.0056,  0.0951, -0.0240,  0.0560,  0.0690,  0.0636,\n",
      "         -0.0916,  0.0020, -0.0134,  0.0796, -0.0850, -0.0028,  0.0361, -0.0669,\n",
      "         -0.3067,  0.1429, -0.0458,  0.0598,  0.0754,  0.0671, -0.0477,  0.1118,\n",
      "         -0.0051, -0.0064, -0.0240,  0.1009,  0.0136, -0.0055,  0.0861,  0.0048,\n",
      "         -0.2677, -0.1203, -0.0678, -0.0073, -0.1324,  0.0625, -0.0334,  0.0507,\n",
      "          0.0945,  0.0802, -0.1084, -0.0306, -0.0056,  0.0800, -0.1123,  0.0358,\n",
      "         -0.0649,  0.1931,  0.0560,  0.1092, -0.1128, -0.0712,  0.1549, -0.1540,\n",
      "          0.0716,  0.1764,  0.0486, -0.0538,  0.0456,  0.1606, -0.0910, -0.0560,\n",
      "          0.0678,  0.1936,  0.0395, -0.0298, -0.0671,  0.0566, -0.1658,  0.0905,\n",
      "          0.0633, -0.0620,  0.1562,  0.0469, -0.1265, -0.2520,  0.0131, -0.0754,\n",
      "         -0.2041,  0.1002,  0.0500, -0.0082,  0.1458, -0.2233, -0.0389,  0.0779,\n",
      "          0.0896,  0.1174,  0.0230,  0.1004,  0.0039,  0.0297,  0.0805, -0.0051,\n",
      "          0.0482, -0.0716,  0.0410,  0.1311]], requires_grad=True) \n",
      "\n",
      "\n",
      "10   torch.Size([10]) \n",
      "  Parameter containing:\n",
      "tensor([-0.0838,  0.0204, -0.0172,  0.0138,  0.0053, -0.0699,  0.0305,  0.0152,\n",
      "        -0.0514, -0.0417], requires_grad=True) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for parameter in net.parameters():\n",
    "    i+=1\n",
    "    print(i,\" \",parameter.shape,\"\\n \",parameter,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.110000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First we have to convert the nodes to points in some d dimenstion. Where d is the number of nodes in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 256])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.state_dict()[\"fc1.weight\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not prune the layer with 256 nodes. We have to prune the layer with 300 nodes. So, that depends on the outgoing edges weights means fc2. Because, if we consider the incoming edges two nodes with similar kind of incoming weights might not have a similar kind of effect on the next layer. But, two nodes having similar kind of outgoing edges will have same effect on the next layer. Example: if node1 has incoming edge i1, and outgoing edge o1. And node2 have incoming edge form same previous incoming node as i2, and same outgoing edge weight as o2. Then if we club them up, the net effect will be some function of f(i1+i2)* o1. assuming o1 and o2 are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 300])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print((net.state_dict()[\"fc2.weight\"]).shape)\n",
    "print(type(net.state_dict()[\"fc2.weight\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After changing the dimension of the layer with 300 nodes, to somehting less than 300. We also have to assing the incoming weight edges accordingly, and assign the outgoing edges equals to be mostly the avg of the nodes in same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first we need to form a space of dimension 100, with 300 points depending on fc2 weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0402,  0.0027,  0.0155,  ...,  0.0570, -0.0099, -0.0361],\n",
      "        [-0.0434,  0.0438, -0.0348,  ..., -0.0697,  0.0150,  0.0464],\n",
      "        [ 0.0285, -0.0360,  0.0268,  ...,  0.0352,  0.0478, -0.0538],\n",
      "        ...,\n",
      "        [-0.0308,  0.0140,  0.0200,  ..., -0.0380,  0.0631,  0.0255],\n",
      "        [-0.0236,  0.0560,  0.0367,  ..., -0.0513, -0.0225, -0.0425],\n",
      "        [ 0.0146,  0.0263,  0.0531,  ..., -0.0107,  0.0330, -0.0736]])\n",
      "torch.Size([300, 100])\n"
     ]
    }
   ],
   "source": [
    "mat=net.state_dict()[\"fc2.weight\"].t()\n",
    "print(mat)\n",
    "print(mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300   100\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "nodes=defaultdict(list)\n",
    "for j in range(len(net.state_dict()[\"fc2.weight\"][0])):\n",
    "    ele=[]\n",
    "    for i in range(len(net.state_dict()[\"fc2.weight\"])):\n",
    "        ele.append(net.state_dict()[\"fc2.weight\"][i][j].item())\n",
    "    nodes[j]=ele\n",
    "\n",
    "print(len(nodes), \" \", len(nodes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nodes[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have nodes as a dictionary, where key is the node id, and values are the outgoing edges. So, now we have to find such node_ids who are having almost similar outgoing edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "distances=[]\n",
    "for i in nodes.keys():\n",
    "    for j in nodes.keys():\n",
    "        if(i!=j and j>i):\n",
    "            distances.append(euclidean_distance(nodes[i], nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44850\n"
     ]
    }
   ],
   "source": [
    "distances.sort()\n",
    "print(len(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "# plt.plot([x for x in range(len(distances))], distances) \n",
    "# plt.ylabel(\"Distances\")\n",
    "# plt.show() \n",
    "x=[k for k in range(len(distances))]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, distances)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take ___ as cut off and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=defaultdict(bool)\n",
    "setpoints=defaultdict(set)\n",
    "for i in nodes.keys():\n",
    "    cluster[i]=False\n",
    "    setpoints[i]={i}\n",
    "    \n",
    "# print(cluster)\n",
    "\n",
    "for i in nodes.keys():\n",
    "    for j in nodes.keys():\n",
    "        if(i!=j and j>i and cluster[i]==False and cluster[j]==False):\n",
    "            \n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            #########   The percentage of reduction in size depends on the cutoff chossen here ######\n",
    "            #########  The higher cut off will result in higher compression  ########################\n",
    "            \n",
    "            cut_off=0.412\n",
    "            if(euclidean_distance(nodes[i], nodes[j])<cut_off):\n",
    "                setpoints[i].add(j)\n",
    "                del setpoints[j]\n",
    "                cluster[j]=True\n",
    "    cluster[i]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ->  {0}\n",
      "1  ->  {1}\n",
      "2  ->  {2}\n",
      "3  ->  {3, 197, 262, 204, 186}\n",
      "4  ->  {4}\n",
      "5  ->  {5}\n",
      "6  ->  {6}\n",
      "7  ->  {7}\n",
      "8  ->  {8, 22}\n",
      "9  ->  {9}\n",
      "10  ->  {16, 10}\n",
      "11  ->  {11}\n",
      "12  ->  {128, 250, 243, 12}\n",
      "13  ->  {13, 127}\n",
      "14  ->  {160, 175, 14, 143}\n",
      "15  ->  {15}\n",
      "17  ->  {17}\n",
      "18  ->  {18, 131, 54, 71}\n",
      "19  ->  {19, 221, 126, 271}\n",
      "20  ->  {83, 20}\n",
      "21  ->  {21}\n",
      "23  ->  {222, 23}\n",
      "24  ->  {24}\n",
      "25  ->  {25}\n",
      "26  ->  {265, 26}\n",
      "27  ->  {281, 106, 27, 85}\n",
      "28  ->  {28}\n",
      "29  ->  {29}\n",
      "30  ->  {30}\n",
      "31  ->  {31}\n",
      "32  ->  {32}\n",
      "33  ->  {33}\n",
      "34  ->  {34}\n",
      "35  ->  {35}\n",
      "36  ->  {36}\n",
      "37  ->  {37}\n",
      "38  ->  {38}\n",
      "39  ->  {39}\n",
      "40  ->  {40}\n",
      "41  ->  {41, 170}\n",
      "42  ->  {42}\n",
      "43  ->  {43}\n",
      "44  ->  {44}\n",
      "45  ->  {45}\n",
      "46  ->  {134, 46}\n",
      "47  ->  {163, 108, 47}\n",
      "48  ->  {48}\n",
      "49  ->  {49}\n",
      "50  ->  {81, 50}\n",
      "51  ->  {296, 51}\n",
      "52  ->  {65, 52}\n",
      "53  ->  {213, 53, 191}\n",
      "55  ->  {80, 91, 55}\n",
      "56  ->  {56}\n",
      "57  ->  {57}\n",
      "58  ->  {58}\n",
      "59  ->  {59}\n",
      "60  ->  {60, 92}\n",
      "61  ->  {61}\n",
      "62  ->  {62}\n",
      "63  ->  {63}\n",
      "64  ->  {64}\n",
      "66  ->  {66}\n",
      "67  ->  {67}\n",
      "68  ->  {68}\n",
      "69  ->  {69}\n",
      "70  ->  {70}\n",
      "72  ->  {72}\n",
      "73  ->  {73}\n",
      "74  ->  {74}\n",
      "75  ->  {75}\n",
      "76  ->  {226, 76}\n",
      "77  ->  {77}\n",
      "78  ->  {78}\n",
      "79  ->  {79}\n",
      "82  ->  {82, 291}\n",
      "84  ->  {84}\n",
      "86  ->  {86}\n",
      "87  ->  {202, 87}\n",
      "88  ->  {88}\n",
      "89  ->  {89, 162}\n",
      "90  ->  {90}\n",
      "93  ->  {93}\n",
      "94  ->  {94}\n",
      "95  ->  {95}\n",
      "96  ->  {96}\n",
      "97  ->  {97, 229}\n",
      "98  ->  {98}\n",
      "99  ->  {99}\n",
      "100  ->  {107, 100}\n",
      "101  ->  {101}\n",
      "102  ->  {102}\n",
      "103  ->  {219, 103}\n",
      "104  ->  {104}\n",
      "105  ->  {105}\n",
      "109  ->  {109}\n",
      "110  ->  {110}\n",
      "111  ->  {111}\n",
      "112  ->  {112}\n",
      "113  ->  {113}\n",
      "114  ->  {114}\n",
      "115  ->  {115}\n",
      "116  ->  {116}\n",
      "117  ->  {117}\n",
      "118  ->  {118}\n",
      "119  ->  {235, 119}\n",
      "120  ->  {120}\n",
      "121  ->  {121}\n",
      "122  ->  {122}\n",
      "123  ->  {123}\n",
      "124  ->  {124}\n",
      "125  ->  {298, 125}\n",
      "129  ->  {129}\n",
      "130  ->  {130}\n",
      "132  ->  {132}\n",
      "133  ->  {290, 133}\n",
      "135  ->  {135}\n",
      "136  ->  {136}\n",
      "137  ->  {264, 137, 182, 257}\n",
      "138  ->  {138}\n",
      "139  ->  {139}\n",
      "140  ->  {140}\n",
      "141  ->  {237, 141}\n",
      "142  ->  {142, 286}\n",
      "144  ->  {144}\n",
      "145  ->  {145}\n",
      "146  ->  {146}\n",
      "147  ->  {147, 255}\n",
      "148  ->  {148}\n",
      "149  ->  {149}\n",
      "150  ->  {150}\n",
      "151  ->  {151}\n",
      "152  ->  {152}\n",
      "153  ->  {153}\n",
      "154  ->  {194, 154}\n",
      "155  ->  {155}\n",
      "156  ->  {156}\n",
      "157  ->  {157, 247}\n",
      "158  ->  {158}\n",
      "159  ->  {159}\n",
      "161  ->  {161}\n",
      "164  ->  {164}\n",
      "165  ->  {165}\n",
      "166  ->  {166}\n",
      "167  ->  {293, 167}\n",
      "168  ->  {168}\n",
      "169  ->  {169}\n",
      "171  ->  {171}\n",
      "172  ->  {172}\n",
      "173  ->  {173}\n",
      "174  ->  {174}\n",
      "176  ->  {176}\n",
      "177  ->  {177}\n",
      "178  ->  {178, 282, 183}\n",
      "179  ->  {179}\n",
      "180  ->  {180}\n",
      "181  ->  {181}\n",
      "184  ->  {184}\n",
      "185  ->  {185}\n",
      "187  ->  {187}\n",
      "188  ->  {188}\n",
      "189  ->  {189}\n",
      "190  ->  {190}\n",
      "192  ->  {192}\n",
      "193  ->  {193}\n",
      "195  ->  {195}\n",
      "196  ->  {196}\n",
      "198  ->  {227, 198}\n",
      "199  ->  {199}\n",
      "200  ->  {200}\n",
      "201  ->  {201}\n",
      "203  ->  {203, 239}\n",
      "205  ->  {205}\n",
      "206  ->  {206}\n",
      "207  ->  {207}\n",
      "208  ->  {208}\n",
      "209  ->  {209}\n",
      "210  ->  {210}\n",
      "211  ->  {211}\n",
      "212  ->  {212}\n",
      "214  ->  {214}\n",
      "215  ->  {215}\n",
      "216  ->  {216}\n",
      "217  ->  {217}\n",
      "218  ->  {218}\n",
      "220  ->  {220}\n",
      "223  ->  {223}\n",
      "224  ->  {224}\n",
      "225  ->  {225}\n",
      "228  ->  {228}\n",
      "230  ->  {230}\n",
      "231  ->  {231}\n",
      "232  ->  {232}\n",
      "233  ->  {233}\n",
      "234  ->  {234}\n",
      "236  ->  {236}\n",
      "238  ->  {238}\n",
      "240  ->  {240}\n",
      "241  ->  {241}\n",
      "242  ->  {242}\n",
      "244  ->  {244}\n",
      "245  ->  {245}\n",
      "246  ->  {246}\n",
      "248  ->  {248}\n",
      "249  ->  {249}\n",
      "251  ->  {251}\n",
      "252  ->  {252}\n",
      "253  ->  {253}\n",
      "254  ->  {254}\n",
      "256  ->  {256}\n",
      "258  ->  {258}\n",
      "259  ->  {259}\n",
      "260  ->  {260}\n",
      "261  ->  {261}\n",
      "263  ->  {263}\n",
      "266  ->  {266}\n",
      "267  ->  {267}\n",
      "268  ->  {268}\n",
      "269  ->  {269}\n",
      "270  ->  {270}\n",
      "272  ->  {272}\n",
      "273  ->  {273}\n",
      "274  ->  {274}\n",
      "275  ->  {275}\n",
      "276  ->  {276}\n",
      "277  ->  {277}\n",
      "278  ->  {278}\n",
      "279  ->  {279}\n",
      "280  ->  {280}\n",
      "283  ->  {283}\n",
      "284  ->  {284}\n",
      "285  ->  {285}\n",
      "287  ->  {287}\n",
      "288  ->  {288}\n",
      "289  ->  {289}\n",
      "292  ->  {292}\n",
      "294  ->  {294}\n",
      "295  ->  {295}\n",
      "297  ->  {297}\n",
      "299  ->  {299}\n"
     ]
    }
   ],
   "source": [
    "for key in setpoints.keys():\n",
    "    print(key,\" -> \",setpoints[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "print(len(setpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "temp=[]\n",
    "for node in setpoints.keys():\n",
    "    row=torch.zeros(len(net.state_dict()[\"fc1.weight\"][0]), dtype=torch.float)\n",
    "    for val in setpoints[node]:\n",
    "        row+=net.state_dict()[\"fc1.weight\"][val]\n",
    "    temp.append(row)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have i many nodes in place of 300. Now, we will first fix the incoming weights this i many nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 256])\n",
      "torch.Size([240, 256])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc1.weight\"].shape)\n",
    "net.state_dict()[\"fc1.weight\"].resize_((len(temp), len(temp[0])))\n",
    "print(net.state_dict()[\"fc1.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp[0])):\n",
    "        net.state_dict()[\"fc1.weight\"][i][j]=temp[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 256])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc1.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "temp=[]\n",
    "for node in setpoints.keys():\n",
    "    ele=0\n",
    "    for val in setpoints[node]:\n",
    "        ele+=net.state_dict()[\"fc1.bias\"][val]\n",
    "    temp.append(ele)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc1.bias\"].shape)\n",
    "net.state_dict()[\"fc1.bias\"].resize_((len(temp)))\n",
    "print(net.state_dict()[\"fc1.bias\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(temp)):\n",
    "    net.state_dict()[\"fc1.bias\"][i]=temp[i]\n",
    "print(net.state_dict()[\"fc1.bias\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to change the outgoing weights. We can try out checking with the average of the same cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 300])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.weight\"].shape)\n",
    "temp=[]\n",
    "for i in range(len(net.state_dict()[\"fc2.weight\"])):\n",
    "    row=[]\n",
    "    for j in setpoints.keys():\n",
    "        ele=0\n",
    "        for val in setpoints[j]:\n",
    "            ele+=net.state_dict()[\"fc2.weight\"][i][val]\n",
    "        row.append(ele/len(setpoints[j]))\n",
    "    temp.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 240\n"
     ]
    }
   ],
   "source": [
    "print(len(temp), len(temp[0]))\n",
    "# print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 240])\n"
     ]
    }
   ],
   "source": [
    "net.state_dict()[\"fc2.weight\"].resize_(len(temp), len(temp[0]))\n",
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp[0])):\n",
    "        net.state_dict()[\"fc2.weight\"][i][j]=temp[i][j]\n",
    "print(net.state_dict()[\"fc2.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print((net.state_dict()[\"fc2.bias\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.690000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the accuracy did not decrease much, it came down to 82.5 from 89.7 even after dropping 114 nodes from layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to prune the next layer which consists of 100 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 240])"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.state_dict()[\"fc2.weight\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.state_dict()[\"fc3.weight\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   10\n"
     ]
    }
   ],
   "source": [
    "nodes=defaultdict(list)\n",
    "for j in range(len(net.state_dict()[\"fc3.weight\"][0])):\n",
    "    ele=[]\n",
    "    for i in range(len(net.state_dict()[\"fc3.weight\"])):\n",
    "        ele.append(net.state_dict()[\"fc3.weight\"][i][j].item())\n",
    "    nodes[j]=ele\n",
    "\n",
    "print(len(nodes), \" \", len(nodes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "distances=[]\n",
    "for i in nodes.keys():\n",
    "    for j in nodes.keys():\n",
    "        if(i!=j and j>i):\n",
    "            distances.append(euclidean_distance(nodes[i], nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n"
     ]
    }
   ],
   "source": [
    "distances.sort()\n",
    "print(len(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "x=[k for k in range(len(distances))]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, distances)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to set the cutoff as ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=defaultdict(bool)\n",
    "setpoints=defaultdict(set)\n",
    "for i in nodes.keys():\n",
    "    cluster[i]=False\n",
    "    setpoints[i]={i}\n",
    "    \n",
    "# print(cluster)\n",
    "\n",
    "for i in nodes.keys():\n",
    "    for j in nodes.keys():\n",
    "        if(i!=j and j>i and cluster[i]==False and cluster[j]==False):\n",
    "            \n",
    "            \n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            #########   The percentage of reduction in size depends on the cutoff chossen here ######\n",
    "            #########  The higher cut off will result in higher compression  ########################\n",
    "            \n",
    "            cut_off=0.18\n",
    "            \n",
    "            if(euclidean_distance(nodes[i], nodes[j])<cut_off):\n",
    "                setpoints[i].add(j)\n",
    "                del setpoints[j]\n",
    "                cluster[j]=True\n",
    "    cluster[i]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ->  {0}\n",
      "1  ->  {1}\n",
      "2  ->  {2}\n",
      "3  ->  {98, 67, 3, 72, 14, 82, 20, 88, 92}\n",
      "4  ->  {57, 4}\n",
      "5  ->  {5}\n",
      "6  ->  {6}\n",
      "7  ->  {7}\n",
      "8  ->  {8}\n",
      "9  ->  {9}\n",
      "10  ->  {10}\n",
      "11  ->  {11}\n",
      "12  ->  {12}\n",
      "13  ->  {90, 13}\n",
      "15  ->  {15}\n",
      "16  ->  {16, 28, 63}\n",
      "17  ->  {17}\n",
      "18  ->  {18}\n",
      "19  ->  {19}\n",
      "21  ->  {21}\n",
      "22  ->  {22}\n",
      "23  ->  {23}\n",
      "24  ->  {24}\n",
      "25  ->  {25}\n",
      "26  ->  {26, 75}\n",
      "27  ->  {27}\n",
      "29  ->  {50, 37, 29}\n",
      "30  ->  {30}\n",
      "31  ->  {31}\n",
      "32  ->  {32}\n",
      "33  ->  {33}\n",
      "34  ->  {34}\n",
      "35  ->  {35, 53}\n",
      "36  ->  {36}\n",
      "38  ->  {38}\n",
      "39  ->  {39}\n",
      "40  ->  {40}\n",
      "41  ->  {41, 83, 46}\n",
      "42  ->  {42}\n",
      "43  ->  {43}\n",
      "44  ->  {44}\n",
      "45  ->  {45, 79}\n",
      "47  ->  {47}\n",
      "48  ->  {48}\n",
      "49  ->  {49}\n",
      "51  ->  {51}\n",
      "52  ->  {52}\n",
      "54  ->  {54}\n",
      "55  ->  {55}\n",
      "56  ->  {56, 80}\n",
      "58  ->  {58}\n",
      "59  ->  {59}\n",
      "60  ->  {60}\n",
      "61  ->  {61}\n",
      "62  ->  {62}\n",
      "64  ->  {64}\n",
      "65  ->  {65}\n",
      "66  ->  {66}\n",
      "68  ->  {68}\n",
      "69  ->  {69}\n",
      "70  ->  {70}\n",
      "71  ->  {71}\n",
      "73  ->  {73}\n",
      "74  ->  {74}\n",
      "76  ->  {76}\n",
      "77  ->  {77}\n",
      "78  ->  {78}\n",
      "81  ->  {81}\n",
      "84  ->  {84}\n",
      "85  ->  {85}\n",
      "86  ->  {86}\n",
      "87  ->  {87}\n",
      "89  ->  {89}\n",
      "91  ->  {91}\n",
      "93  ->  {93}\n",
      "94  ->  {94}\n",
      "95  ->  {95}\n",
      "96  ->  {96}\n",
      "97  ->  {97}\n",
      "99  ->  {99}\n"
     ]
    }
   ],
   "source": [
    "for key in setpoints.keys():\n",
    "    print(key,\" -> \",setpoints[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(setpoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have i many nodes in place of 100. Now, we will first fix the incoming weights this i many nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "temp=[]\n",
    "for node in setpoints.keys():\n",
    "    row=torch.zeros(len(net.state_dict()[\"fc2.weight\"][0]), dtype=torch.float)\n",
    "    for val in setpoints[node]:\n",
    "        row+=net.state_dict()[\"fc2.weight\"][val]\n",
    "    temp.append(row)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 240])\n",
      "torch.Size([80, 240])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.weight\"].shape)\n",
    "net.state_dict()[\"fc2.weight\"].resize_((len(temp), len(temp[0])))\n",
    "print(net.state_dict()[\"fc2.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp[0])):\n",
    "        net.state_dict()[\"fc2.weight\"][i][j]=temp[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 240])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "temp=[]\n",
    "for node in setpoints.keys():\n",
    "    ele=0\n",
    "    for val in setpoints[node]:\n",
    "        ele+=net.state_dict()[\"fc2.bias\"][val]\n",
    "    temp.append(ele)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.bias\"].shape)\n",
    "net.state_dict()[\"fc2.bias\"].resize_((len(temp)))\n",
    "print(net.state_dict()[\"fc2.bias\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(temp)):\n",
    "    net.state_dict()[\"fc2.bias\"][i]=temp[i]\n",
    "print(net.state_dict()[\"fc2.bias\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc3.weight\"].shape)\n",
    "temp=[]\n",
    "for i in range(len(net.state_dict()[\"fc3.weight\"])):\n",
    "    row=[]\n",
    "    for j in setpoints.keys():\n",
    "        ele=0\n",
    "        for val in setpoints[j]:\n",
    "            ele+=net.state_dict()[\"fc3.weight\"][i][val]\n",
    "        row.append(ele/len(setpoints[j]))\n",
    "    temp.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 80\n"
     ]
    }
   ],
   "source": [
    "print(len(temp), len(temp[0]))\n",
    "# print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 80])\n"
     ]
    }
   ],
   "source": [
    "net.state_dict()[\"fc3.weight\"].resize_(len(temp), len(temp[0]))\n",
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp[0])):\n",
    "        net.state_dict()[\"fc3.weight\"][i][j]=temp[i][j]\n",
    "print(net.state_dict()[\"fc3.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print((net.state_dict()[\"fc3.bias\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.570000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the accuracy actually come up to 76.5, the actual accuracy without any pruning was 89.7. Let's try to freeze the previous layers, and do a fine_tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for parameter in net.parameters():\n",
    "    i+=1\n",
    "    if(i<5):\n",
    "        parameter.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=200,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   torch.Size([6, 1, 5, 5]) \n",
      "  Parameter containing:\n",
      "tensor([[[[-0.0192,  0.2511,  0.2183,  0.0210,  0.0190],\n",
      "          [-0.0522,  0.3645,  0.3561,  0.5513,  0.3440],\n",
      "          [ 0.0598,  0.5070,  0.6116,  0.6220,  0.2953],\n",
      "          [ 0.2985,  0.1301,  0.4567,  0.1638,  0.2149],\n",
      "          [-0.0246,  0.3206,  0.1249,  0.1288, -0.1632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1775, -0.0977,  0.0400,  0.2623,  0.2931],\n",
      "          [ 0.0318, -0.1988, -0.1956,  0.1583,  0.1678],\n",
      "          [-0.2439, -0.2983, -0.0792,  0.2235,  0.3461],\n",
      "          [-0.0280, -0.2769, -0.1121,  0.0926,  0.3560],\n",
      "          [-0.1970, -0.1645,  0.1938,  0.2115,  0.1824]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1362,  0.2118, -0.2154, -0.2076, -0.2334],\n",
      "          [ 0.2467, -0.0821, -0.1670, -0.0738,  0.2066],\n",
      "          [ 0.2564,  0.1952,  0.2048,  0.3864,  0.0963],\n",
      "          [ 0.0786,  0.3546,  0.3418, -0.0168,  0.1037],\n",
      "          [ 0.0178, -0.2039, -0.1759, -0.0146, -0.0048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299,  0.1694, -0.0029, -0.1827, -0.0718],\n",
      "          [-0.1909, -0.1638,  0.1044, -0.2096,  0.0405],\n",
      "          [ 0.0873, -0.1777, -0.1820, -0.0258, -0.1887],\n",
      "          [ 0.0190, -0.1462, -0.0486,  0.0288, -0.2104],\n",
      "          [ 0.1727, -0.1630, -0.0454, -0.2031, -0.1915]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1378,  0.0087, -0.1978, -0.2727,  0.0345],\n",
      "          [ 0.2285, -0.0154, -0.1342, -0.2998, -0.0739],\n",
      "          [ 0.2801,  0.2191,  0.1079,  0.2947,  0.1320],\n",
      "          [ 0.3344,  0.4029,  0.3628,  0.5148,  0.0400],\n",
      "          [ 0.0682,  0.0300,  0.3382,  0.2907,  0.3091]]],\n",
      "\n",
      "\n",
      "        [[[-0.2584, -0.2308, -0.1856,  0.0330,  0.0465],\n",
      "          [-0.2978, -0.1939, -0.2057,  0.0400,  0.1692],\n",
      "          [ 0.0883, -0.1518,  0.0861, -0.1625,  0.0874],\n",
      "          [ 0.1464,  0.2543,  0.0292,  0.0581, -0.0674],\n",
      "          [ 0.3818,  0.3657,  0.2362, -0.0540,  0.1097]]]]) \n",
      "\n",
      "\n",
      "2   torch.Size([6]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.2434, -0.0632,  0.2198,  0.1060,  0.1912,  0.0086]) \n",
      "\n",
      "\n",
      "3   torch.Size([16, 6, 5, 5]) \n",
      "  Parameter containing:\n",
      "tensor([[[[-0.0109, -0.0548, -0.0268,  0.0555,  0.0529],\n",
      "          [-0.0747, -0.0093, -0.0871,  0.0756,  0.0290],\n",
      "          [ 0.0085, -0.0744, -0.1693, -0.1346, -0.1455],\n",
      "          [ 0.0382, -0.0446, -0.1455, -0.0654, -0.0830],\n",
      "          [ 0.0522,  0.1614,  0.1331,  0.1559,  0.0276]],\n",
      "\n",
      "         [[-0.0135,  0.0531,  0.0238,  0.0102,  0.0312],\n",
      "          [ 0.0628, -0.0327,  0.0206, -0.0319,  0.0428],\n",
      "          [-0.0834, -0.1235, -0.0557,  0.0262, -0.0169],\n",
      "          [ 0.0244, -0.0334,  0.0382, -0.0140,  0.0345],\n",
      "          [ 0.1239, -0.0369, -0.0981, -0.0913, -0.0969]],\n",
      "\n",
      "         [[-0.0242, -0.0831,  0.0583,  0.0632, -0.0662],\n",
      "          [-0.0261, -0.0701,  0.0334, -0.0154,  0.0068],\n",
      "          [-0.0244,  0.0824, -0.1057, -0.1036,  0.0335],\n",
      "          [-0.0140,  0.1386,  0.0705,  0.0171, -0.0072],\n",
      "          [ 0.0018,  0.1739,  0.1223,  0.0542,  0.0006]],\n",
      "\n",
      "         [[ 0.0685,  0.0419, -0.0685, -0.0128, -0.0514],\n",
      "          [ 0.0531,  0.0907, -0.0022,  0.0279,  0.0706],\n",
      "          [ 0.0644,  0.0581,  0.0447,  0.0862, -0.0299],\n",
      "          [-0.0563, -0.0664,  0.0204, -0.0461, -0.0049],\n",
      "          [-0.0201, -0.0669, -0.0425, -0.0638,  0.0590]],\n",
      "\n",
      "         [[-0.0472,  0.0201, -0.0225, -0.0141, -0.0058],\n",
      "          [ 0.0624,  0.0067,  0.0042,  0.0046,  0.0419],\n",
      "          [ 0.0394,  0.0676, -0.0732, -0.1638, -0.0343],\n",
      "          [ 0.1362,  0.1999,  0.0978,  0.1239,  0.0379],\n",
      "          [ 0.0652,  0.1395,  0.1929,  0.1191,  0.0623]],\n",
      "\n",
      "         [[ 0.0314, -0.0123, -0.0633,  0.0803,  0.0407],\n",
      "          [-0.0420,  0.0859,  0.0217, -0.0667,  0.0375],\n",
      "          [ 0.0298,  0.0806,  0.0096,  0.0557, -0.0537],\n",
      "          [ 0.0555, -0.0591,  0.0920,  0.1185,  0.0150],\n",
      "          [-0.0733, -0.1120, -0.0059,  0.1112,  0.0869]]],\n",
      "\n",
      "\n",
      "        [[[-0.0222, -0.0632,  0.0230,  0.0518,  0.0471],\n",
      "          [-0.0131, -0.0666, -0.0177, -0.0039,  0.0601],\n",
      "          [ 0.0307,  0.0119,  0.1052, -0.0188,  0.0316],\n",
      "          [-0.0572, -0.0849, -0.0043,  0.1368,  0.0699],\n",
      "          [-0.0658, -0.1288, -0.1282, -0.0940, -0.0316]],\n",
      "\n",
      "         [[ 0.0440,  0.0498,  0.0480,  0.0980,  0.0044],\n",
      "          [-0.0486,  0.0267,  0.0349, -0.0436,  0.0280],\n",
      "          [-0.0089,  0.0524,  0.0789, -0.0944, -0.0586],\n",
      "          [-0.0659,  0.0035,  0.0432, -0.0714, -0.0228],\n",
      "          [ 0.0241, -0.0759, -0.0205,  0.0458, -0.0653]],\n",
      "\n",
      "         [[-0.0281,  0.0291, -0.0404,  0.0430, -0.0491],\n",
      "          [ 0.0727, -0.0770, -0.0016,  0.0005,  0.0026],\n",
      "          [-0.0638,  0.0402,  0.0268,  0.0278, -0.0408],\n",
      "          [-0.0593, -0.0857, -0.0497,  0.1150,  0.1128],\n",
      "          [-0.0520, -0.0732, -0.0377, -0.0443, -0.0022]],\n",
      "\n",
      "         [[-0.0036, -0.0737,  0.0009,  0.0342,  0.0625],\n",
      "          [ 0.0713, -0.0301, -0.0545, -0.0360, -0.0155],\n",
      "          [ 0.0323,  0.0786, -0.0489, -0.0150, -0.0732],\n",
      "          [-0.0087,  0.0101,  0.0736, -0.0299,  0.0564],\n",
      "          [ 0.0291, -0.0514,  0.0165, -0.0648, -0.0488]],\n",
      "\n",
      "         [[-0.0781,  0.0070, -0.0439,  0.0522,  0.0101],\n",
      "          [-0.0655, -0.0328,  0.0275, -0.0029, -0.0499],\n",
      "          [-0.0462,  0.0460,  0.0361,  0.1371,  0.0840],\n",
      "          [ 0.0280, -0.0768,  0.0399,  0.0632,  0.1553],\n",
      "          [ 0.0167,  0.0032, -0.0908, -0.1111, -0.0763]],\n",
      "\n",
      "         [[ 0.0044,  0.0608, -0.0532, -0.0601,  0.0587],\n",
      "          [-0.0058, -0.0142, -0.0047, -0.0417, -0.0031],\n",
      "          [-0.0824, -0.0439, -0.0758, -0.0655, -0.0323],\n",
      "          [ 0.0141, -0.0282,  0.0304,  0.0188, -0.0538],\n",
      "          [-0.0402,  0.0069,  0.0732,  0.0188, -0.0666]]],\n",
      "\n",
      "\n",
      "        [[[-0.0290, -0.0331, -0.0362, -0.0128, -0.1624],\n",
      "          [ 0.0264,  0.0704,  0.0559,  0.0057, -0.1444],\n",
      "          [ 0.0334, -0.0197,  0.0786,  0.1296, -0.0665],\n",
      "          [-0.1325, -0.0002, -0.0102,  0.0673,  0.1488],\n",
      "          [ 0.0281,  0.0752,  0.1247,  0.0654,  0.0795]],\n",
      "\n",
      "         [[ 0.0432,  0.0453, -0.0163, -0.0221, -0.0871],\n",
      "          [ 0.0297,  0.0078, -0.0213, -0.0102,  0.0096],\n",
      "          [-0.0610,  0.0428,  0.1683,  0.1447,  0.1145],\n",
      "          [ 0.0129,  0.1065,  0.1422,  0.0659,  0.0251],\n",
      "          [-0.0014,  0.0767,  0.0355,  0.0775,  0.0778]],\n",
      "\n",
      "         [[-0.0408,  0.0344,  0.0030, -0.0213, -0.0086],\n",
      "          [ 0.0332,  0.0192,  0.0254,  0.0032,  0.0026],\n",
      "          [-0.0381, -0.0148,  0.0117,  0.0037,  0.0669],\n",
      "          [ 0.0096, -0.0044,  0.0353, -0.0138,  0.0783],\n",
      "          [ 0.0304,  0.0076,  0.0405,  0.0741,  0.0048]],\n",
      "\n",
      "         [[-0.0272,  0.0014, -0.0568,  0.0614,  0.0953],\n",
      "          [ 0.0689, -0.0018,  0.0250,  0.0122,  0.0488],\n",
      "          [ 0.0131, -0.0347,  0.0192,  0.0197, -0.0680],\n",
      "          [ 0.0553, -0.0440, -0.0922, -0.0444,  0.0591],\n",
      "          [ 0.0409, -0.0933,  0.0049, -0.0250,  0.0385]],\n",
      "\n",
      "         [[ 0.0040,  0.0977,  0.0278,  0.0440, -0.1256],\n",
      "          [ 0.0426,  0.0925,  0.1221,  0.1534, -0.0139],\n",
      "          [-0.0277,  0.0244, -0.0303,  0.1574,  0.0935],\n",
      "          [-0.0314,  0.0749,  0.0602,  0.0665,  0.0982],\n",
      "          [ 0.0176,  0.0285,  0.0090,  0.0930, -0.0757]],\n",
      "\n",
      "         [[-0.0138,  0.1097,  0.0643,  0.1291,  0.1020],\n",
      "          [ 0.0214,  0.0248, -0.0334,  0.1013,  0.0531],\n",
      "          [-0.0008, -0.0636,  0.0248, -0.0394, -0.0282],\n",
      "          [ 0.1024,  0.0172,  0.0456, -0.0545,  0.0182],\n",
      "          [-0.0197,  0.1342,  0.0421,  0.0906,  0.0427]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0875,  0.0322,  0.0375,  0.0408,  0.0054],\n",
      "          [-0.0460, -0.0633, -0.0649,  0.0760, -0.0567],\n",
      "          [ 0.0322, -0.0105, -0.0606, -0.0797, -0.0679],\n",
      "          [-0.0232, -0.0553, -0.0724,  0.0120, -0.0809],\n",
      "          [ 0.0408, -0.0291, -0.0568, -0.0558, -0.0341]],\n",
      "\n",
      "         [[-0.0568, -0.0631,  0.0572, -0.0491, -0.0536],\n",
      "          [-0.0138,  0.0440,  0.0787,  0.0738,  0.0220],\n",
      "          [-0.0163,  0.0168, -0.0073,  0.0200, -0.0652],\n",
      "          [ 0.0009,  0.0010, -0.0487, -0.0715,  0.0072],\n",
      "          [-0.0719, -0.0316, -0.0134, -0.0032,  0.0724]],\n",
      "\n",
      "         [[ 0.0582, -0.0085,  0.0132, -0.0703,  0.0159],\n",
      "          [ 0.0671, -0.0448,  0.0393, -0.0356,  0.0394],\n",
      "          [ 0.0450,  0.0304, -0.0662,  0.0135,  0.0431],\n",
      "          [-0.0309,  0.0268, -0.0411, -0.0587,  0.0662],\n",
      "          [ 0.0126,  0.0711, -0.0226, -0.0170,  0.0568]],\n",
      "\n",
      "         [[ 0.0282, -0.0763, -0.0282, -0.0218, -0.0370],\n",
      "          [ 0.0586, -0.0726, -0.0211, -0.0571, -0.0420],\n",
      "          [ 0.0335,  0.0127,  0.0681,  0.0412,  0.0192],\n",
      "          [-0.0766, -0.0016, -0.0073,  0.0633, -0.0200],\n",
      "          [-0.0027, -0.0093,  0.0077, -0.0436,  0.0106]],\n",
      "\n",
      "         [[-0.0392,  0.0598, -0.0669,  0.0659, -0.0636],\n",
      "          [-0.0772, -0.0788, -0.0499, -0.0290,  0.0244],\n",
      "          [-0.0204, -0.0283,  0.0083, -0.0568,  0.0490],\n",
      "          [ 0.0234, -0.0123, -0.0521,  0.0399, -0.0175],\n",
      "          [ 0.0072, -0.0435, -0.0295, -0.0564, -0.0306]],\n",
      "\n",
      "         [[ 0.0560, -0.0579, -0.0484,  0.0134,  0.0548],\n",
      "          [-0.0322,  0.0032,  0.0593, -0.0421, -0.0552],\n",
      "          [ 0.0751,  0.0288, -0.0811, -0.0192, -0.0289],\n",
      "          [-0.0103,  0.0337, -0.0256, -0.0466,  0.0394],\n",
      "          [ 0.0175,  0.0412, -0.0659, -0.0206,  0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340,  0.0957,  0.0862, -0.0030,  0.0326],\n",
      "          [ 0.0625,  0.1338,  0.0669, -0.0977,  0.0272],\n",
      "          [ 0.1258,  0.2758,  0.0864, -0.1109, -0.1041],\n",
      "          [ 0.1510,  0.1942,  0.0006, -0.1443, -0.1476],\n",
      "          [ 0.1227,  0.0747, -0.0599, -0.0265, -0.0703]],\n",
      "\n",
      "         [[ 0.0696,  0.0604,  0.0966,  0.0341, -0.0003],\n",
      "          [ 0.2600,  0.1970,  0.0654, -0.0413,  0.0700],\n",
      "          [ 0.2760,  0.1238,  0.0613,  0.0552, -0.0365],\n",
      "          [ 0.2264,  0.0426,  0.0530, -0.0057,  0.0397],\n",
      "          [ 0.0955,  0.0771,  0.0420, -0.0585, -0.0408]],\n",
      "\n",
      "         [[-0.0805, -0.0571,  0.0686,  0.0679,  0.0019],\n",
      "          [ 0.0002, -0.0732,  0.0858,  0.0114, -0.0765],\n",
      "          [-0.0982,  0.0699,  0.1299,  0.0603, -0.0301],\n",
      "          [-0.0205,  0.0617,  0.0674,  0.0328,  0.0286],\n",
      "          [-0.0963,  0.0384,  0.1049,  0.1011,  0.0627]],\n",
      "\n",
      "         [[ 0.0357,  0.0340,  0.0522,  0.0495,  0.0708],\n",
      "          [-0.0845, -0.0224, -0.0304, -0.0551, -0.0190],\n",
      "          [-0.0668,  0.0409, -0.0619, -0.0562,  0.0009],\n",
      "          [ 0.0180, -0.0264,  0.0483,  0.0653,  0.0173],\n",
      "          [ 0.0295,  0.0553,  0.0585, -0.0805,  0.0243]],\n",
      "\n",
      "         [[ 0.1076, -0.0023, -0.0163, -0.0140,  0.0564],\n",
      "          [ 0.0750,  0.0792, -0.0273,  0.0257, -0.0842],\n",
      "          [ 0.0222,  0.0533, -0.0139, -0.0419, -0.0244],\n",
      "          [ 0.0319,  0.0712,  0.0382,  0.0173,  0.0096],\n",
      "          [-0.0934,  0.0405,  0.1524,  0.0470,  0.0904]],\n",
      "\n",
      "         [[-0.0850,  0.0575,  0.0297,  0.0397,  0.0481],\n",
      "          [-0.0801, -0.0908, -0.0910, -0.0494,  0.0574],\n",
      "          [ 0.1137, -0.0567, -0.1489, -0.0140,  0.0657],\n",
      "          [ 0.0604, -0.0574, -0.0979,  0.0109,  0.0994],\n",
      "          [ 0.0521, -0.0905,  0.0108,  0.0659,  0.0968]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1651,  0.1899,  0.1213,  0.0844,  0.0381],\n",
      "          [ 0.0872,  0.0726,  0.1388,  0.1072,  0.0381],\n",
      "          [-0.0822, -0.0235, -0.0253,  0.0575,  0.0126],\n",
      "          [-0.1143, -0.0686,  0.0064,  0.0676,  0.0519],\n",
      "          [-0.0414, -0.0143, -0.0119,  0.0260,  0.0315]],\n",
      "\n",
      "         [[-0.0832, -0.0186,  0.0401, -0.0030,  0.0215],\n",
      "          [-0.0418,  0.0453,  0.0292,  0.0065,  0.0488],\n",
      "          [ 0.0127, -0.1047,  0.0074,  0.0770,  0.0699],\n",
      "          [-0.1040,  0.0111, -0.0547,  0.0259,  0.0403],\n",
      "          [-0.0534, -0.0387, -0.0023,  0.0301, -0.0905]],\n",
      "\n",
      "         [[ 0.0518,  0.1405,  0.1145,  0.0954, -0.0037],\n",
      "          [-0.0601,  0.0425, -0.0251,  0.0921, -0.0411],\n",
      "          [-0.0104, -0.0991, -0.0232, -0.0040,  0.0175],\n",
      "          [ 0.0217, -0.0569, -0.0210, -0.0877,  0.0530],\n",
      "          [ 0.0366, -0.0513,  0.0025, -0.0891,  0.0799]],\n",
      "\n",
      "         [[ 0.0528,  0.0358, -0.0843,  0.0293,  0.0443],\n",
      "          [ 0.0218,  0.0180, -0.0699, -0.0587,  0.0115],\n",
      "          [ 0.0040,  0.0050,  0.0047,  0.0408,  0.0669],\n",
      "          [ 0.0741, -0.0620,  0.0226,  0.0472,  0.0682],\n",
      "          [ 0.0717,  0.0853, -0.0000, -0.0112, -0.0270]],\n",
      "\n",
      "         [[ 0.1038,  0.1426,  0.0998,  0.0320,  0.1111],\n",
      "          [-0.1177, -0.0672,  0.0677,  0.1034, -0.0451],\n",
      "          [-0.0871, -0.1239, -0.0099, -0.0290, -0.0613],\n",
      "          [-0.0878, -0.0596, -0.0144, -0.0210,  0.0641],\n",
      "          [-0.0289,  0.0519, -0.0234,  0.0174,  0.0056]],\n",
      "\n",
      "         [[-0.0465,  0.0461,  0.0285,  0.0367,  0.0337],\n",
      "          [ 0.0520, -0.0826, -0.0852, -0.0947, -0.1000],\n",
      "          [ 0.0594,  0.0626,  0.0371,  0.0097, -0.0623],\n",
      "          [ 0.0788, -0.0128,  0.0158, -0.0668, -0.0158],\n",
      "          [-0.0014,  0.0584, -0.0293, -0.0480, -0.0613]]]]) \n",
      "\n",
      "\n",
      "4   torch.Size([16]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0730,  0.0296,  0.0032,  0.0657,  0.0553,  0.0152, -0.0441, -0.0667,\n",
      "         0.0277,  0.0222, -0.0358,  0.0246,  0.0430, -0.0688, -0.0241,  0.0027]) \n",
      "\n",
      "\n",
      "5   torch.Size([240, 256]) \n",
      "  Parameter containing:\n",
      "tensor([[ 0.0272,  0.0675,  0.0587,  ...,  0.0256, -0.0110, -0.0366],\n",
      "        [ 0.0592,  0.0532,  0.0201,  ..., -0.0201, -0.0062,  0.0536],\n",
      "        [-0.0039, -0.0355, -0.0470,  ...,  0.0263, -0.0019, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0156, -0.0477,  0.0329,  ...,  0.0190, -0.0335,  0.0158],\n",
      "        [ 0.0400,  0.0265, -0.0372,  ..., -0.0495,  0.0229,  0.0028],\n",
      "        [ 0.0613,  0.0368, -0.0311,  ..., -0.0378, -0.0014,  0.0345]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "6   torch.Size([240]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0125,  0.0496,  0.0088,  0.0892,  0.0095, -0.0318, -0.0080,  0.0076,\n",
      "         0.1121, -0.0552, -0.0590, -0.0311, -0.1044,  0.0289, -0.1501,  0.0239,\n",
      "        -0.0497, -0.1206, -0.0486,  0.0315, -0.0028,  0.0640, -0.0397,  0.0408,\n",
      "        -0.0344,  0.0475,  0.0011,  0.0218, -0.0139,  0.0099, -0.0637, -0.0040,\n",
      "         0.0283, -0.0100, -0.0033,  0.0194,  0.0250,  0.0024, -0.0618,  0.0670,\n",
      "        -0.0075, -0.0286, -0.0415,  0.0529,  0.0515, -0.0339,  0.0435,  0.0390,\n",
      "        -0.0633, -0.0635,  0.0749,  0.0338, -0.0741, -0.0068,  0.0022, -0.0306,\n",
      "         0.0367, -0.0081, -0.0037, -0.0045,  0.0001,  0.0466,  0.0454,  0.0361,\n",
      "         0.0541, -0.0258, -0.0537, -0.0620,  0.0021,  0.0354,  0.0299,  0.0189,\n",
      "         0.0373, -0.0519,  0.0416, -0.0254, -0.0425,  0.0406, -0.0551,  0.0194,\n",
      "         0.0171,  0.0000,  0.0631,  0.0508, -0.0166, -0.0199,  0.0328, -0.0614,\n",
      "        -0.0028,  0.0111, -0.0030, -0.0501, -0.0229,  0.0413, -0.0580,  0.0094,\n",
      "         0.0230,  0.0387, -0.0001,  0.0430, -0.0575,  0.0499, -0.0169,  0.0482,\n",
      "         0.0148, -0.0124,  0.0406, -0.0329, -0.0347, -0.0067,  0.0056,  0.0478,\n",
      "         0.0292, -0.0203,  0.0126, -0.0361,  0.0398,  0.0525, -0.1081,  0.0590,\n",
      "        -0.0428, -0.0445, -0.0392,  0.0405,  0.0348,  0.0296, -0.0262, -0.0133,\n",
      "         0.0530, -0.0203,  0.0406, -0.0518,  0.0414,  0.0028,  0.0611,  0.0647,\n",
      "        -0.0044,  0.0347, -0.0470,  0.0146, -0.0190, -0.0501,  0.0331, -0.0305,\n",
      "        -0.0679,  0.0532, -0.0100, -0.0257,  0.0368,  0.0359,  0.0030, -0.0206,\n",
      "        -0.0331,  0.0385, -0.0050,  0.0362, -0.0105, -0.0539,  0.0082, -0.0274,\n",
      "        -0.0482, -0.0504, -0.0238,  0.0111, -0.0489,  0.0606, -0.0305,  0.0884,\n",
      "         0.0450, -0.0125,  0.0277, -0.0629, -0.0444, -0.0058,  0.0355,  0.0439,\n",
      "        -0.0299, -0.0286, -0.0260, -0.0086,  0.0235,  0.0480,  0.0576, -0.0442,\n",
      "        -0.0399,  0.0131,  0.0340, -0.0147, -0.0120,  0.0293,  0.0324, -0.0166,\n",
      "        -0.0555,  0.0493, -0.0615,  0.0566,  0.0540, -0.0254, -0.0540, -0.0359,\n",
      "         0.0491, -0.0252,  0.0568, -0.0021, -0.0235,  0.0607,  0.0233, -0.0354,\n",
      "         0.0298,  0.0625,  0.0386, -0.0303,  0.0186, -0.0189, -0.0094, -0.0109,\n",
      "        -0.0081,  0.0345, -0.0408, -0.0037,  0.0441,  0.0274, -0.0352,  0.0627,\n",
      "        -0.0417,  0.0531, -0.0357,  0.0050,  0.0328, -0.0267,  0.0032, -0.0200,\n",
      "        -0.0275, -0.0526,  0.0087,  0.0208, -0.0119,  0.0515, -0.0366,  0.0507],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "7   torch.Size([80, 240]) \n",
      "  Parameter containing:\n",
      "tensor([[ 0.0402, -0.0434,  0.0285,  ...,  0.0529, -0.0308,  0.0146],\n",
      "        [ 0.0027,  0.0438, -0.0360,  ..., -0.0557,  0.0140,  0.0263],\n",
      "        [ 0.0155, -0.0348,  0.0268,  ..., -0.0498,  0.0200,  0.0531],\n",
      "        ...,\n",
      "        [ 0.0506,  0.0316, -0.0397,  ...,  0.0654,  0.0248,  0.0681],\n",
      "        [ 0.0570, -0.0697,  0.0352,  ..., -0.0591, -0.0380, -0.0107],\n",
      "        [-0.0361,  0.0464, -0.0538,  ..., -0.0188,  0.0255, -0.0736]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "8   torch.Size([80]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0397, -0.0087, -0.0203,  0.0456,  0.0315, -0.0314, -0.0457,  0.0263,\n",
      "        -0.0335,  0.0092, -0.0015, -0.0373,  0.0526,  0.0804, -0.0135,  0.1263,\n",
      "         0.0312,  0.0291,  0.0140,  0.0081, -0.0266,  0.0049,  0.0078,  0.0362,\n",
      "        -0.0298,  0.0693,  0.0446,  0.0119, -0.0411,  0.0599, -0.0495, -0.0417,\n",
      "        -0.0507, -0.0123, -0.0193, -0.0215,  0.0462, -0.0018, -0.0284,  0.0102,\n",
      "        -0.0230,  0.0454, -0.0338,  0.0525, -0.0359,  0.0639,  0.0477,  0.0393,\n",
      "         0.0211,  0.0039, -0.0399, -0.0345, -0.0042, -0.0272,  0.0004, -0.0283,\n",
      "         0.0067, -0.0566,  0.0067,  0.0128, -0.0366, -0.0135, -0.0400,  0.0588,\n",
      "         0.0412,  0.0009,  0.0443,  0.0051,  0.0548,  0.0299,  0.0294, -0.0427,\n",
      "        -0.0392,  0.0366, -0.0465, -0.0501, -0.0336, -0.0278,  0.0443, -0.0206],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "9   torch.Size([10, 80]) \n",
      "  Parameter containing:\n",
      "tensor([[-0.0799, -0.0514,  0.0154,  0.0115, -0.0843, -0.0275,  0.0830, -0.0554,\n",
      "         -0.0946, -0.0866, -0.1241,  0.0442, -0.0847,  0.0520, -0.0870, -0.0025,\n",
      "          0.0141, -0.1187,  0.0135,  0.0641,  0.0971,  0.1102, -0.0481, -0.1465,\n",
      "         -0.0098, -0.1137,  0.0834,  0.0754, -0.0701, -0.0155,  0.1341, -0.0931,\n",
      "         -0.0880,  0.0836,  0.1427, -0.1458, -0.1428,  0.0597,  0.0834, -0.0122,\n",
      "          0.0128, -0.0915, -0.0585,  0.1132, -0.0641,  0.0277,  0.0624, -0.0213,\n",
      "          0.1714, -0.1378, -0.0149,  0.1095, -0.0326, -0.0129,  0.1646, -0.1588,\n",
      "         -0.0397, -0.0864,  0.0431,  0.0588, -0.0666,  0.0588,  0.0891, -0.0775,\n",
      "         -0.0723,  0.0007, -0.1620,  0.0115,  0.0489, -0.0417, -0.0408,  0.1485,\n",
      "          0.1629, -0.0451,  0.1137, -0.0703,  0.1330, -0.0167, -0.0833,  0.0967],\n",
      "        [ 0.0605, -0.0714, -0.1559, -0.0228, -0.0891, -0.0269,  0.0549, -0.1048,\n",
      "          0.1263, -0.1483,  0.1830, -0.1227,  0.1579,  0.0762, -0.0642,  0.0115,\n",
      "          0.0171,  0.0748, -0.0418,  0.1174,  0.1188, -0.1357, -0.0569,  0.1670,\n",
      "          0.1431,  0.1882, -0.0619, -0.0153,  0.0541,  0.1883, -0.0972, -0.0972,\n",
      "         -0.0629, -0.1072, -0.0642,  0.1352,  0.0150,  0.0358, -0.1752,  0.1126,\n",
      "         -0.0674,  0.0176, -0.0967,  0.1348, -0.1442, -0.1404,  0.1202, -0.1379,\n",
      "          0.0576,  0.0873,  0.1180, -0.1181,  0.1069,  0.1197,  0.1634,  0.1384,\n",
      "         -0.0552,  0.0004, -0.0005, -0.0646, -0.0370, -0.0633,  0.0869, -0.1731,\n",
      "         -0.1184, -0.1579, -0.0766, -0.0766, -0.0060, -0.0381,  0.0882,  0.0496,\n",
      "         -0.0410, -0.0701, -0.1803,  0.1029, -0.0843, -0.0158, -0.1036, -0.0733],\n",
      "        [ 0.1163,  0.0529,  0.0872, -0.0318, -0.1313,  0.1147,  0.0832, -0.0014,\n",
      "         -0.0788,  0.0679,  0.0367,  0.0653,  0.2598, -0.0491,  0.0351,  0.0421,\n",
      "          0.0893, -0.1037, -0.0686, -0.0415,  0.1333,  0.0031,  0.0858, -0.0589,\n",
      "         -0.0327, -0.0715, -0.0293,  0.0300, -0.1061,  0.1893,  0.0260, -0.0539,\n",
      "         -0.0177,  0.2075, -0.1141,  0.0259,  0.0277, -0.0682, -0.0361, -0.0954,\n",
      "          0.1004, -0.0525, -0.1756,  0.0812,  0.0002,  0.1229,  0.0020,  0.0071,\n",
      "          0.0194,  0.0297, -0.0266, -0.0476, -0.0040, -0.1298,  0.0797, -0.0125,\n",
      "         -0.0713,  0.0886, -0.1828,  0.1266, -0.0312, -0.1565,  0.0433, -0.0532,\n",
      "          0.0067,  0.2707, -0.1773,  0.1225, -0.1364, -0.1227, -0.0573,  0.0918,\n",
      "          0.1082, -0.0893, -0.0031,  0.0543, -0.1526, -0.1887,  0.0908,  0.1185],\n",
      "        [-0.1174, -0.0956, -0.0778, -0.0557, -0.0976,  0.1048, -0.0927,  0.0063,\n",
      "         -0.0726, -0.1231,  0.0053,  0.0911, -0.0725, -0.1313,  0.0743,  0.0058,\n",
      "          0.0899,  0.2005, -0.1634, -0.0126, -0.0801, -0.0196, -0.1274, -0.1908,\n",
      "         -0.0726,  0.0627,  0.0747,  0.0219, -0.1379,  0.1264,  0.0201,  0.1231,\n",
      "         -0.0525, -0.0175,  0.2067,  0.1731,  0.1828,  0.0216, -0.0757,  0.1103,\n",
      "          0.0470, -0.0428, -0.1289, -0.1138,  0.1339,  0.0968, -0.1750, -0.1629,\n",
      "         -0.1297,  0.1004, -0.0119, -0.0819, -0.0517,  0.1247, -0.0554, -0.1113,\n",
      "          0.0962, -0.1201,  0.1305, -0.0559, -0.1068, -0.1020,  0.0870,  0.1114,\n",
      "         -0.0069,  0.1589, -0.0137,  0.0635, -0.0781,  0.0995, -0.0672,  0.0887,\n",
      "         -0.0577,  0.0129,  0.0918,  0.0148, -0.0447,  0.0651,  0.0980,  0.1064],\n",
      "        [ 0.1344, -0.0527, -0.1721,  0.0271,  0.0508, -0.0212,  0.0978, -0.0764,\n",
      "         -0.0590,  0.0773,  0.0813,  0.1136,  0.0045, -0.0976, -0.1574,  0.0167,\n",
      "         -0.0855, -0.0298, -0.1251,  0.1594,  0.1165, -0.0861, -0.1141, -0.0640,\n",
      "         -0.1274, -0.0167, -0.0561, -0.1808,  0.0198, -0.1278,  0.0701, -0.1002,\n",
      "          0.0427, -0.0676,  0.0505, -0.0883, -0.0518, -0.0048,  0.0013, -0.1970,\n",
      "          0.0006,  0.0662,  0.0711, -0.0467, -0.0935, -0.0608,  0.0216,  0.0968,\n",
      "         -0.0574,  0.0614,  0.1699,  0.1728,  0.2329, -0.0513,  0.0553,  0.0237,\n",
      "         -0.1188,  0.1711,  0.0112,  0.0978,  0.0454,  0.0862,  0.0608,  0.0068,\n",
      "          0.0083, -0.1468,  0.1673,  0.0521,  0.1892, -0.1432,  0.0500,  0.0170,\n",
      "          0.0035,  0.0328, -0.1939,  0.1007,  0.0200, -0.0886, -0.0748, -0.0459],\n",
      "        [-0.0702,  0.0972, -0.1089, -0.0081,  0.1649,  0.1673,  0.0355, -0.0335,\n",
      "          0.0098, -0.0335, -0.0313,  0.0474, -0.2167,  0.1124,  0.1774,  0.0653,\n",
      "         -0.1510, -0.0437,  0.0292,  0.0118, -0.1072,  0.0775,  0.1941,  0.0081,\n",
      "         -0.0513,  0.1146, -0.0216,  0.0142,  0.1406, -0.0295,  0.0406,  0.1028,\n",
      "          0.0935,  0.0855, -0.0241, -0.0670,  0.0759,  0.0414, -0.1360,  0.1547,\n",
      "         -0.0115, -0.0555,  0.0071, -0.1181, -0.0383,  0.0827,  0.0534, -0.1492,\n",
      "          0.0719, -0.0411, -0.0322,  0.0634, -0.1965,  0.0355, -0.1877, -0.0153,\n",
      "          0.1204, -0.0848, -0.0121,  0.0730, -0.0186,  0.0754,  0.0924,  0.1594,\n",
      "          0.0770, -0.0528,  0.0936, -0.0935, -0.0352,  0.0623, -0.0253, -0.1390,\n",
      "         -0.1633, -0.1226,  0.1635, -0.1238, -0.0575,  0.2134, -0.1288, -0.1462],\n",
      "        [-0.1267,  0.0608,  0.0945, -0.0559,  0.0080, -0.1502, -0.0231, -0.0381,\n",
      "          0.0615,  0.1791,  0.0682, -0.0547, -0.1165, -0.0081,  0.1215,  0.1019,\n",
      "          0.0552,  0.0252,  0.0310,  0.0416, -0.0192, -0.0318,  0.1169, -0.0378,\n",
      "         -0.0668, -0.1821, -0.0261, -0.0966,  0.0173, -0.1199, -0.0362, -0.0371,\n",
      "         -0.0146, -0.0871, -0.0467, -0.1680, -0.0357,  0.0334, -0.1178, -0.0236,\n",
      "          0.0737, -0.0489, -0.0225,  0.0352, -0.2038,  0.0369, -0.0268,  0.0528,\n",
      "          0.2330, -0.0967, -0.0252,  0.0119, -0.1204, -0.0712, -0.1209,  0.0809,\n",
      "          0.0293,  0.0905, -0.0375, -0.0021,  0.1119,  0.0360,  0.0578, -0.1174,\n",
      "         -0.1238, -0.0625,  0.0602, -0.1556, -0.0276,  0.1907,  0.1193,  0.1279,\n",
      "          0.0608, -0.2167, -0.0226, -0.0062,  0.2766,  0.0613,  0.0723,  0.0362],\n",
      "        [ 0.1253, -0.0567, -0.0470, -0.0233, -0.1216,  0.1846, -0.0189,  0.0406,\n",
      "          0.0422, -0.0780, -0.0071, -0.2156, -0.0201,  0.0156, -0.0102, -0.0017,\n",
      "         -0.1399,  0.1202, -0.0799, -0.0376,  0.0086, -0.1423, -0.1784,  0.1010,\n",
      "          0.0510,  0.0469, -0.0048, -0.1095,  0.1140,  0.0282,  0.0649, -0.0995,\n",
      "         -0.0484,  0.1417, -0.0900,  0.0731, -0.1580,  0.0391,  0.1625, -0.1173,\n",
      "          0.0578,  0.0791, -0.0279,  0.0966,  0.2265, -0.0968,  0.1544, -0.0833,\n",
      "         -0.0294,  0.0427,  0.1117,  0.0786, -0.0282, -0.1254, -0.0582, -0.1701,\n",
      "         -0.0076, -0.1343, -0.1298, -0.0760, -0.0093, -0.0526,  0.0873,  0.0920,\n",
      "          0.0591, -0.0053,  0.0125, -0.1147, -0.0274, -0.1307, -0.1444,  0.0165,\n",
      "          0.1172,  0.0969,  0.1024,  0.1399, -0.1740, -0.1500,  0.0137,  0.0711],\n",
      "        [ 0.0179,  0.0685,  0.0873,  0.0141,  0.0886,  0.0197,  0.1005,  0.0666,\n",
      "         -0.1361,  0.1357, -0.0054, -0.0601,  0.1132, -0.1532, -0.0954, -0.0529,\n",
      "          0.1773, -0.1578, -0.0489, -0.0846, -0.0196,  0.1207,  0.0869, -0.0223,\n",
      "          0.0139, -0.0834,  0.0432,  0.0749,  0.0071,  0.0989, -0.0011, -0.0934,\n",
      "         -0.0596, -0.0573,  0.0562, -0.0934, -0.0274,  0.0464,  0.2146,  0.0413,\n",
      "         -0.0074, -0.0026,  0.1217, -0.0909,  0.1196,  0.0224,  0.0212,  0.0734,\n",
      "         -0.1166, -0.0079, -0.1991, -0.1180,  0.0931, -0.0650, -0.0690,  0.0157,\n",
      "          0.1252, -0.0985,  0.1322, -0.1907, -0.0653, -0.0911,  0.0222, -0.1590,\n",
      "         -0.0225,  0.1439,  0.0272, -0.1818, -0.1797,  0.0371,  0.1511, -0.0627,\n",
      "         -0.0997,  0.0007, -0.0139,  0.0055, -0.0742, -0.0375,  0.1489, -0.1793],\n",
      "        [ 0.0146, -0.0842,  0.0056,  0.0472,  0.0762,  0.0560,  0.0690,  0.0636,\n",
      "         -0.0916,  0.0020, -0.0134,  0.0796, -0.0850,  0.0101, -0.0669, -0.1164,\n",
      "          0.1429, -0.0458,  0.0598,  0.0671, -0.0477,  0.1118, -0.0051, -0.0064,\n",
      "          0.0115,  0.1009,  0.0377,  0.0861,  0.0048, -0.2677, -0.1203, -0.0678,\n",
      "         -0.0392, -0.1324, -0.0334,  0.0507,  0.0945, -0.0135, -0.1084, -0.0306,\n",
      "         -0.0056,  0.0023,  0.0358, -0.0649,  0.1931,  0.1092, -0.1128,  0.1549,\n",
      "         -0.1540, -0.0663,  0.0486, -0.0538,  0.0456,  0.1606, -0.0910,  0.0678,\n",
      "          0.1936,  0.0395, -0.0671,  0.0566, -0.1658,  0.0905, -0.0620,  0.1562,\n",
      "         -0.1265, -0.2520,  0.0131,  0.1002,  0.1458, -0.2233, -0.0389,  0.0779,\n",
      "          0.1174,  0.1004,  0.0297,  0.0805, -0.0051,  0.0482, -0.0716,  0.1311]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "10   torch.Size([10]) \n",
      "  Parameter containing:\n",
      "tensor([-0.0838,  0.0204, -0.0172,  0.0138,  0.0053, -0.0699,  0.0305,  0.0152,\n",
      "        -0.0514, -0.0417], requires_grad=True) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for parameter in net.parameters():\n",
    "    i+=1\n",
    "    print(i,\" \",parameter.shape,\"\\n \",parameter,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 20] loss: 0.0006094838506542146\n",
      "[1, 40] loss: 0.0005989485625177622\n",
      "[1, 60] loss: 0.000614115558564663\n",
      "[1, 80] loss: 0.0005456928666681051\n",
      "[1, 100] loss: 0.0007145761726424098\n",
      "[1, 120] loss: 0.0006162588186562061\n",
      "[1, 140] loss: 0.0006485054539516568\n",
      "[1, 160] loss: 0.0005409880904480815\n",
      "[1, 180] loss: 0.0006107934704050422\n",
      "[1, 200] loss: 0.0006601964691653848\n",
      "[1, 220] loss: 0.0006399058680981398\n",
      "[1, 240] loss: 0.0006563514890149236\n",
      "[1, 260] loss: 0.000514534461311996\n",
      "[1, 280] loss: 0.0005778991207480431\n",
      "[1, 300] loss: 0.0006308611407876014\n",
      "[2, 20] loss: 0.0005711859473958612\n",
      "[2, 40] loss: 0.0006681019682437181\n",
      "[2, 60] loss: 0.0006105819940567017\n",
      "[2, 80] loss: 0.0006261244425550104\n",
      "[2, 100] loss: 0.0005564392665401101\n",
      "[2, 120] loss: 0.0005441961912438274\n",
      "[2, 140] loss: 0.0006443125121295452\n",
      "[2, 160] loss: 0.0005723859127610922\n",
      "[2, 180] loss: 0.0006872186250984669\n",
      "[2, 200] loss: 0.0004929434973746539\n",
      "[2, 220] loss: 0.0005046845898032189\n",
      "[2, 240] loss: 0.0005455361176282167\n",
      "[2, 260] loss: 0.0005879000145941973\n",
      "[2, 280] loss: 0.0006231492357328534\n",
      "[2, 300] loss: 0.0005232371198944747\n",
      "[3, 20] loss: 0.0006763437390327454\n",
      "[3, 40] loss: 0.0004790929676964879\n",
      "[3, 60] loss: 0.0005459856577217579\n",
      "[3, 80] loss: 0.0005615692269057036\n",
      "[3, 100] loss: 0.0005780042130500078\n",
      "[3, 120] loss: 0.0004976474707946182\n",
      "[3, 140] loss: 0.0005885746264830231\n",
      "[3, 160] loss: 0.0004883112744428217\n",
      "[3, 180] loss: 0.000533210311550647\n",
      "[3, 200] loss: 0.0004826009180396795\n",
      "[3, 220] loss: 0.0005505596534349024\n",
      "[3, 240] loss: 0.0006670389128848911\n",
      "[3, 260] loss: 0.0005880691697821021\n",
      "[3, 280] loss: 0.0006000191122293473\n",
      "[3, 300] loss: 0.0006217156229540706\n",
      "[4, 20] loss: 0.0005227151950821281\n",
      "[4, 40] loss: 0.0006126206861808896\n",
      "[4, 60] loss: 0.0005278273187577725\n",
      "[4, 80] loss: 0.00044376255571842196\n",
      "[4, 100] loss: 0.0005332460934296251\n",
      "[4, 120] loss: 0.0004639899954199791\n",
      "[4, 140] loss: 0.0005795501573011279\n",
      "[4, 160] loss: 0.0005849826447665692\n",
      "[4, 180] loss: 0.0005772415208630264\n",
      "[4, 200] loss: 0.0005498866951093078\n",
      "[4, 220] loss: 0.0005083164116367698\n",
      "[4, 240] loss: 0.0006081370580941439\n",
      "[4, 260] loss: 0.0005045387521386146\n",
      "[4, 280] loss: 0.0005900949686765671\n",
      "[4, 300] loss: 0.0006031344439834356\n",
      "[5, 20] loss: 0.0005600014375522732\n",
      "[5, 40] loss: 0.0005885712811723352\n",
      "[5, 60] loss: 0.00045999042224138977\n",
      "[5, 80] loss: 0.000490723307710141\n",
      "[5, 100] loss: 0.0005365110170096159\n",
      "[5, 120] loss: 0.0005622474146075547\n",
      "[5, 140] loss: 0.000481177591253072\n",
      "[5, 160] loss: 0.0005379469357430934\n",
      "[5, 180] loss: 0.0005274299820885063\n",
      "[5, 200] loss: 0.0005639947997406126\n",
      "[5, 220] loss: 0.0005909952130168677\n",
      "[5, 240] loss: 0.0005010412577539683\n",
      "[5, 260] loss: 0.0005693945791572332\n",
      "[5, 280] loss: 0.00055536157079041\n",
      "[5, 300] loss: 0.000522371020168066\n",
      "[6, 20] loss: 0.0005452175224199891\n",
      "[6, 40] loss: 0.00045903315022587777\n",
      "[6, 60] loss: 0.0005523016676306725\n",
      "[6, 80] loss: 0.0004787145061418414\n",
      "[6, 100] loss: 0.0005516642834991216\n",
      "[6, 120] loss: 0.0004992805407382548\n",
      "[6, 140] loss: 0.0005060768816620111\n",
      "[6, 160] loss: 0.0005680106719955801\n",
      "[6, 180] loss: 0.0004749607564881444\n",
      "[6, 200] loss: 0.0005505152782425284\n",
      "[6, 220] loss: 0.0005511045372113586\n",
      "[6, 240] loss: 0.0005159659273922443\n",
      "[6, 260] loss: 0.0005782151641324163\n",
      "[6, 280] loss: 0.0004243526086211204\n",
      "[6, 300] loss: 0.0004899295759387314\n",
      "[7, 20] loss: 0.0004344201758503914\n",
      "[7, 40] loss: 0.0004672129908576608\n",
      "[7, 60] loss: 0.0004921621726825834\n",
      "[7, 80] loss: 0.000500955562107265\n",
      "[7, 100] loss: 0.0004427766581065953\n",
      "[7, 120] loss: 0.0004979471261613071\n",
      "[7, 140] loss: 0.0005978645551949739\n",
      "[7, 160] loss: 0.0005137870218604803\n",
      "[7, 180] loss: 0.0004949097353965044\n",
      "[7, 200] loss: 0.0005934455813840032\n",
      "[7, 220] loss: 0.0004966074051335453\n",
      "[7, 240] loss: 0.0004300751956179738\n",
      "[7, 260] loss: 0.00046238895133137706\n",
      "[7, 280] loss: 0.0006110331444069743\n",
      "[7, 300] loss: 0.0005097589641809464\n",
      "[8, 20] loss: 0.0005929021015763283\n",
      "[8, 40] loss: 0.00045165003929287196\n",
      "[8, 60] loss: 0.0005535669019445777\n",
      "[8, 80] loss: 0.0005045200875028968\n",
      "[8, 100] loss: 0.0005295088393613696\n",
      "[8, 120] loss: 0.0005319107538089156\n",
      "[8, 140] loss: 0.00044412848772481086\n",
      "[8, 160] loss: 0.0004416512893512845\n",
      "[8, 180] loss: 0.0004419649997726083\n",
      "[8, 200] loss: 0.0004845060254447162\n",
      "[8, 220] loss: 0.0005402313899248838\n",
      "[8, 240] loss: 0.00048672749660909174\n",
      "[8, 260] loss: 0.0004957431852817536\n",
      "[8, 280] loss: 0.0004456661930307746\n",
      "[8, 300] loss: 0.0005000597713515163\n",
      "[9, 20] loss: 0.0004948761975392699\n",
      "[9, 40] loss: 0.0004895320534706116\n",
      "[9, 60] loss: 0.000455465123988688\n",
      "[9, 80] loss: 0.00043103747349232434\n",
      "[9, 100] loss: 0.0004253123593516648\n",
      "[9, 120] loss: 0.00048210541438311336\n",
      "[9, 140] loss: 0.00047409825725480914\n",
      "[9, 160] loss: 0.00046525874501094223\n",
      "[9, 180] loss: 0.0004244289696216583\n",
      "[9, 200] loss: 0.0005048634158447385\n",
      "[9, 220] loss: 0.0005654252031818032\n",
      "[9, 240] loss: 0.0004978362526744604\n",
      "[9, 260] loss: 0.0005296485889703035\n",
      "[9, 280] loss: 0.000447359231300652\n",
      "[9, 300] loss: 0.0004696164168417454\n",
      "[10, 20] loss: 0.00042655241023749113\n",
      "[10, 40] loss: 0.0004921558015048504\n",
      "[10, 60] loss: 0.0004662403780966997\n",
      "[10, 80] loss: 0.00044347004406154153\n",
      "[10, 100] loss: 0.0005277091553434729\n",
      "[10, 120] loss: 0.0004766905540600419\n",
      "[10, 140] loss: 0.00044948387425392867\n",
      "[10, 160] loss: 0.0003644965561106801\n",
      "[10, 180] loss: 0.000627122575417161\n",
      "[10, 200] loss: 0.0004712388552725315\n",
      "[10, 220] loss: 0.000503275491297245\n",
      "[10, 240] loss: 0.0005312914685346186\n",
      "[10, 260] loss: 0.0004815002274699509\n",
      "[10, 280] loss: 0.00048331773187965154\n",
      "[10, 300] loss: 0.0003758778311312199\n",
      "[11, 20] loss: 0.0004281612141057849\n",
      "[11, 40] loss: 0.0004616882912814617\n",
      "[11, 60] loss: 0.0005436991779133678\n",
      "[11, 80] loss: 0.00043450171453878286\n",
      "[11, 100] loss: 0.0004513270505703986\n",
      "[11, 120] loss: 0.00041818152600899336\n",
      "[11, 140] loss: 0.0004656374268233776\n",
      "[11, 160] loss: 0.0004379143756814301\n",
      "[11, 180] loss: 0.000502127792686224\n",
      "[11, 200] loss: 0.0004158345297910273\n",
      "[11, 220] loss: 0.0004893293222412468\n",
      "[11, 240] loss: 0.00034344736859202385\n",
      "[11, 260] loss: 0.0005011999476701021\n",
      "[11, 280] loss: 0.00042695831228047607\n",
      "[11, 300] loss: 0.0005534013682045042\n",
      "[12, 20] loss: 0.0004015915337949991\n",
      "[12, 40] loss: 0.0004076596824452281\n",
      "[12, 60] loss: 0.00043150245514698326\n",
      "[12, 80] loss: 0.0004150813976302743\n",
      "[12, 100] loss: 0.00047874674433842304\n",
      "[12, 120] loss: 0.0003308373093605042\n",
      "[12, 140] loss: 0.00045581107959151266\n",
      "[12, 160] loss: 0.0005110415234230458\n",
      "[12, 180] loss: 0.0005265158116817474\n",
      "[12, 200] loss: 0.0004712127069942653\n",
      "[12, 220] loss: 0.0004892436806112528\n",
      "[12, 240] loss: 0.0004972059526480734\n",
      "[12, 260] loss: 0.00040028234384953973\n",
      "[12, 280] loss: 0.00047804163210093975\n",
      "[12, 300] loss: 0.0005050404276698828\n",
      "[13, 20] loss: 0.000489983812905848\n",
      "[13, 40] loss: 0.0003668384794145823\n",
      "[13, 60] loss: 0.00043401610665023325\n",
      "[13, 80] loss: 0.0004265152718871832\n",
      "[13, 100] loss: 0.0004382628919556737\n",
      "[13, 120] loss: 0.0004361007697880268\n",
      "[13, 140] loss: 0.000503761675208807\n",
      "[13, 160] loss: 0.0004174282778985798\n",
      "[13, 180] loss: 0.00048115124087780715\n",
      "[13, 200] loss: 0.0004891070951707661\n",
      "[13, 220] loss: 0.0003955024275928736\n",
      "[13, 240] loss: 0.00040240181889384985\n",
      "[13, 260] loss: 0.00040801032073795794\n",
      "[13, 280] loss: 0.00048561200499534605\n",
      "[13, 300] loss: 0.0004838571436703205\n",
      "[14, 20] loss: 0.0004082207726314664\n",
      "[14, 40] loss: 0.00042453015316277744\n",
      "[14, 60] loss: 0.0004278052090667188\n",
      "[14, 80] loss: 0.0004265911914408207\n",
      "[14, 100] loss: 0.00039615868544206023\n",
      "[14, 120] loss: 0.0003599369749426842\n",
      "[14, 140] loss: 0.0003662749128416181\n",
      "[14, 160] loss: 0.0004944332148879766\n",
      "[14, 180] loss: 0.00039313528873026373\n",
      "[14, 200] loss: 0.0004442601362243295\n",
      "[14, 220] loss: 0.00045287116896361115\n",
      "[14, 240] loss: 0.0004598165340721607\n",
      "[14, 260] loss: 0.0004558416032232344\n",
      "[14, 280] loss: 0.0004306425470858812\n",
      "[14, 300] loss: 0.00046038255328312516\n",
      "[15, 20] loss: 0.00039045754028484223\n",
      "[15, 40] loss: 0.0004224449400790036\n",
      "[15, 60] loss: 0.00039674176648259163\n",
      "[15, 80] loss: 0.0005040556080639362\n",
      "[15, 100] loss: 0.00043975734524428844\n",
      "[15, 120] loss: 0.00039075074438005687\n",
      "[15, 140] loss: 0.00035142396297305824\n",
      "[15, 160] loss: 0.00040547118429094554\n",
      "[15, 180] loss: 0.0005269229132682085\n",
      "[15, 200] loss: 0.0005035096760839224\n",
      "[15, 220] loss: 0.0004157774862833321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 240] loss: 0.0004092882517725229\n",
      "[15, 260] loss: 0.00038776806835085154\n",
      "[15, 280] loss: 0.0004309146022424102\n",
      "[15, 300] loss: 0.00036990816053003074\n",
      "[16, 20] loss: 0.00040172801027074455\n",
      "[16, 40] loss: 0.0004473083820194006\n",
      "[16, 60] loss: 0.0003166138557717204\n",
      "[16, 80] loss: 0.0004004784752614796\n",
      "[16, 100] loss: 0.0004455102402716875\n",
      "[16, 120] loss: 0.000468946386128664\n",
      "[16, 140] loss: 0.00036403168737888337\n",
      "[16, 160] loss: 0.00046338377520442007\n",
      "[16, 180] loss: 0.00039138597808778287\n",
      "[16, 200] loss: 0.00041086198668926956\n",
      "[16, 220] loss: 0.0004276456325314939\n",
      "[16, 240] loss: 0.0004526570960879326\n",
      "[16, 260] loss: 0.000488707528449595\n",
      "[16, 280] loss: 0.0005021094372496009\n",
      "[16, 300] loss: 0.00040513503458350895\n",
      "[17, 20] loss: 0.00048456576373428106\n",
      "[17, 40] loss: 0.0003779346025548875\n",
      "[17, 60] loss: 0.00044843075051903726\n",
      "[17, 80] loss: 0.0004043118520639837\n",
      "[17, 100] loss: 0.0005051882192492485\n",
      "[17, 120] loss: 0.0004244099222123623\n",
      "[17, 140] loss: 0.00036282245675101874\n",
      "[17, 160] loss: 0.0004340560077689588\n",
      "[17, 180] loss: 0.00034721253672614693\n",
      "[17, 200] loss: 0.0004620823422446847\n",
      "[17, 220] loss: 0.0004295433978550136\n",
      "[17, 240] loss: 0.0003255522162653506\n",
      "[17, 260] loss: 0.000414320407435298\n",
      "[17, 280] loss: 0.000400908128824085\n",
      "[17, 300] loss: 0.0004513907544314861\n",
      "[18, 20] loss: 0.0004165950338356197\n",
      "[18, 40] loss: 0.00040050435904413463\n",
      "[18, 60] loss: 0.0004114843998104334\n",
      "[18, 80] loss: 0.0003974463874474168\n",
      "[18, 100] loss: 0.00036596256820485\n",
      "[18, 120] loss: 0.0003573303511366248\n",
      "[18, 140] loss: 0.0005284478552639485\n",
      "[18, 160] loss: 0.00038658142276108264\n",
      "[18, 180] loss: 0.00031534369383007287\n",
      "[18, 200] loss: 0.0004469557218253613\n",
      "[18, 220] loss: 0.00040205667540431025\n",
      "[18, 240] loss: 0.0004351590797305107\n",
      "[18, 260] loss: 0.0004224602496251464\n",
      "[18, 280] loss: 0.0003834470654837787\n",
      "[18, 300] loss: 0.0005667690788395702\n",
      "[19, 20] loss: 0.00037582368310540916\n",
      "[19, 40] loss: 0.0004300864022225141\n",
      "[19, 60] loss: 0.00037310117715969683\n",
      "[19, 80] loss: 0.0003859096672385931\n",
      "[19, 100] loss: 0.0004377932893112302\n",
      "[19, 120] loss: 0.0004451912390068173\n",
      "[19, 140] loss: 0.00039021782763302325\n",
      "[19, 160] loss: 0.0003863530014641583\n",
      "[19, 180] loss: 0.0003854589257389307\n",
      "[19, 200] loss: 0.0003828730876557529\n",
      "[19, 220] loss: 0.0003399860896170139\n",
      "[19, 240] loss: 0.0004440893465653062\n",
      "[19, 260] loss: 0.00042444044165313246\n",
      "[19, 280] loss: 0.000377911034040153\n",
      "[19, 300] loss: 0.0004742761962115765\n",
      "[20, 20] loss: 0.0003742967052385211\n",
      "[20, 40] loss: 0.00039691861113533376\n",
      "[20, 60] loss: 0.00035808268655091526\n",
      "[20, 80] loss: 0.00036388567090034487\n",
      "[20, 100] loss: 0.00031301607144996525\n",
      "[20, 120] loss: 0.000444205105304718\n",
      "[20, 140] loss: 0.0004095647251233459\n",
      "[20, 160] loss: 0.0003316349401138723\n",
      "[20, 180] loss: 0.00035568922106176613\n",
      "[20, 200] loss: 0.00037358566792681814\n",
      "[20, 220] loss: 0.0003947940468788147\n",
      "[20, 240] loss: 0.00040678903460502627\n",
      "[20, 260] loss: 0.00041920614708215\n",
      "[20, 280] loss: 0.0004721935628913343\n",
      "[20, 300] loss: 0.00041513866931200025\n",
      "Finished Reraining\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # print(inputs.shape)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print(\"[{}, {}] loss: {}\".format\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Reraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"LeNET_x_y_MNIST_Model_My_Exiperiment_1_Fine_Tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, len(net.state_dict()[\"fc1.weight\"]))\n",
    "        self.fc2 = nn.Linear(len(net.state_dict()[\"fc1.weight\"]), len(net.state_dict()[\"fc2.weight\"]))\n",
    "        self.fc3 = nn.Linear(len(net.state_dict()[\"fc2.weight\"]), 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(\"LeNET_x_y_MNIST_Model_My_Exiperiment_1_Fine_Tuned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 240]          61,680\n",
      "            Linear-6                   [-1, 80]          19,280\n",
      "            Linear-7                   [-1, 10]             810\n",
      "================================================================\n",
      "Total params: 84,342\n",
      "Trainable params: 84,342\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.32\n",
      "Estimated Total Size (MB): 0.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device=torch.device(\"cpu\")\n",
    "model=Net().to(device)\n",
    "summary(model, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, number of parametes reduced down to 86k from 110 k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.420000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "[https://towardsdatascience.com/how-to-cluster-in-high-dimensions-4ef693bacc6] [1/11/2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
