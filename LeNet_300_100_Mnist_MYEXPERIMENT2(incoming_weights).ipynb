{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(\"LeNET_300_100_MNIST_Model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 300]          77,100\n",
      "            Linear-6                  [-1, 100]          30,100\n",
      "            Linear-7                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 110,782\n",
      "Trainable params: 110,782\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 0.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device=torch.device(\"cpu\")\n",
    "model=Net().to(device)\n",
    "summary(model, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([6, 1, 5, 5])\n",
      "conv1.weight \t tensor([[[[-0.0192,  0.2511,  0.2183,  0.0210,  0.0190],\n",
      "          [-0.0522,  0.3645,  0.3561,  0.5513,  0.3440],\n",
      "          [ 0.0598,  0.5070,  0.6116,  0.6220,  0.2953],\n",
      "          [ 0.2985,  0.1301,  0.4567,  0.1638,  0.2149],\n",
      "          [-0.0246,  0.3206,  0.1249,  0.1288, -0.1632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1775, -0.0977,  0.0400,  0.2623,  0.2931],\n",
      "          [ 0.0318, -0.1988, -0.1956,  0.1583,  0.1678],\n",
      "          [-0.2439, -0.2983, -0.0792,  0.2235,  0.3461],\n",
      "          [-0.0280, -0.2769, -0.1121,  0.0926,  0.3560],\n",
      "          [-0.1970, -0.1645,  0.1938,  0.2115,  0.1824]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1362,  0.2118, -0.2154, -0.2076, -0.2334],\n",
      "          [ 0.2467, -0.0821, -0.1670, -0.0738,  0.2066],\n",
      "          [ 0.2564,  0.1952,  0.2048,  0.3864,  0.0963],\n",
      "          [ 0.0786,  0.3546,  0.3418, -0.0168,  0.1037],\n",
      "          [ 0.0178, -0.2039, -0.1759, -0.0146, -0.0048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299,  0.1694, -0.0029, -0.1827, -0.0718],\n",
      "          [-0.1909, -0.1638,  0.1044, -0.2096,  0.0405],\n",
      "          [ 0.0873, -0.1777, -0.1820, -0.0258, -0.1887],\n",
      "          [ 0.0190, -0.1462, -0.0486,  0.0288, -0.2104],\n",
      "          [ 0.1727, -0.1630, -0.0454, -0.2031, -0.1915]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1378,  0.0087, -0.1978, -0.2727,  0.0345],\n",
      "          [ 0.2285, -0.0154, -0.1342, -0.2998, -0.0739],\n",
      "          [ 0.2801,  0.2191,  0.1079,  0.2947,  0.1320],\n",
      "          [ 0.3344,  0.4029,  0.3628,  0.5148,  0.0400],\n",
      "          [ 0.0682,  0.0300,  0.3382,  0.2907,  0.3091]]],\n",
      "\n",
      "\n",
      "        [[[-0.2584, -0.2308, -0.1856,  0.0330,  0.0465],\n",
      "          [-0.2978, -0.1939, -0.2057,  0.0400,  0.1692],\n",
      "          [ 0.0883, -0.1518,  0.0861, -0.1625,  0.0874],\n",
      "          [ 0.1464,  0.2543,  0.0292,  0.0581, -0.0674],\n",
      "          [ 0.3818,  0.3657,  0.2362, -0.0540,  0.1097]]]])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv1.bias \t tensor([ 0.2434, -0.0632,  0.2198,  0.1060,  0.1912,  0.0086])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.weight \t tensor([[[[-0.0109, -0.0548, -0.0268,  0.0555,  0.0529],\n",
      "          [-0.0747, -0.0093, -0.0871,  0.0756,  0.0290],\n",
      "          [ 0.0085, -0.0744, -0.1693, -0.1346, -0.1455],\n",
      "          [ 0.0382, -0.0446, -0.1455, -0.0654, -0.0830],\n",
      "          [ 0.0522,  0.1614,  0.1331,  0.1559,  0.0276]],\n",
      "\n",
      "         [[-0.0135,  0.0531,  0.0238,  0.0102,  0.0312],\n",
      "          [ 0.0628, -0.0327,  0.0206, -0.0319,  0.0428],\n",
      "          [-0.0834, -0.1235, -0.0557,  0.0262, -0.0169],\n",
      "          [ 0.0244, -0.0334,  0.0382, -0.0140,  0.0345],\n",
      "          [ 0.1239, -0.0369, -0.0981, -0.0913, -0.0969]],\n",
      "\n",
      "         [[-0.0242, -0.0831,  0.0583,  0.0632, -0.0662],\n",
      "          [-0.0261, -0.0701,  0.0334, -0.0154,  0.0068],\n",
      "          [-0.0244,  0.0824, -0.1057, -0.1036,  0.0335],\n",
      "          [-0.0140,  0.1386,  0.0705,  0.0171, -0.0072],\n",
      "          [ 0.0018,  0.1739,  0.1223,  0.0542,  0.0006]],\n",
      "\n",
      "         [[ 0.0685,  0.0419, -0.0685, -0.0128, -0.0514],\n",
      "          [ 0.0531,  0.0907, -0.0022,  0.0279,  0.0706],\n",
      "          [ 0.0644,  0.0581,  0.0447,  0.0862, -0.0299],\n",
      "          [-0.0563, -0.0664,  0.0204, -0.0461, -0.0049],\n",
      "          [-0.0201, -0.0669, -0.0425, -0.0638,  0.0590]],\n",
      "\n",
      "         [[-0.0472,  0.0201, -0.0225, -0.0141, -0.0058],\n",
      "          [ 0.0624,  0.0067,  0.0042,  0.0046,  0.0419],\n",
      "          [ 0.0394,  0.0676, -0.0732, -0.1638, -0.0343],\n",
      "          [ 0.1362,  0.1999,  0.0978,  0.1239,  0.0379],\n",
      "          [ 0.0652,  0.1395,  0.1929,  0.1191,  0.0623]],\n",
      "\n",
      "         [[ 0.0314, -0.0123, -0.0633,  0.0803,  0.0407],\n",
      "          [-0.0420,  0.0859,  0.0217, -0.0667,  0.0375],\n",
      "          [ 0.0298,  0.0806,  0.0096,  0.0557, -0.0537],\n",
      "          [ 0.0555, -0.0591,  0.0920,  0.1185,  0.0150],\n",
      "          [-0.0733, -0.1120, -0.0059,  0.1112,  0.0869]]],\n",
      "\n",
      "\n",
      "        [[[-0.0222, -0.0632,  0.0230,  0.0518,  0.0471],\n",
      "          [-0.0131, -0.0666, -0.0177, -0.0039,  0.0601],\n",
      "          [ 0.0307,  0.0119,  0.1052, -0.0188,  0.0316],\n",
      "          [-0.0572, -0.0849, -0.0043,  0.1368,  0.0699],\n",
      "          [-0.0658, -0.1288, -0.1282, -0.0940, -0.0316]],\n",
      "\n",
      "         [[ 0.0440,  0.0498,  0.0480,  0.0980,  0.0044],\n",
      "          [-0.0486,  0.0267,  0.0349, -0.0436,  0.0280],\n",
      "          [-0.0089,  0.0524,  0.0789, -0.0944, -0.0586],\n",
      "          [-0.0659,  0.0035,  0.0432, -0.0714, -0.0228],\n",
      "          [ 0.0241, -0.0759, -0.0205,  0.0458, -0.0653]],\n",
      "\n",
      "         [[-0.0281,  0.0291, -0.0404,  0.0430, -0.0491],\n",
      "          [ 0.0727, -0.0770, -0.0016,  0.0005,  0.0026],\n",
      "          [-0.0638,  0.0402,  0.0268,  0.0278, -0.0408],\n",
      "          [-0.0593, -0.0857, -0.0497,  0.1150,  0.1128],\n",
      "          [-0.0520, -0.0732, -0.0377, -0.0443, -0.0022]],\n",
      "\n",
      "         [[-0.0036, -0.0737,  0.0009,  0.0342,  0.0625],\n",
      "          [ 0.0713, -0.0301, -0.0545, -0.0360, -0.0155],\n",
      "          [ 0.0323,  0.0786, -0.0489, -0.0150, -0.0732],\n",
      "          [-0.0087,  0.0101,  0.0736, -0.0299,  0.0564],\n",
      "          [ 0.0291, -0.0514,  0.0165, -0.0648, -0.0488]],\n",
      "\n",
      "         [[-0.0781,  0.0070, -0.0439,  0.0522,  0.0101],\n",
      "          [-0.0655, -0.0328,  0.0275, -0.0029, -0.0499],\n",
      "          [-0.0462,  0.0460,  0.0361,  0.1371,  0.0840],\n",
      "          [ 0.0280, -0.0768,  0.0399,  0.0632,  0.1553],\n",
      "          [ 0.0167,  0.0032, -0.0908, -0.1111, -0.0763]],\n",
      "\n",
      "         [[ 0.0044,  0.0608, -0.0532, -0.0601,  0.0587],\n",
      "          [-0.0058, -0.0142, -0.0047, -0.0417, -0.0031],\n",
      "          [-0.0824, -0.0439, -0.0758, -0.0655, -0.0323],\n",
      "          [ 0.0141, -0.0282,  0.0304,  0.0188, -0.0538],\n",
      "          [-0.0402,  0.0069,  0.0732,  0.0188, -0.0666]]],\n",
      "\n",
      "\n",
      "        [[[-0.0290, -0.0331, -0.0362, -0.0128, -0.1624],\n",
      "          [ 0.0264,  0.0704,  0.0559,  0.0057, -0.1444],\n",
      "          [ 0.0334, -0.0197,  0.0786,  0.1296, -0.0665],\n",
      "          [-0.1325, -0.0002, -0.0102,  0.0673,  0.1488],\n",
      "          [ 0.0281,  0.0752,  0.1247,  0.0654,  0.0795]],\n",
      "\n",
      "         [[ 0.0432,  0.0453, -0.0163, -0.0221, -0.0871],\n",
      "          [ 0.0297,  0.0078, -0.0213, -0.0102,  0.0096],\n",
      "          [-0.0610,  0.0428,  0.1683,  0.1447,  0.1145],\n",
      "          [ 0.0129,  0.1065,  0.1422,  0.0659,  0.0251],\n",
      "          [-0.0014,  0.0767,  0.0355,  0.0775,  0.0778]],\n",
      "\n",
      "         [[-0.0408,  0.0344,  0.0030, -0.0213, -0.0086],\n",
      "          [ 0.0332,  0.0192,  0.0254,  0.0032,  0.0026],\n",
      "          [-0.0381, -0.0148,  0.0117,  0.0037,  0.0669],\n",
      "          [ 0.0096, -0.0044,  0.0353, -0.0138,  0.0783],\n",
      "          [ 0.0304,  0.0076,  0.0405,  0.0741,  0.0048]],\n",
      "\n",
      "         [[-0.0272,  0.0014, -0.0568,  0.0614,  0.0953],\n",
      "          [ 0.0689, -0.0018,  0.0250,  0.0122,  0.0488],\n",
      "          [ 0.0131, -0.0347,  0.0192,  0.0197, -0.0680],\n",
      "          [ 0.0553, -0.0440, -0.0922, -0.0444,  0.0591],\n",
      "          [ 0.0409, -0.0933,  0.0049, -0.0250,  0.0385]],\n",
      "\n",
      "         [[ 0.0040,  0.0977,  0.0278,  0.0440, -0.1256],\n",
      "          [ 0.0426,  0.0925,  0.1221,  0.1534, -0.0139],\n",
      "          [-0.0277,  0.0244, -0.0303,  0.1574,  0.0935],\n",
      "          [-0.0314,  0.0749,  0.0602,  0.0665,  0.0982],\n",
      "          [ 0.0176,  0.0285,  0.0090,  0.0930, -0.0757]],\n",
      "\n",
      "         [[-0.0138,  0.1097,  0.0643,  0.1291,  0.1020],\n",
      "          [ 0.0214,  0.0248, -0.0334,  0.1013,  0.0531],\n",
      "          [-0.0008, -0.0636,  0.0248, -0.0394, -0.0282],\n",
      "          [ 0.1024,  0.0172,  0.0456, -0.0545,  0.0182],\n",
      "          [-0.0197,  0.1342,  0.0421,  0.0906,  0.0427]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0875,  0.0322,  0.0375,  0.0408,  0.0054],\n",
      "          [-0.0460, -0.0633, -0.0649,  0.0760, -0.0567],\n",
      "          [ 0.0322, -0.0105, -0.0606, -0.0797, -0.0679],\n",
      "          [-0.0232, -0.0553, -0.0724,  0.0120, -0.0809],\n",
      "          [ 0.0408, -0.0291, -0.0568, -0.0558, -0.0341]],\n",
      "\n",
      "         [[-0.0568, -0.0631,  0.0572, -0.0491, -0.0536],\n",
      "          [-0.0138,  0.0440,  0.0787,  0.0738,  0.0220],\n",
      "          [-0.0163,  0.0168, -0.0073,  0.0200, -0.0652],\n",
      "          [ 0.0009,  0.0010, -0.0487, -0.0715,  0.0072],\n",
      "          [-0.0719, -0.0316, -0.0134, -0.0032,  0.0724]],\n",
      "\n",
      "         [[ 0.0582, -0.0085,  0.0132, -0.0703,  0.0159],\n",
      "          [ 0.0671, -0.0448,  0.0393, -0.0356,  0.0394],\n",
      "          [ 0.0450,  0.0304, -0.0662,  0.0135,  0.0431],\n",
      "          [-0.0309,  0.0268, -0.0411, -0.0587,  0.0662],\n",
      "          [ 0.0126,  0.0711, -0.0226, -0.0170,  0.0568]],\n",
      "\n",
      "         [[ 0.0282, -0.0763, -0.0282, -0.0218, -0.0370],\n",
      "          [ 0.0586, -0.0726, -0.0211, -0.0571, -0.0420],\n",
      "          [ 0.0335,  0.0127,  0.0681,  0.0412,  0.0192],\n",
      "          [-0.0766, -0.0016, -0.0073,  0.0633, -0.0200],\n",
      "          [-0.0027, -0.0093,  0.0077, -0.0436,  0.0106]],\n",
      "\n",
      "         [[-0.0392,  0.0598, -0.0669,  0.0659, -0.0636],\n",
      "          [-0.0772, -0.0788, -0.0499, -0.0290,  0.0244],\n",
      "          [-0.0204, -0.0283,  0.0083, -0.0568,  0.0490],\n",
      "          [ 0.0234, -0.0123, -0.0521,  0.0399, -0.0175],\n",
      "          [ 0.0072, -0.0435, -0.0295, -0.0564, -0.0306]],\n",
      "\n",
      "         [[ 0.0560, -0.0579, -0.0484,  0.0134,  0.0548],\n",
      "          [-0.0322,  0.0032,  0.0593, -0.0421, -0.0552],\n",
      "          [ 0.0751,  0.0288, -0.0811, -0.0192, -0.0289],\n",
      "          [-0.0103,  0.0337, -0.0256, -0.0466,  0.0394],\n",
      "          [ 0.0175,  0.0412, -0.0659, -0.0206,  0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340,  0.0957,  0.0862, -0.0030,  0.0326],\n",
      "          [ 0.0625,  0.1338,  0.0669, -0.0977,  0.0272],\n",
      "          [ 0.1258,  0.2758,  0.0864, -0.1109, -0.1041],\n",
      "          [ 0.1510,  0.1942,  0.0006, -0.1443, -0.1476],\n",
      "          [ 0.1227,  0.0747, -0.0599, -0.0265, -0.0703]],\n",
      "\n",
      "         [[ 0.0696,  0.0604,  0.0966,  0.0341, -0.0003],\n",
      "          [ 0.2600,  0.1970,  0.0654, -0.0413,  0.0700],\n",
      "          [ 0.2760,  0.1238,  0.0613,  0.0552, -0.0365],\n",
      "          [ 0.2264,  0.0426,  0.0530, -0.0057,  0.0397],\n",
      "          [ 0.0955,  0.0771,  0.0420, -0.0585, -0.0408]],\n",
      "\n",
      "         [[-0.0805, -0.0571,  0.0686,  0.0679,  0.0019],\n",
      "          [ 0.0002, -0.0732,  0.0858,  0.0114, -0.0765],\n",
      "          [-0.0982,  0.0699,  0.1299,  0.0603, -0.0301],\n",
      "          [-0.0205,  0.0617,  0.0674,  0.0328,  0.0286],\n",
      "          [-0.0963,  0.0384,  0.1049,  0.1011,  0.0627]],\n",
      "\n",
      "         [[ 0.0357,  0.0340,  0.0522,  0.0495,  0.0708],\n",
      "          [-0.0845, -0.0224, -0.0304, -0.0551, -0.0190],\n",
      "          [-0.0668,  0.0409, -0.0619, -0.0562,  0.0009],\n",
      "          [ 0.0180, -0.0264,  0.0483,  0.0653,  0.0173],\n",
      "          [ 0.0295,  0.0553,  0.0585, -0.0805,  0.0243]],\n",
      "\n",
      "         [[ 0.1076, -0.0023, -0.0163, -0.0140,  0.0564],\n",
      "          [ 0.0750,  0.0792, -0.0273,  0.0257, -0.0842],\n",
      "          [ 0.0222,  0.0533, -0.0139, -0.0419, -0.0244],\n",
      "          [ 0.0319,  0.0712,  0.0382,  0.0173,  0.0096],\n",
      "          [-0.0934,  0.0405,  0.1524,  0.0470,  0.0904]],\n",
      "\n",
      "         [[-0.0850,  0.0575,  0.0297,  0.0397,  0.0481],\n",
      "          [-0.0801, -0.0908, -0.0910, -0.0494,  0.0574],\n",
      "          [ 0.1137, -0.0567, -0.1489, -0.0140,  0.0657],\n",
      "          [ 0.0604, -0.0574, -0.0979,  0.0109,  0.0994],\n",
      "          [ 0.0521, -0.0905,  0.0108,  0.0659,  0.0968]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1651,  0.1899,  0.1213,  0.0844,  0.0381],\n",
      "          [ 0.0872,  0.0726,  0.1388,  0.1072,  0.0381],\n",
      "          [-0.0822, -0.0235, -0.0253,  0.0575,  0.0126],\n",
      "          [-0.1143, -0.0686,  0.0064,  0.0676,  0.0519],\n",
      "          [-0.0414, -0.0143, -0.0119,  0.0260,  0.0315]],\n",
      "\n",
      "         [[-0.0832, -0.0186,  0.0401, -0.0030,  0.0215],\n",
      "          [-0.0418,  0.0453,  0.0292,  0.0065,  0.0488],\n",
      "          [ 0.0127, -0.1047,  0.0074,  0.0770,  0.0699],\n",
      "          [-0.1040,  0.0111, -0.0547,  0.0259,  0.0403],\n",
      "          [-0.0534, -0.0387, -0.0023,  0.0301, -0.0905]],\n",
      "\n",
      "         [[ 0.0518,  0.1405,  0.1145,  0.0954, -0.0037],\n",
      "          [-0.0601,  0.0425, -0.0251,  0.0921, -0.0411],\n",
      "          [-0.0104, -0.0991, -0.0232, -0.0040,  0.0175],\n",
      "          [ 0.0217, -0.0569, -0.0210, -0.0877,  0.0530],\n",
      "          [ 0.0366, -0.0513,  0.0025, -0.0891,  0.0799]],\n",
      "\n",
      "         [[ 0.0528,  0.0358, -0.0843,  0.0293,  0.0443],\n",
      "          [ 0.0218,  0.0180, -0.0699, -0.0587,  0.0115],\n",
      "          [ 0.0040,  0.0050,  0.0047,  0.0408,  0.0669],\n",
      "          [ 0.0741, -0.0620,  0.0226,  0.0472,  0.0682],\n",
      "          [ 0.0717,  0.0853, -0.0000, -0.0112, -0.0270]],\n",
      "\n",
      "         [[ 0.1038,  0.1426,  0.0998,  0.0320,  0.1111],\n",
      "          [-0.1177, -0.0672,  0.0677,  0.1034, -0.0451],\n",
      "          [-0.0871, -0.1239, -0.0099, -0.0290, -0.0613],\n",
      "          [-0.0878, -0.0596, -0.0144, -0.0210,  0.0641],\n",
      "          [-0.0289,  0.0519, -0.0234,  0.0174,  0.0056]],\n",
      "\n",
      "         [[-0.0465,  0.0461,  0.0285,  0.0367,  0.0337],\n",
      "          [ 0.0520, -0.0826, -0.0852, -0.0947, -0.1000],\n",
      "          [ 0.0594,  0.0626,  0.0371,  0.0097, -0.0623],\n",
      "          [ 0.0788, -0.0128,  0.0158, -0.0668, -0.0158],\n",
      "          [-0.0014,  0.0584, -0.0293, -0.0480, -0.0613]]]])\n",
      "conv2.bias \t torch.Size([16])\n",
      "conv2.bias \t tensor([ 0.0730,  0.0296,  0.0032,  0.0657,  0.0553,  0.0152, -0.0441, -0.0667,\n",
      "         0.0277,  0.0222, -0.0358,  0.0246,  0.0430, -0.0688, -0.0241,  0.0027])\n",
      "fc1.weight \t torch.Size([300, 256])\n",
      "fc1.weight \t tensor([[ 0.0272,  0.0675,  0.0587,  ...,  0.0256, -0.0110, -0.0366],\n",
      "        [ 0.0592,  0.0532,  0.0201,  ..., -0.0201, -0.0062,  0.0536],\n",
      "        [-0.0039, -0.0355, -0.0470,  ...,  0.0263, -0.0019, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0400,  0.0265, -0.0372,  ..., -0.0495,  0.0229,  0.0028],\n",
      "        [-0.0405,  0.0220, -0.0051,  ...,  0.0582, -0.0126, -0.0565],\n",
      "        [ 0.0613,  0.0368, -0.0311,  ..., -0.0378, -0.0014,  0.0345]])\n",
      "fc1.bias \t torch.Size([300])\n",
      "fc1.bias \t tensor([ 0.0125,  0.0496,  0.0088,  0.0085,  0.0095, -0.0318, -0.0080,  0.0076,\n",
      "         0.0534, -0.0552, -0.0182, -0.0311, -0.0108, -0.0078, -0.0341,  0.0239,\n",
      "        -0.0408, -0.0497, -0.0399, -0.0287,  0.0010, -0.0028,  0.0587,  0.0291,\n",
      "        -0.0397,  0.0408, -0.0342,  0.0057,  0.0011,  0.0218, -0.0139,  0.0099,\n",
      "        -0.0637, -0.0040,  0.0283, -0.0100, -0.0033,  0.0194,  0.0250,  0.0024,\n",
      "        -0.0618,  0.0368, -0.0075, -0.0286, -0.0415,  0.0529,  0.0308, -0.0437,\n",
      "         0.0435,  0.0390, -0.0456, -0.0025,  0.0445, -0.0515,  0.0301, -0.0621,\n",
      "        -0.0068,  0.0022, -0.0306,  0.0367, -0.0544, -0.0037, -0.0045,  0.0001,\n",
      "         0.0466,  0.0304,  0.0454,  0.0361,  0.0541, -0.0258, -0.0537, -0.0621,\n",
      "        -0.0620,  0.0021,  0.0354,  0.0299,  0.0376,  0.0373, -0.0519,  0.0416,\n",
      "         0.0261, -0.0177, -0.0317,  0.0305, -0.0425,  0.0313,  0.0406,  0.0041,\n",
      "         0.0194,  0.0046,  0.0000, -0.0381,  0.0463,  0.0631,  0.0508, -0.0166,\n",
      "        -0.0199, -0.0257, -0.0614, -0.0028,  0.0009, -0.0030, -0.0501, -0.0544,\n",
      "         0.0413, -0.0580,  0.0460,  0.0103,  0.0018,  0.0094,  0.0230,  0.0387,\n",
      "        -0.0001,  0.0430, -0.0575,  0.0499, -0.0169,  0.0482,  0.0148,  0.0434,\n",
      "         0.0406, -0.0329, -0.0347, -0.0067,  0.0056,  0.0384,  0.0232,  0.0367,\n",
      "        -0.0032,  0.0292, -0.0203, -0.0486,  0.0126, -0.0531,  0.0207,  0.0398,\n",
      "         0.0525, -0.0629,  0.0590, -0.0428, -0.0445, -0.0290, -0.0070, -0.0509,\n",
      "         0.0348,  0.0296, -0.0262, -0.0463,  0.0530, -0.0203,  0.0406, -0.0518,\n",
      "         0.0414,  0.0028,  0.0606,  0.0647, -0.0044,  0.0148, -0.0470,  0.0146,\n",
      "        -0.0530, -0.0190,  0.0125,  0.0080, -0.0501,  0.0331, -0.0305, -0.0425,\n",
      "         0.0532, -0.0100,  0.0302, -0.0257,  0.0368,  0.0359,  0.0030, -0.0122,\n",
      "        -0.0206, -0.0331,  0.0122, -0.0050,  0.0362, -0.0105,  0.0027, -0.0145,\n",
      "        -0.0539,  0.0082,  0.0559, -0.0274, -0.0482, -0.0504, -0.0238,  0.0578,\n",
      "         0.0111, -0.0489,  0.0005,  0.0606, -0.0305,  0.0339,  0.0619,  0.0450,\n",
      "        -0.0125,  0.0277, -0.0592, -0.0471, -0.0499, -0.0444, -0.0058,  0.0355,\n",
      "         0.0439, -0.0299, -0.0286, -0.0260, -0.0086,  0.0275,  0.0235,  0.0480,\n",
      "         0.0576, -0.0442, -0.0399,  0.0315,  0.0131, -0.0080,  0.0349,  0.0340,\n",
      "        -0.0147, -0.0120, -0.0187,  0.0264,  0.0293,  0.0585,  0.0324, -0.0166,\n",
      "        -0.0555,  0.0493, -0.0615, -0.0557,  0.0566, -0.0102,  0.0540, -0.0159,\n",
      "        -0.0254, -0.0540, -0.0359, -0.0339,  0.0491, -0.0252,  0.0568,  0.0199,\n",
      "        -0.0021, -0.0235, -0.0565,  0.0607,  0.0233, -0.0354,  0.0298,  0.0329,\n",
      "         0.0625, -0.0587,  0.0386, -0.0303,  0.0186, -0.0189,  0.0408, -0.0094,\n",
      "         0.0109, -0.0002, -0.0109, -0.0081,  0.0345, -0.0408, -0.0037, -0.0351,\n",
      "         0.0441,  0.0274, -0.0352,  0.0627, -0.0417,  0.0531, -0.0357,  0.0050,\n",
      "         0.0328, -0.0356,  0.0408, -0.0267,  0.0032, -0.0200,  0.0475, -0.0275,\n",
      "        -0.0526,  0.0087,  0.0170,  0.0063,  0.0208, -0.0253, -0.0119,  0.0515,\n",
      "        -0.0609, -0.0366,  0.0094,  0.0507])\n",
      "fc2.weight \t torch.Size([100, 300])\n",
      "fc2.weight \t tensor([[ 0.0402, -0.0434,  0.0285,  ..., -0.0308, -0.0236,  0.0146],\n",
      "        [ 0.0027,  0.0438, -0.0360,  ...,  0.0140,  0.0560,  0.0263],\n",
      "        [ 0.0155, -0.0348,  0.0268,  ...,  0.0200,  0.0367,  0.0531],\n",
      "        ...,\n",
      "        [ 0.0570, -0.0697,  0.0352,  ..., -0.0380, -0.0513, -0.0107],\n",
      "        [-0.0099,  0.0150,  0.0478,  ...,  0.0631, -0.0225,  0.0330],\n",
      "        [-0.0361,  0.0464, -0.0538,  ...,  0.0255, -0.0425, -0.0736]])\n",
      "fc2.bias \t torch.Size([100])\n",
      "fc2.bias \t tensor([ 0.0397, -0.0087, -0.0203,  0.0181,  0.0399, -0.0314, -0.0457,  0.0263,\n",
      "        -0.0335,  0.0092, -0.0015, -0.0373,  0.0526,  0.0600,  0.0099, -0.0135,\n",
      "         0.0540,  0.0312,  0.0291,  0.0140,  0.0042,  0.0081, -0.0266,  0.0049,\n",
      "         0.0078,  0.0362, -0.0094,  0.0693,  0.0406,  0.0248,  0.0119, -0.0411,\n",
      "         0.0599, -0.0495, -0.0417, -0.0585, -0.0123,  0.0157, -0.0193, -0.0215,\n",
      "         0.0462, -0.0107, -0.0284,  0.0102, -0.0230,  0.0080, -0.0122, -0.0338,\n",
      "         0.0525, -0.0359,  0.0041,  0.0639,  0.0477,  0.0078,  0.0393,  0.0211,\n",
      "        -0.0103, -0.0084, -0.0399, -0.0345, -0.0042, -0.0272,  0.0004,  0.0318,\n",
      "        -0.0283,  0.0067, -0.0566,  0.0388,  0.0067,  0.0128, -0.0366, -0.0135,\n",
      "        -0.0255, -0.0400,  0.0588, -0.0205,  0.0412,  0.0009,  0.0443,  0.0374,\n",
      "         0.0142,  0.0051,  0.0115,  0.0211,  0.0548,  0.0299,  0.0294, -0.0427,\n",
      "        -0.0211, -0.0392,  0.0204,  0.0366, -0.0392, -0.0465, -0.0501, -0.0336,\n",
      "        -0.0278,  0.0443,  0.0489, -0.0206])\n",
      "fc3.weight \t torch.Size([10, 100])\n",
      "fc3.weight \t tensor([[-0.0799, -0.0514,  0.0154,  0.0339, -0.0871, -0.0275,  0.0830, -0.0554,\n",
      "         -0.0946, -0.0866, -0.1241,  0.0442, -0.0847,  0.1099, -0.0807, -0.0870,\n",
      "         -0.0052,  0.0141, -0.1187,  0.0135, -0.0574,  0.0641,  0.0971,  0.1102,\n",
      "         -0.0481, -0.1465, -0.0263, -0.1137, -0.0913,  0.1041,  0.0754, -0.0701,\n",
      "         -0.0155,  0.1341, -0.0931, -0.0810,  0.0836,  0.0484,  0.1427, -0.1458,\n",
      "         -0.1428,  0.0642,  0.0834, -0.0122,  0.0128, -0.0866,  0.1009, -0.0585,\n",
      "          0.1132, -0.0641,  0.0977,  0.0277,  0.0624, -0.0949, -0.0213,  0.1714,\n",
      "         -0.1392, -0.0815, -0.0149,  0.1095, -0.0326, -0.0129,  0.1646,  0.0889,\n",
      "         -0.1588, -0.0397, -0.0864,  0.0607,  0.0431,  0.0588, -0.0666,  0.0588,\n",
      "          0.0749,  0.0891, -0.0775,  0.0067, -0.0723,  0.0007, -0.1620, -0.0965,\n",
      "         -0.1363,  0.0115,  0.0796,  0.0139,  0.0489, -0.0417, -0.0408,  0.1485,\n",
      "          0.0496,  0.1629, -0.0059, -0.0451, -0.0528,  0.1137, -0.0703,  0.1330,\n",
      "         -0.0167, -0.0833, -0.0043,  0.0967],\n",
      "        [ 0.0605, -0.0714, -0.1559, -0.0261, -0.0907, -0.0269,  0.0549, -0.1048,\n",
      "          0.1263, -0.1483,  0.1830, -0.1227,  0.1579,  0.0976, -0.0630, -0.0642,\n",
      "          0.0642,  0.0171,  0.0748, -0.0418, -0.0341,  0.1174,  0.1188, -0.1357,\n",
      "         -0.0569,  0.1670,  0.1514,  0.1882, -0.0166, -0.0848, -0.0153,  0.0541,\n",
      "          0.1883, -0.0972, -0.0972, -0.0510, -0.1072, -0.0235, -0.0642,  0.1352,\n",
      "          0.0150, -0.0173, -0.1752,  0.1126, -0.0674,  0.0020,  0.0317, -0.0967,\n",
      "          0.1348, -0.1442, -0.0774, -0.1404,  0.1202, -0.0748, -0.1379,  0.0576,\n",
      "          0.1114, -0.0876,  0.1180, -0.1181,  0.1069,  0.1197,  0.1634, -0.0131,\n",
      "          0.1384, -0.0552,  0.0004, -0.0252, -0.0005, -0.0646, -0.0370, -0.0633,\n",
      "         -0.0284,  0.0869, -0.1731,  0.1347, -0.1184, -0.1579, -0.0766,  0.0333,\n",
      "          0.0633, -0.0766, -0.0020,  0.0931, -0.0060, -0.0381,  0.0882,  0.0496,\n",
      "         -0.0383, -0.0410,  0.0548, -0.0701, -0.0373, -0.1803,  0.1029, -0.0843,\n",
      "         -0.0158, -0.1036,  0.0492, -0.0733],\n",
      "        [ 0.1163,  0.0529,  0.0872, -0.0351, -0.0968,  0.1147,  0.0832, -0.0014,\n",
      "         -0.0788,  0.0679,  0.0367,  0.0653,  0.2598, -0.0424, -0.0725,  0.0351,\n",
      "          0.0587,  0.0893, -0.1037, -0.0686, -0.0598, -0.0415,  0.1333,  0.0031,\n",
      "          0.0858, -0.0589, -0.0039, -0.0715,  0.0809, -0.0254,  0.0300, -0.1061,\n",
      "          0.1893,  0.0260, -0.0539,  0.0239,  0.2075, -0.0787, -0.1141,  0.0259,\n",
      "          0.0277, -0.0784, -0.0361, -0.0954,  0.1004, -0.0387, -0.0915, -0.1756,\n",
      "          0.0812,  0.0002,  0.0163,  0.1229,  0.0020, -0.0592,  0.0071,  0.0194,\n",
      "         -0.0185, -0.1658, -0.0266, -0.0476, -0.0040, -0.1298,  0.0797, -0.0132,\n",
      "         -0.0125, -0.0713,  0.0886,  0.0654, -0.1828,  0.1266, -0.0312, -0.1565,\n",
      "         -0.0008,  0.0433, -0.0532, -0.0615,  0.0067,  0.2707, -0.1773, -0.0664,\n",
      "          0.0780,  0.1225, -0.0906, -0.0346, -0.1364, -0.1227, -0.0573,  0.0918,\n",
      "         -0.1081,  0.1082, -0.0558, -0.0893,  0.0545, -0.0031,  0.0543, -0.1526,\n",
      "         -0.1887,  0.0908, -0.0388,  0.1185],\n",
      "        [-0.1174, -0.0956, -0.0778, -0.0839, -0.1300,  0.1048, -0.0927,  0.0063,\n",
      "         -0.0726, -0.1231,  0.0053,  0.0911, -0.0725, -0.1458, -0.0792,  0.0743,\n",
      "          0.0184,  0.0899,  0.2005, -0.1634, -0.0627, -0.0126, -0.0801, -0.0196,\n",
      "         -0.1274, -0.1908, -0.0943,  0.0627, -0.0222,  0.0798,  0.0219, -0.1379,\n",
      "          0.1264,  0.0201,  0.1231, -0.0776, -0.0175,  0.0741,  0.2067,  0.1731,\n",
      "          0.1828,  0.0337, -0.0757,  0.1103,  0.0470, -0.0247,  0.0363, -0.1289,\n",
      "         -0.1138,  0.1339,  0.0701,  0.0968, -0.1750, -0.0275, -0.1629, -0.1297,\n",
      "          0.1120, -0.0651, -0.0119, -0.0819, -0.0517,  0.1247, -0.0554,  0.0212,\n",
      "         -0.1113,  0.0962, -0.1201, -0.0136,  0.1305, -0.0559, -0.1068, -0.1020,\n",
      "         -0.0739,  0.0870,  0.1114, -0.0510, -0.0069,  0.1589, -0.0137, -0.0609,\n",
      "          0.0888,  0.0635, -0.0432, -0.0050, -0.0781,  0.0995, -0.0672,  0.0887,\n",
      "         -0.0514, -0.0577, -0.1168,  0.0129, -0.0206,  0.0918,  0.0148, -0.0447,\n",
      "          0.0651,  0.0980, -0.0730,  0.1064],\n",
      "        [ 0.1344, -0.0527, -0.1721,  0.0228,  0.0103, -0.0212,  0.0978, -0.0764,\n",
      "         -0.0590,  0.0773,  0.0813,  0.1136,  0.0045, -0.0662,  0.0648, -0.1574,\n",
      "          0.0122, -0.0855, -0.0298, -0.1251,  0.0770,  0.1594,  0.1165, -0.0861,\n",
      "         -0.1141, -0.0640, -0.1431, -0.0167,  0.0222, -0.0586, -0.1808,  0.0198,\n",
      "         -0.1278,  0.0701, -0.1002,  0.0631, -0.0676, -0.0541,  0.0505, -0.0883,\n",
      "         -0.0518,  0.0285,  0.0013, -0.1970,  0.0006,  0.0546, -0.0142,  0.0711,\n",
      "         -0.0467, -0.0935, -0.0555, -0.0608,  0.0216,  0.0222,  0.0968, -0.0574,\n",
      "          0.0568,  0.0914,  0.1699,  0.1728,  0.2329, -0.0513,  0.0553,  0.0158,\n",
      "          0.0237, -0.1188,  0.1711, -0.0002,  0.0112,  0.0978,  0.0454,  0.0862,\n",
      "          0.0530,  0.0608,  0.0068, -0.1118,  0.0083, -0.1468,  0.1673,  0.0777,\n",
      "          0.0660,  0.0521,  0.0632, -0.0286,  0.1892, -0.1432,  0.0500,  0.0170,\n",
      "          0.0163,  0.0035, -0.1290,  0.0328, -0.0114, -0.1939,  0.1007,  0.0200,\n",
      "         -0.0886, -0.0748, -0.0414, -0.0459],\n",
      "        [-0.0702,  0.0972, -0.1089, -0.0164,  0.1510,  0.1673,  0.0355, -0.0335,\n",
      "          0.0098, -0.0335, -0.0313,  0.0474, -0.2167,  0.0931,  0.0591,  0.1774,\n",
      "          0.0661, -0.1510, -0.0437,  0.0292,  0.0459,  0.0118, -0.1072,  0.0775,\n",
      "          0.1941,  0.0081, -0.0666,  0.1146,  0.0869, -0.0447,  0.0142,  0.1406,\n",
      "         -0.0295,  0.0406,  0.1028,  0.1352,  0.0855, -0.0151, -0.0241, -0.0670,\n",
      "          0.0759,  0.0260, -0.1360,  0.1547, -0.0115, -0.0346,  0.1122,  0.0071,\n",
      "         -0.1181, -0.0383, -0.0050,  0.0827,  0.0534,  0.0518, -0.1492,  0.0719,\n",
      "         -0.0114,  0.1788, -0.0322,  0.0634, -0.1965,  0.0355, -0.1877,  0.0430,\n",
      "         -0.0153,  0.1204, -0.0848, -0.0072, -0.0121,  0.0730, -0.0186,  0.0754,\n",
      "          0.0164,  0.0924,  0.1594, -0.0360,  0.0770, -0.0528,  0.0936, -0.0763,\n",
      "         -0.0707, -0.0935, -0.0783, -0.0139, -0.0352,  0.0623, -0.0253, -0.1390,\n",
      "          0.0381, -0.1633,  0.1316, -0.1226, -0.0330,  0.1635, -0.1238, -0.0575,\n",
      "          0.2134, -0.1288, -0.0973, -0.1462],\n",
      "        [-0.1267,  0.0608,  0.0945, -0.0855,  0.0137, -0.1502, -0.0231, -0.0381,\n",
      "          0.0615,  0.1791,  0.0682, -0.0547, -0.1165,  0.0160, -0.0677,  0.1215,\n",
      "          0.1270,  0.0552,  0.0252,  0.0310, -0.0521,  0.0416, -0.0192, -0.0318,\n",
      "          0.1169, -0.0378, -0.0670, -0.1821,  0.0983, -0.0484, -0.0966,  0.0173,\n",
      "         -0.1199, -0.0362, -0.0371, -0.0377, -0.0871, -0.0873, -0.0467, -0.1680,\n",
      "         -0.0357,  0.0793, -0.1178, -0.0236,  0.0737, -0.0153, -0.0381, -0.0225,\n",
      "          0.0352, -0.2038,  0.0576,  0.0369, -0.0268,  0.0086,  0.0528,  0.2330,\n",
      "         -0.0982,  0.0023, -0.0252,  0.0119, -0.1204, -0.0712, -0.1209,  0.0804,\n",
      "          0.0809,  0.0293,  0.0905, -0.0259, -0.0375, -0.0021,  0.1119,  0.0360,\n",
      "         -0.0401,  0.0578, -0.1174, -0.0666, -0.1238, -0.0625,  0.0602, -0.0826,\n",
      "         -0.0951, -0.1556, -0.0814,  0.0590, -0.0276,  0.1907,  0.1193,  0.1279,\n",
      "          0.0191,  0.0608, -0.0323, -0.2167, -0.0499, -0.0226, -0.0062,  0.2766,\n",
      "          0.0613,  0.0723, -0.1194,  0.0362],\n",
      "        [ 0.1253, -0.0567, -0.0470,  0.0057, -0.0911,  0.1846, -0.0189,  0.0406,\n",
      "          0.0422, -0.0780, -0.0071, -0.2156, -0.0201, -0.0144, -0.0221, -0.0102,\n",
      "         -0.0019, -0.1399,  0.1202, -0.0799, -0.0560, -0.0376,  0.0086, -0.1423,\n",
      "         -0.1784,  0.1010,  0.0851,  0.0469, -0.0839, -0.0062, -0.1095,  0.1140,\n",
      "          0.0282,  0.0649, -0.0995, -0.0791,  0.1417, -0.0001, -0.0900,  0.0731,\n",
      "         -0.1580,  0.0366,  0.1625, -0.1173,  0.0578,  0.0885,  0.0795, -0.0279,\n",
      "          0.0966,  0.2265, -0.0082, -0.0968,  0.1544, -0.0177, -0.0833, -0.0294,\n",
      "          0.0502, -0.1520,  0.1117,  0.0786, -0.0282, -0.1254, -0.0582,  0.0808,\n",
      "         -0.1701, -0.0076, -0.1343, -0.0174, -0.1298, -0.0760, -0.0093, -0.0526,\n",
      "          0.0641,  0.0873,  0.0920,  0.0170,  0.0591, -0.0053,  0.0125,  0.0697,\n",
      "          0.0351, -0.1147, -0.1147,  0.0011, -0.0274, -0.1307, -0.1444,  0.0165,\n",
      "         -0.0701,  0.1172,  0.0456,  0.0969,  0.0502,  0.1024,  0.1399, -0.1740,\n",
      "         -0.1500,  0.0137, -0.0495,  0.0711],\n",
      "        [ 0.0179,  0.0685,  0.0873,  0.0371,  0.0590,  0.0197,  0.1005,  0.0666,\n",
      "         -0.1361,  0.1357, -0.0054, -0.0601,  0.1132, -0.1546, -0.0455, -0.0954,\n",
      "         -0.0512,  0.1773, -0.1578, -0.0489, -0.0059, -0.0846, -0.0196,  0.1207,\n",
      "          0.0869, -0.0223, -0.0498, -0.0834, -0.0735,  0.0632,  0.0749,  0.0071,\n",
      "          0.0989, -0.0011, -0.0934, -0.0903, -0.0573,  0.0495,  0.0562, -0.0934,\n",
      "         -0.0274,  0.0376,  0.2146,  0.0413, -0.0074, -0.0715,  0.0373,  0.1217,\n",
      "         -0.0909,  0.1196,  0.0167,  0.0224,  0.0212, -0.0289,  0.0734, -0.1166,\n",
      "          0.0324,  0.1181, -0.1991, -0.1180,  0.0931, -0.0650, -0.0690, -0.0341,\n",
      "          0.0157,  0.1252, -0.0985,  0.0414,  0.1322, -0.1907, -0.0653, -0.0911,\n",
      "         -0.0773,  0.0222, -0.1590,  0.0777, -0.0225,  0.1439,  0.0272,  0.0664,\n",
      "         -0.0481, -0.1818,  0.0939,  0.0641, -0.1797,  0.0371,  0.1511, -0.0627,\n",
      "          0.0379, -0.0997, -0.1519,  0.0007,  0.0910, -0.0139,  0.0055, -0.0742,\n",
      "         -0.0375,  0.1489, -0.0454, -0.1793],\n",
      "        [ 0.0146, -0.0842,  0.0056,  0.0951, -0.0240,  0.0560,  0.0690,  0.0636,\n",
      "         -0.0916,  0.0020, -0.0134,  0.0796, -0.0850, -0.0028,  0.0361, -0.0669,\n",
      "         -0.3067,  0.1429, -0.0458,  0.0598,  0.0754,  0.0671, -0.0477,  0.1118,\n",
      "         -0.0051, -0.0064, -0.0240,  0.1009,  0.0136, -0.0055,  0.0861,  0.0048,\n",
      "         -0.2677, -0.1203, -0.0678, -0.0073, -0.1324,  0.0625, -0.0334,  0.0507,\n",
      "          0.0945,  0.0802, -0.1084, -0.0306, -0.0056,  0.0800, -0.1123,  0.0358,\n",
      "         -0.0649,  0.1931,  0.0560,  0.1092, -0.1128, -0.0712,  0.1549, -0.1540,\n",
      "          0.0716,  0.1764,  0.0486, -0.0538,  0.0456,  0.1606, -0.0910, -0.0560,\n",
      "          0.0678,  0.1936,  0.0395, -0.0298, -0.0671,  0.0566, -0.1658,  0.0905,\n",
      "          0.0633, -0.0620,  0.1562,  0.0469, -0.1265, -0.2520,  0.0131, -0.0754,\n",
      "         -0.2041,  0.1002,  0.0500, -0.0082,  0.1458, -0.2233, -0.0389,  0.0779,\n",
      "          0.0896,  0.1174,  0.0230,  0.1004,  0.0039,  0.0297,  0.0805, -0.0051,\n",
      "          0.0482, -0.0716,  0.0410,  0.1311]])\n",
      "fc3.bias \t torch.Size([10])\n",
      "fc3.bias \t tensor([-0.0838,  0.0204, -0.0172,  0.0138,  0.0053, -0.0699,  0.0305,  0.0152,\n",
      "        -0.0514, -0.0417])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   torch.Size([6, 1, 5, 5]) \n",
      "  Parameter containing:\n",
      "tensor([[[[-0.0192,  0.2511,  0.2183,  0.0210,  0.0190],\n",
      "          [-0.0522,  0.3645,  0.3561,  0.5513,  0.3440],\n",
      "          [ 0.0598,  0.5070,  0.6116,  0.6220,  0.2953],\n",
      "          [ 0.2985,  0.1301,  0.4567,  0.1638,  0.2149],\n",
      "          [-0.0246,  0.3206,  0.1249,  0.1288, -0.1632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1775, -0.0977,  0.0400,  0.2623,  0.2931],\n",
      "          [ 0.0318, -0.1988, -0.1956,  0.1583,  0.1678],\n",
      "          [-0.2439, -0.2983, -0.0792,  0.2235,  0.3461],\n",
      "          [-0.0280, -0.2769, -0.1121,  0.0926,  0.3560],\n",
      "          [-0.1970, -0.1645,  0.1938,  0.2115,  0.1824]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1362,  0.2118, -0.2154, -0.2076, -0.2334],\n",
      "          [ 0.2467, -0.0821, -0.1670, -0.0738,  0.2066],\n",
      "          [ 0.2564,  0.1952,  0.2048,  0.3864,  0.0963],\n",
      "          [ 0.0786,  0.3546,  0.3418, -0.0168,  0.1037],\n",
      "          [ 0.0178, -0.2039, -0.1759, -0.0146, -0.0048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299,  0.1694, -0.0029, -0.1827, -0.0718],\n",
      "          [-0.1909, -0.1638,  0.1044, -0.2096,  0.0405],\n",
      "          [ 0.0873, -0.1777, -0.1820, -0.0258, -0.1887],\n",
      "          [ 0.0190, -0.1462, -0.0486,  0.0288, -0.2104],\n",
      "          [ 0.1727, -0.1630, -0.0454, -0.2031, -0.1915]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1378,  0.0087, -0.1978, -0.2727,  0.0345],\n",
      "          [ 0.2285, -0.0154, -0.1342, -0.2998, -0.0739],\n",
      "          [ 0.2801,  0.2191,  0.1079,  0.2947,  0.1320],\n",
      "          [ 0.3344,  0.4029,  0.3628,  0.5148,  0.0400],\n",
      "          [ 0.0682,  0.0300,  0.3382,  0.2907,  0.3091]]],\n",
      "\n",
      "\n",
      "        [[[-0.2584, -0.2308, -0.1856,  0.0330,  0.0465],\n",
      "          [-0.2978, -0.1939, -0.2057,  0.0400,  0.1692],\n",
      "          [ 0.0883, -0.1518,  0.0861, -0.1625,  0.0874],\n",
      "          [ 0.1464,  0.2543,  0.0292,  0.0581, -0.0674],\n",
      "          [ 0.3818,  0.3657,  0.2362, -0.0540,  0.1097]]]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "2   torch.Size([6]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.2434, -0.0632,  0.2198,  0.1060,  0.1912,  0.0086], requires_grad=True) \n",
      "\n",
      "\n",
      "3   torch.Size([16, 6, 5, 5]) \n",
      "  Parameter containing:\n",
      "tensor([[[[-0.0109, -0.0548, -0.0268,  0.0555,  0.0529],\n",
      "          [-0.0747, -0.0093, -0.0871,  0.0756,  0.0290],\n",
      "          [ 0.0085, -0.0744, -0.1693, -0.1346, -0.1455],\n",
      "          [ 0.0382, -0.0446, -0.1455, -0.0654, -0.0830],\n",
      "          [ 0.0522,  0.1614,  0.1331,  0.1559,  0.0276]],\n",
      "\n",
      "         [[-0.0135,  0.0531,  0.0238,  0.0102,  0.0312],\n",
      "          [ 0.0628, -0.0327,  0.0206, -0.0319,  0.0428],\n",
      "          [-0.0834, -0.1235, -0.0557,  0.0262, -0.0169],\n",
      "          [ 0.0244, -0.0334,  0.0382, -0.0140,  0.0345],\n",
      "          [ 0.1239, -0.0369, -0.0981, -0.0913, -0.0969]],\n",
      "\n",
      "         [[-0.0242, -0.0831,  0.0583,  0.0632, -0.0662],\n",
      "          [-0.0261, -0.0701,  0.0334, -0.0154,  0.0068],\n",
      "          [-0.0244,  0.0824, -0.1057, -0.1036,  0.0335],\n",
      "          [-0.0140,  0.1386,  0.0705,  0.0171, -0.0072],\n",
      "          [ 0.0018,  0.1739,  0.1223,  0.0542,  0.0006]],\n",
      "\n",
      "         [[ 0.0685,  0.0419, -0.0685, -0.0128, -0.0514],\n",
      "          [ 0.0531,  0.0907, -0.0022,  0.0279,  0.0706],\n",
      "          [ 0.0644,  0.0581,  0.0447,  0.0862, -0.0299],\n",
      "          [-0.0563, -0.0664,  0.0204, -0.0461, -0.0049],\n",
      "          [-0.0201, -0.0669, -0.0425, -0.0638,  0.0590]],\n",
      "\n",
      "         [[-0.0472,  0.0201, -0.0225, -0.0141, -0.0058],\n",
      "          [ 0.0624,  0.0067,  0.0042,  0.0046,  0.0419],\n",
      "          [ 0.0394,  0.0676, -0.0732, -0.1638, -0.0343],\n",
      "          [ 0.1362,  0.1999,  0.0978,  0.1239,  0.0379],\n",
      "          [ 0.0652,  0.1395,  0.1929,  0.1191,  0.0623]],\n",
      "\n",
      "         [[ 0.0314, -0.0123, -0.0633,  0.0803,  0.0407],\n",
      "          [-0.0420,  0.0859,  0.0217, -0.0667,  0.0375],\n",
      "          [ 0.0298,  0.0806,  0.0096,  0.0557, -0.0537],\n",
      "          [ 0.0555, -0.0591,  0.0920,  0.1185,  0.0150],\n",
      "          [-0.0733, -0.1120, -0.0059,  0.1112,  0.0869]]],\n",
      "\n",
      "\n",
      "        [[[-0.0222, -0.0632,  0.0230,  0.0518,  0.0471],\n",
      "          [-0.0131, -0.0666, -0.0177, -0.0039,  0.0601],\n",
      "          [ 0.0307,  0.0119,  0.1052, -0.0188,  0.0316],\n",
      "          [-0.0572, -0.0849, -0.0043,  0.1368,  0.0699],\n",
      "          [-0.0658, -0.1288, -0.1282, -0.0940, -0.0316]],\n",
      "\n",
      "         [[ 0.0440,  0.0498,  0.0480,  0.0980,  0.0044],\n",
      "          [-0.0486,  0.0267,  0.0349, -0.0436,  0.0280],\n",
      "          [-0.0089,  0.0524,  0.0789, -0.0944, -0.0586],\n",
      "          [-0.0659,  0.0035,  0.0432, -0.0714, -0.0228],\n",
      "          [ 0.0241, -0.0759, -0.0205,  0.0458, -0.0653]],\n",
      "\n",
      "         [[-0.0281,  0.0291, -0.0404,  0.0430, -0.0491],\n",
      "          [ 0.0727, -0.0770, -0.0016,  0.0005,  0.0026],\n",
      "          [-0.0638,  0.0402,  0.0268,  0.0278, -0.0408],\n",
      "          [-0.0593, -0.0857, -0.0497,  0.1150,  0.1128],\n",
      "          [-0.0520, -0.0732, -0.0377, -0.0443, -0.0022]],\n",
      "\n",
      "         [[-0.0036, -0.0737,  0.0009,  0.0342,  0.0625],\n",
      "          [ 0.0713, -0.0301, -0.0545, -0.0360, -0.0155],\n",
      "          [ 0.0323,  0.0786, -0.0489, -0.0150, -0.0732],\n",
      "          [-0.0087,  0.0101,  0.0736, -0.0299,  0.0564],\n",
      "          [ 0.0291, -0.0514,  0.0165, -0.0648, -0.0488]],\n",
      "\n",
      "         [[-0.0781,  0.0070, -0.0439,  0.0522,  0.0101],\n",
      "          [-0.0655, -0.0328,  0.0275, -0.0029, -0.0499],\n",
      "          [-0.0462,  0.0460,  0.0361,  0.1371,  0.0840],\n",
      "          [ 0.0280, -0.0768,  0.0399,  0.0632,  0.1553],\n",
      "          [ 0.0167,  0.0032, -0.0908, -0.1111, -0.0763]],\n",
      "\n",
      "         [[ 0.0044,  0.0608, -0.0532, -0.0601,  0.0587],\n",
      "          [-0.0058, -0.0142, -0.0047, -0.0417, -0.0031],\n",
      "          [-0.0824, -0.0439, -0.0758, -0.0655, -0.0323],\n",
      "          [ 0.0141, -0.0282,  0.0304,  0.0188, -0.0538],\n",
      "          [-0.0402,  0.0069,  0.0732,  0.0188, -0.0666]]],\n",
      "\n",
      "\n",
      "        [[[-0.0290, -0.0331, -0.0362, -0.0128, -0.1624],\n",
      "          [ 0.0264,  0.0704,  0.0559,  0.0057, -0.1444],\n",
      "          [ 0.0334, -0.0197,  0.0786,  0.1296, -0.0665],\n",
      "          [-0.1325, -0.0002, -0.0102,  0.0673,  0.1488],\n",
      "          [ 0.0281,  0.0752,  0.1247,  0.0654,  0.0795]],\n",
      "\n",
      "         [[ 0.0432,  0.0453, -0.0163, -0.0221, -0.0871],\n",
      "          [ 0.0297,  0.0078, -0.0213, -0.0102,  0.0096],\n",
      "          [-0.0610,  0.0428,  0.1683,  0.1447,  0.1145],\n",
      "          [ 0.0129,  0.1065,  0.1422,  0.0659,  0.0251],\n",
      "          [-0.0014,  0.0767,  0.0355,  0.0775,  0.0778]],\n",
      "\n",
      "         [[-0.0408,  0.0344,  0.0030, -0.0213, -0.0086],\n",
      "          [ 0.0332,  0.0192,  0.0254,  0.0032,  0.0026],\n",
      "          [-0.0381, -0.0148,  0.0117,  0.0037,  0.0669],\n",
      "          [ 0.0096, -0.0044,  0.0353, -0.0138,  0.0783],\n",
      "          [ 0.0304,  0.0076,  0.0405,  0.0741,  0.0048]],\n",
      "\n",
      "         [[-0.0272,  0.0014, -0.0568,  0.0614,  0.0953],\n",
      "          [ 0.0689, -0.0018,  0.0250,  0.0122,  0.0488],\n",
      "          [ 0.0131, -0.0347,  0.0192,  0.0197, -0.0680],\n",
      "          [ 0.0553, -0.0440, -0.0922, -0.0444,  0.0591],\n",
      "          [ 0.0409, -0.0933,  0.0049, -0.0250,  0.0385]],\n",
      "\n",
      "         [[ 0.0040,  0.0977,  0.0278,  0.0440, -0.1256],\n",
      "          [ 0.0426,  0.0925,  0.1221,  0.1534, -0.0139],\n",
      "          [-0.0277,  0.0244, -0.0303,  0.1574,  0.0935],\n",
      "          [-0.0314,  0.0749,  0.0602,  0.0665,  0.0982],\n",
      "          [ 0.0176,  0.0285,  0.0090,  0.0930, -0.0757]],\n",
      "\n",
      "         [[-0.0138,  0.1097,  0.0643,  0.1291,  0.1020],\n",
      "          [ 0.0214,  0.0248, -0.0334,  0.1013,  0.0531],\n",
      "          [-0.0008, -0.0636,  0.0248, -0.0394, -0.0282],\n",
      "          [ 0.1024,  0.0172,  0.0456, -0.0545,  0.0182],\n",
      "          [-0.0197,  0.1342,  0.0421,  0.0906,  0.0427]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0875,  0.0322,  0.0375,  0.0408,  0.0054],\n",
      "          [-0.0460, -0.0633, -0.0649,  0.0760, -0.0567],\n",
      "          [ 0.0322, -0.0105, -0.0606, -0.0797, -0.0679],\n",
      "          [-0.0232, -0.0553, -0.0724,  0.0120, -0.0809],\n",
      "          [ 0.0408, -0.0291, -0.0568, -0.0558, -0.0341]],\n",
      "\n",
      "         [[-0.0568, -0.0631,  0.0572, -0.0491, -0.0536],\n",
      "          [-0.0138,  0.0440,  0.0787,  0.0738,  0.0220],\n",
      "          [-0.0163,  0.0168, -0.0073,  0.0200, -0.0652],\n",
      "          [ 0.0009,  0.0010, -0.0487, -0.0715,  0.0072],\n",
      "          [-0.0719, -0.0316, -0.0134, -0.0032,  0.0724]],\n",
      "\n",
      "         [[ 0.0582, -0.0085,  0.0132, -0.0703,  0.0159],\n",
      "          [ 0.0671, -0.0448,  0.0393, -0.0356,  0.0394],\n",
      "          [ 0.0450,  0.0304, -0.0662,  0.0135,  0.0431],\n",
      "          [-0.0309,  0.0268, -0.0411, -0.0587,  0.0662],\n",
      "          [ 0.0126,  0.0711, -0.0226, -0.0170,  0.0568]],\n",
      "\n",
      "         [[ 0.0282, -0.0763, -0.0282, -0.0218, -0.0370],\n",
      "          [ 0.0586, -0.0726, -0.0211, -0.0571, -0.0420],\n",
      "          [ 0.0335,  0.0127,  0.0681,  0.0412,  0.0192],\n",
      "          [-0.0766, -0.0016, -0.0073,  0.0633, -0.0200],\n",
      "          [-0.0027, -0.0093,  0.0077, -0.0436,  0.0106]],\n",
      "\n",
      "         [[-0.0392,  0.0598, -0.0669,  0.0659, -0.0636],\n",
      "          [-0.0772, -0.0788, -0.0499, -0.0290,  0.0244],\n",
      "          [-0.0204, -0.0283,  0.0083, -0.0568,  0.0490],\n",
      "          [ 0.0234, -0.0123, -0.0521,  0.0399, -0.0175],\n",
      "          [ 0.0072, -0.0435, -0.0295, -0.0564, -0.0306]],\n",
      "\n",
      "         [[ 0.0560, -0.0579, -0.0484,  0.0134,  0.0548],\n",
      "          [-0.0322,  0.0032,  0.0593, -0.0421, -0.0552],\n",
      "          [ 0.0751,  0.0288, -0.0811, -0.0192, -0.0289],\n",
      "          [-0.0103,  0.0337, -0.0256, -0.0466,  0.0394],\n",
      "          [ 0.0175,  0.0412, -0.0659, -0.0206,  0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340,  0.0957,  0.0862, -0.0030,  0.0326],\n",
      "          [ 0.0625,  0.1338,  0.0669, -0.0977,  0.0272],\n",
      "          [ 0.1258,  0.2758,  0.0864, -0.1109, -0.1041],\n",
      "          [ 0.1510,  0.1942,  0.0006, -0.1443, -0.1476],\n",
      "          [ 0.1227,  0.0747, -0.0599, -0.0265, -0.0703]],\n",
      "\n",
      "         [[ 0.0696,  0.0604,  0.0966,  0.0341, -0.0003],\n",
      "          [ 0.2600,  0.1970,  0.0654, -0.0413,  0.0700],\n",
      "          [ 0.2760,  0.1238,  0.0613,  0.0552, -0.0365],\n",
      "          [ 0.2264,  0.0426,  0.0530, -0.0057,  0.0397],\n",
      "          [ 0.0955,  0.0771,  0.0420, -0.0585, -0.0408]],\n",
      "\n",
      "         [[-0.0805, -0.0571,  0.0686,  0.0679,  0.0019],\n",
      "          [ 0.0002, -0.0732,  0.0858,  0.0114, -0.0765],\n",
      "          [-0.0982,  0.0699,  0.1299,  0.0603, -0.0301],\n",
      "          [-0.0205,  0.0617,  0.0674,  0.0328,  0.0286],\n",
      "          [-0.0963,  0.0384,  0.1049,  0.1011,  0.0627]],\n",
      "\n",
      "         [[ 0.0357,  0.0340,  0.0522,  0.0495,  0.0708],\n",
      "          [-0.0845, -0.0224, -0.0304, -0.0551, -0.0190],\n",
      "          [-0.0668,  0.0409, -0.0619, -0.0562,  0.0009],\n",
      "          [ 0.0180, -0.0264,  0.0483,  0.0653,  0.0173],\n",
      "          [ 0.0295,  0.0553,  0.0585, -0.0805,  0.0243]],\n",
      "\n",
      "         [[ 0.1076, -0.0023, -0.0163, -0.0140,  0.0564],\n",
      "          [ 0.0750,  0.0792, -0.0273,  0.0257, -0.0842],\n",
      "          [ 0.0222,  0.0533, -0.0139, -0.0419, -0.0244],\n",
      "          [ 0.0319,  0.0712,  0.0382,  0.0173,  0.0096],\n",
      "          [-0.0934,  0.0405,  0.1524,  0.0470,  0.0904]],\n",
      "\n",
      "         [[-0.0850,  0.0575,  0.0297,  0.0397,  0.0481],\n",
      "          [-0.0801, -0.0908, -0.0910, -0.0494,  0.0574],\n",
      "          [ 0.1137, -0.0567, -0.1489, -0.0140,  0.0657],\n",
      "          [ 0.0604, -0.0574, -0.0979,  0.0109,  0.0994],\n",
      "          [ 0.0521, -0.0905,  0.0108,  0.0659,  0.0968]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1651,  0.1899,  0.1213,  0.0844,  0.0381],\n",
      "          [ 0.0872,  0.0726,  0.1388,  0.1072,  0.0381],\n",
      "          [-0.0822, -0.0235, -0.0253,  0.0575,  0.0126],\n",
      "          [-0.1143, -0.0686,  0.0064,  0.0676,  0.0519],\n",
      "          [-0.0414, -0.0143, -0.0119,  0.0260,  0.0315]],\n",
      "\n",
      "         [[-0.0832, -0.0186,  0.0401, -0.0030,  0.0215],\n",
      "          [-0.0418,  0.0453,  0.0292,  0.0065,  0.0488],\n",
      "          [ 0.0127, -0.1047,  0.0074,  0.0770,  0.0699],\n",
      "          [-0.1040,  0.0111, -0.0547,  0.0259,  0.0403],\n",
      "          [-0.0534, -0.0387, -0.0023,  0.0301, -0.0905]],\n",
      "\n",
      "         [[ 0.0518,  0.1405,  0.1145,  0.0954, -0.0037],\n",
      "          [-0.0601,  0.0425, -0.0251,  0.0921, -0.0411],\n",
      "          [-0.0104, -0.0991, -0.0232, -0.0040,  0.0175],\n",
      "          [ 0.0217, -0.0569, -0.0210, -0.0877,  0.0530],\n",
      "          [ 0.0366, -0.0513,  0.0025, -0.0891,  0.0799]],\n",
      "\n",
      "         [[ 0.0528,  0.0358, -0.0843,  0.0293,  0.0443],\n",
      "          [ 0.0218,  0.0180, -0.0699, -0.0587,  0.0115],\n",
      "          [ 0.0040,  0.0050,  0.0047,  0.0408,  0.0669],\n",
      "          [ 0.0741, -0.0620,  0.0226,  0.0472,  0.0682],\n",
      "          [ 0.0717,  0.0853, -0.0000, -0.0112, -0.0270]],\n",
      "\n",
      "         [[ 0.1038,  0.1426,  0.0998,  0.0320,  0.1111],\n",
      "          [-0.1177, -0.0672,  0.0677,  0.1034, -0.0451],\n",
      "          [-0.0871, -0.1239, -0.0099, -0.0290, -0.0613],\n",
      "          [-0.0878, -0.0596, -0.0144, -0.0210,  0.0641],\n",
      "          [-0.0289,  0.0519, -0.0234,  0.0174,  0.0056]],\n",
      "\n",
      "         [[-0.0465,  0.0461,  0.0285,  0.0367,  0.0337],\n",
      "          [ 0.0520, -0.0826, -0.0852, -0.0947, -0.1000],\n",
      "          [ 0.0594,  0.0626,  0.0371,  0.0097, -0.0623],\n",
      "          [ 0.0788, -0.0128,  0.0158, -0.0668, -0.0158],\n",
      "          [-0.0014,  0.0584, -0.0293, -0.0480, -0.0613]]]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "4   torch.Size([16]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0730,  0.0296,  0.0032,  0.0657,  0.0553,  0.0152, -0.0441, -0.0667,\n",
      "         0.0277,  0.0222, -0.0358,  0.0246,  0.0430, -0.0688, -0.0241,  0.0027],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "5   torch.Size([300, 256]) \n",
      "  Parameter containing:\n",
      "tensor([[ 0.0272,  0.0675,  0.0587,  ...,  0.0256, -0.0110, -0.0366],\n",
      "        [ 0.0592,  0.0532,  0.0201,  ..., -0.0201, -0.0062,  0.0536],\n",
      "        [-0.0039, -0.0355, -0.0470,  ...,  0.0263, -0.0019, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0400,  0.0265, -0.0372,  ..., -0.0495,  0.0229,  0.0028],\n",
      "        [-0.0405,  0.0220, -0.0051,  ...,  0.0582, -0.0126, -0.0565],\n",
      "        [ 0.0613,  0.0368, -0.0311,  ..., -0.0378, -0.0014,  0.0345]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "6   torch.Size([300]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0125,  0.0496,  0.0088,  0.0085,  0.0095, -0.0318, -0.0080,  0.0076,\n",
      "         0.0534, -0.0552, -0.0182, -0.0311, -0.0108, -0.0078, -0.0341,  0.0239,\n",
      "        -0.0408, -0.0497, -0.0399, -0.0287,  0.0010, -0.0028,  0.0587,  0.0291,\n",
      "        -0.0397,  0.0408, -0.0342,  0.0057,  0.0011,  0.0218, -0.0139,  0.0099,\n",
      "        -0.0637, -0.0040,  0.0283, -0.0100, -0.0033,  0.0194,  0.0250,  0.0024,\n",
      "        -0.0618,  0.0368, -0.0075, -0.0286, -0.0415,  0.0529,  0.0308, -0.0437,\n",
      "         0.0435,  0.0390, -0.0456, -0.0025,  0.0445, -0.0515,  0.0301, -0.0621,\n",
      "        -0.0068,  0.0022, -0.0306,  0.0367, -0.0544, -0.0037, -0.0045,  0.0001,\n",
      "         0.0466,  0.0304,  0.0454,  0.0361,  0.0541, -0.0258, -0.0537, -0.0621,\n",
      "        -0.0620,  0.0021,  0.0354,  0.0299,  0.0376,  0.0373, -0.0519,  0.0416,\n",
      "         0.0261, -0.0177, -0.0317,  0.0305, -0.0425,  0.0313,  0.0406,  0.0041,\n",
      "         0.0194,  0.0046,  0.0000, -0.0381,  0.0463,  0.0631,  0.0508, -0.0166,\n",
      "        -0.0199, -0.0257, -0.0614, -0.0028,  0.0009, -0.0030, -0.0501, -0.0544,\n",
      "         0.0413, -0.0580,  0.0460,  0.0103,  0.0018,  0.0094,  0.0230,  0.0387,\n",
      "        -0.0001,  0.0430, -0.0575,  0.0499, -0.0169,  0.0482,  0.0148,  0.0434,\n",
      "         0.0406, -0.0329, -0.0347, -0.0067,  0.0056,  0.0384,  0.0232,  0.0367,\n",
      "        -0.0032,  0.0292, -0.0203, -0.0486,  0.0126, -0.0531,  0.0207,  0.0398,\n",
      "         0.0525, -0.0629,  0.0590, -0.0428, -0.0445, -0.0290, -0.0070, -0.0509,\n",
      "         0.0348,  0.0296, -0.0262, -0.0463,  0.0530, -0.0203,  0.0406, -0.0518,\n",
      "         0.0414,  0.0028,  0.0606,  0.0647, -0.0044,  0.0148, -0.0470,  0.0146,\n",
      "        -0.0530, -0.0190,  0.0125,  0.0080, -0.0501,  0.0331, -0.0305, -0.0425,\n",
      "         0.0532, -0.0100,  0.0302, -0.0257,  0.0368,  0.0359,  0.0030, -0.0122,\n",
      "        -0.0206, -0.0331,  0.0122, -0.0050,  0.0362, -0.0105,  0.0027, -0.0145,\n",
      "        -0.0539,  0.0082,  0.0559, -0.0274, -0.0482, -0.0504, -0.0238,  0.0578,\n",
      "         0.0111, -0.0489,  0.0005,  0.0606, -0.0305,  0.0339,  0.0619,  0.0450,\n",
      "        -0.0125,  0.0277, -0.0592, -0.0471, -0.0499, -0.0444, -0.0058,  0.0355,\n",
      "         0.0439, -0.0299, -0.0286, -0.0260, -0.0086,  0.0275,  0.0235,  0.0480,\n",
      "         0.0576, -0.0442, -0.0399,  0.0315,  0.0131, -0.0080,  0.0349,  0.0340,\n",
      "        -0.0147, -0.0120, -0.0187,  0.0264,  0.0293,  0.0585,  0.0324, -0.0166,\n",
      "        -0.0555,  0.0493, -0.0615, -0.0557,  0.0566, -0.0102,  0.0540, -0.0159,\n",
      "        -0.0254, -0.0540, -0.0359, -0.0339,  0.0491, -0.0252,  0.0568,  0.0199,\n",
      "        -0.0021, -0.0235, -0.0565,  0.0607,  0.0233, -0.0354,  0.0298,  0.0329,\n",
      "         0.0625, -0.0587,  0.0386, -0.0303,  0.0186, -0.0189,  0.0408, -0.0094,\n",
      "         0.0109, -0.0002, -0.0109, -0.0081,  0.0345, -0.0408, -0.0037, -0.0351,\n",
      "         0.0441,  0.0274, -0.0352,  0.0627, -0.0417,  0.0531, -0.0357,  0.0050,\n",
      "         0.0328, -0.0356,  0.0408, -0.0267,  0.0032, -0.0200,  0.0475, -0.0275,\n",
      "        -0.0526,  0.0087,  0.0170,  0.0063,  0.0208, -0.0253, -0.0119,  0.0515,\n",
      "        -0.0609, -0.0366,  0.0094,  0.0507], requires_grad=True) \n",
      "\n",
      "\n",
      "7   torch.Size([100, 300]) \n",
      "  Parameter containing:\n",
      "tensor([[ 0.0402, -0.0434,  0.0285,  ..., -0.0308, -0.0236,  0.0146],\n",
      "        [ 0.0027,  0.0438, -0.0360,  ...,  0.0140,  0.0560,  0.0263],\n",
      "        [ 0.0155, -0.0348,  0.0268,  ...,  0.0200,  0.0367,  0.0531],\n",
      "        ...,\n",
      "        [ 0.0570, -0.0697,  0.0352,  ..., -0.0380, -0.0513, -0.0107],\n",
      "        [-0.0099,  0.0150,  0.0478,  ...,  0.0631, -0.0225,  0.0330],\n",
      "        [-0.0361,  0.0464, -0.0538,  ...,  0.0255, -0.0425, -0.0736]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "8   torch.Size([100]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0397, -0.0087, -0.0203,  0.0181,  0.0399, -0.0314, -0.0457,  0.0263,\n",
      "        -0.0335,  0.0092, -0.0015, -0.0373,  0.0526,  0.0600,  0.0099, -0.0135,\n",
      "         0.0540,  0.0312,  0.0291,  0.0140,  0.0042,  0.0081, -0.0266,  0.0049,\n",
      "         0.0078,  0.0362, -0.0094,  0.0693,  0.0406,  0.0248,  0.0119, -0.0411,\n",
      "         0.0599, -0.0495, -0.0417, -0.0585, -0.0123,  0.0157, -0.0193, -0.0215,\n",
      "         0.0462, -0.0107, -0.0284,  0.0102, -0.0230,  0.0080, -0.0122, -0.0338,\n",
      "         0.0525, -0.0359,  0.0041,  0.0639,  0.0477,  0.0078,  0.0393,  0.0211,\n",
      "        -0.0103, -0.0084, -0.0399, -0.0345, -0.0042, -0.0272,  0.0004,  0.0318,\n",
      "        -0.0283,  0.0067, -0.0566,  0.0388,  0.0067,  0.0128, -0.0366, -0.0135,\n",
      "        -0.0255, -0.0400,  0.0588, -0.0205,  0.0412,  0.0009,  0.0443,  0.0374,\n",
      "         0.0142,  0.0051,  0.0115,  0.0211,  0.0548,  0.0299,  0.0294, -0.0427,\n",
      "        -0.0211, -0.0392,  0.0204,  0.0366, -0.0392, -0.0465, -0.0501, -0.0336,\n",
      "        -0.0278,  0.0443,  0.0489, -0.0206], requires_grad=True) \n",
      "\n",
      "\n",
      "9   torch.Size([10, 100]) \n",
      "  Parameter containing:\n",
      "tensor([[-0.0799, -0.0514,  0.0154,  0.0339, -0.0871, -0.0275,  0.0830, -0.0554,\n",
      "         -0.0946, -0.0866, -0.1241,  0.0442, -0.0847,  0.1099, -0.0807, -0.0870,\n",
      "         -0.0052,  0.0141, -0.1187,  0.0135, -0.0574,  0.0641,  0.0971,  0.1102,\n",
      "         -0.0481, -0.1465, -0.0263, -0.1137, -0.0913,  0.1041,  0.0754, -0.0701,\n",
      "         -0.0155,  0.1341, -0.0931, -0.0810,  0.0836,  0.0484,  0.1427, -0.1458,\n",
      "         -0.1428,  0.0642,  0.0834, -0.0122,  0.0128, -0.0866,  0.1009, -0.0585,\n",
      "          0.1132, -0.0641,  0.0977,  0.0277,  0.0624, -0.0949, -0.0213,  0.1714,\n",
      "         -0.1392, -0.0815, -0.0149,  0.1095, -0.0326, -0.0129,  0.1646,  0.0889,\n",
      "         -0.1588, -0.0397, -0.0864,  0.0607,  0.0431,  0.0588, -0.0666,  0.0588,\n",
      "          0.0749,  0.0891, -0.0775,  0.0067, -0.0723,  0.0007, -0.1620, -0.0965,\n",
      "         -0.1363,  0.0115,  0.0796,  0.0139,  0.0489, -0.0417, -0.0408,  0.1485,\n",
      "          0.0496,  0.1629, -0.0059, -0.0451, -0.0528,  0.1137, -0.0703,  0.1330,\n",
      "         -0.0167, -0.0833, -0.0043,  0.0967],\n",
      "        [ 0.0605, -0.0714, -0.1559, -0.0261, -0.0907, -0.0269,  0.0549, -0.1048,\n",
      "          0.1263, -0.1483,  0.1830, -0.1227,  0.1579,  0.0976, -0.0630, -0.0642,\n",
      "          0.0642,  0.0171,  0.0748, -0.0418, -0.0341,  0.1174,  0.1188, -0.1357,\n",
      "         -0.0569,  0.1670,  0.1514,  0.1882, -0.0166, -0.0848, -0.0153,  0.0541,\n",
      "          0.1883, -0.0972, -0.0972, -0.0510, -0.1072, -0.0235, -0.0642,  0.1352,\n",
      "          0.0150, -0.0173, -0.1752,  0.1126, -0.0674,  0.0020,  0.0317, -0.0967,\n",
      "          0.1348, -0.1442, -0.0774, -0.1404,  0.1202, -0.0748, -0.1379,  0.0576,\n",
      "          0.1114, -0.0876,  0.1180, -0.1181,  0.1069,  0.1197,  0.1634, -0.0131,\n",
      "          0.1384, -0.0552,  0.0004, -0.0252, -0.0005, -0.0646, -0.0370, -0.0633,\n",
      "         -0.0284,  0.0869, -0.1731,  0.1347, -0.1184, -0.1579, -0.0766,  0.0333,\n",
      "          0.0633, -0.0766, -0.0020,  0.0931, -0.0060, -0.0381,  0.0882,  0.0496,\n",
      "         -0.0383, -0.0410,  0.0548, -0.0701, -0.0373, -0.1803,  0.1029, -0.0843,\n",
      "         -0.0158, -0.1036,  0.0492, -0.0733],\n",
      "        [ 0.1163,  0.0529,  0.0872, -0.0351, -0.0968,  0.1147,  0.0832, -0.0014,\n",
      "         -0.0788,  0.0679,  0.0367,  0.0653,  0.2598, -0.0424, -0.0725,  0.0351,\n",
      "          0.0587,  0.0893, -0.1037, -0.0686, -0.0598, -0.0415,  0.1333,  0.0031,\n",
      "          0.0858, -0.0589, -0.0039, -0.0715,  0.0809, -0.0254,  0.0300, -0.1061,\n",
      "          0.1893,  0.0260, -0.0539,  0.0239,  0.2075, -0.0787, -0.1141,  0.0259,\n",
      "          0.0277, -0.0784, -0.0361, -0.0954,  0.1004, -0.0387, -0.0915, -0.1756,\n",
      "          0.0812,  0.0002,  0.0163,  0.1229,  0.0020, -0.0592,  0.0071,  0.0194,\n",
      "         -0.0185, -0.1658, -0.0266, -0.0476, -0.0040, -0.1298,  0.0797, -0.0132,\n",
      "         -0.0125, -0.0713,  0.0886,  0.0654, -0.1828,  0.1266, -0.0312, -0.1565,\n",
      "         -0.0008,  0.0433, -0.0532, -0.0615,  0.0067,  0.2707, -0.1773, -0.0664,\n",
      "          0.0780,  0.1225, -0.0906, -0.0346, -0.1364, -0.1227, -0.0573,  0.0918,\n",
      "         -0.1081,  0.1082, -0.0558, -0.0893,  0.0545, -0.0031,  0.0543, -0.1526,\n",
      "         -0.1887,  0.0908, -0.0388,  0.1185],\n",
      "        [-0.1174, -0.0956, -0.0778, -0.0839, -0.1300,  0.1048, -0.0927,  0.0063,\n",
      "         -0.0726, -0.1231,  0.0053,  0.0911, -0.0725, -0.1458, -0.0792,  0.0743,\n",
      "          0.0184,  0.0899,  0.2005, -0.1634, -0.0627, -0.0126, -0.0801, -0.0196,\n",
      "         -0.1274, -0.1908, -0.0943,  0.0627, -0.0222,  0.0798,  0.0219, -0.1379,\n",
      "          0.1264,  0.0201,  0.1231, -0.0776, -0.0175,  0.0741,  0.2067,  0.1731,\n",
      "          0.1828,  0.0337, -0.0757,  0.1103,  0.0470, -0.0247,  0.0363, -0.1289,\n",
      "         -0.1138,  0.1339,  0.0701,  0.0968, -0.1750, -0.0275, -0.1629, -0.1297,\n",
      "          0.1120, -0.0651, -0.0119, -0.0819, -0.0517,  0.1247, -0.0554,  0.0212,\n",
      "         -0.1113,  0.0962, -0.1201, -0.0136,  0.1305, -0.0559, -0.1068, -0.1020,\n",
      "         -0.0739,  0.0870,  0.1114, -0.0510, -0.0069,  0.1589, -0.0137, -0.0609,\n",
      "          0.0888,  0.0635, -0.0432, -0.0050, -0.0781,  0.0995, -0.0672,  0.0887,\n",
      "         -0.0514, -0.0577, -0.1168,  0.0129, -0.0206,  0.0918,  0.0148, -0.0447,\n",
      "          0.0651,  0.0980, -0.0730,  0.1064],\n",
      "        [ 0.1344, -0.0527, -0.1721,  0.0228,  0.0103, -0.0212,  0.0978, -0.0764,\n",
      "         -0.0590,  0.0773,  0.0813,  0.1136,  0.0045, -0.0662,  0.0648, -0.1574,\n",
      "          0.0122, -0.0855, -0.0298, -0.1251,  0.0770,  0.1594,  0.1165, -0.0861,\n",
      "         -0.1141, -0.0640, -0.1431, -0.0167,  0.0222, -0.0586, -0.1808,  0.0198,\n",
      "         -0.1278,  0.0701, -0.1002,  0.0631, -0.0676, -0.0541,  0.0505, -0.0883,\n",
      "         -0.0518,  0.0285,  0.0013, -0.1970,  0.0006,  0.0546, -0.0142,  0.0711,\n",
      "         -0.0467, -0.0935, -0.0555, -0.0608,  0.0216,  0.0222,  0.0968, -0.0574,\n",
      "          0.0568,  0.0914,  0.1699,  0.1728,  0.2329, -0.0513,  0.0553,  0.0158,\n",
      "          0.0237, -0.1188,  0.1711, -0.0002,  0.0112,  0.0978,  0.0454,  0.0862,\n",
      "          0.0530,  0.0608,  0.0068, -0.1118,  0.0083, -0.1468,  0.1673,  0.0777,\n",
      "          0.0660,  0.0521,  0.0632, -0.0286,  0.1892, -0.1432,  0.0500,  0.0170,\n",
      "          0.0163,  0.0035, -0.1290,  0.0328, -0.0114, -0.1939,  0.1007,  0.0200,\n",
      "         -0.0886, -0.0748, -0.0414, -0.0459],\n",
      "        [-0.0702,  0.0972, -0.1089, -0.0164,  0.1510,  0.1673,  0.0355, -0.0335,\n",
      "          0.0098, -0.0335, -0.0313,  0.0474, -0.2167,  0.0931,  0.0591,  0.1774,\n",
      "          0.0661, -0.1510, -0.0437,  0.0292,  0.0459,  0.0118, -0.1072,  0.0775,\n",
      "          0.1941,  0.0081, -0.0666,  0.1146,  0.0869, -0.0447,  0.0142,  0.1406,\n",
      "         -0.0295,  0.0406,  0.1028,  0.1352,  0.0855, -0.0151, -0.0241, -0.0670,\n",
      "          0.0759,  0.0260, -0.1360,  0.1547, -0.0115, -0.0346,  0.1122,  0.0071,\n",
      "         -0.1181, -0.0383, -0.0050,  0.0827,  0.0534,  0.0518, -0.1492,  0.0719,\n",
      "         -0.0114,  0.1788, -0.0322,  0.0634, -0.1965,  0.0355, -0.1877,  0.0430,\n",
      "         -0.0153,  0.1204, -0.0848, -0.0072, -0.0121,  0.0730, -0.0186,  0.0754,\n",
      "          0.0164,  0.0924,  0.1594, -0.0360,  0.0770, -0.0528,  0.0936, -0.0763,\n",
      "         -0.0707, -0.0935, -0.0783, -0.0139, -0.0352,  0.0623, -0.0253, -0.1390,\n",
      "          0.0381, -0.1633,  0.1316, -0.1226, -0.0330,  0.1635, -0.1238, -0.0575,\n",
      "          0.2134, -0.1288, -0.0973, -0.1462],\n",
      "        [-0.1267,  0.0608,  0.0945, -0.0855,  0.0137, -0.1502, -0.0231, -0.0381,\n",
      "          0.0615,  0.1791,  0.0682, -0.0547, -0.1165,  0.0160, -0.0677,  0.1215,\n",
      "          0.1270,  0.0552,  0.0252,  0.0310, -0.0521,  0.0416, -0.0192, -0.0318,\n",
      "          0.1169, -0.0378, -0.0670, -0.1821,  0.0983, -0.0484, -0.0966,  0.0173,\n",
      "         -0.1199, -0.0362, -0.0371, -0.0377, -0.0871, -0.0873, -0.0467, -0.1680,\n",
      "         -0.0357,  0.0793, -0.1178, -0.0236,  0.0737, -0.0153, -0.0381, -0.0225,\n",
      "          0.0352, -0.2038,  0.0576,  0.0369, -0.0268,  0.0086,  0.0528,  0.2330,\n",
      "         -0.0982,  0.0023, -0.0252,  0.0119, -0.1204, -0.0712, -0.1209,  0.0804,\n",
      "          0.0809,  0.0293,  0.0905, -0.0259, -0.0375, -0.0021,  0.1119,  0.0360,\n",
      "         -0.0401,  0.0578, -0.1174, -0.0666, -0.1238, -0.0625,  0.0602, -0.0826,\n",
      "         -0.0951, -0.1556, -0.0814,  0.0590, -0.0276,  0.1907,  0.1193,  0.1279,\n",
      "          0.0191,  0.0608, -0.0323, -0.2167, -0.0499, -0.0226, -0.0062,  0.2766,\n",
      "          0.0613,  0.0723, -0.1194,  0.0362],\n",
      "        [ 0.1253, -0.0567, -0.0470,  0.0057, -0.0911,  0.1846, -0.0189,  0.0406,\n",
      "          0.0422, -0.0780, -0.0071, -0.2156, -0.0201, -0.0144, -0.0221, -0.0102,\n",
      "         -0.0019, -0.1399,  0.1202, -0.0799, -0.0560, -0.0376,  0.0086, -0.1423,\n",
      "         -0.1784,  0.1010,  0.0851,  0.0469, -0.0839, -0.0062, -0.1095,  0.1140,\n",
      "          0.0282,  0.0649, -0.0995, -0.0791,  0.1417, -0.0001, -0.0900,  0.0731,\n",
      "         -0.1580,  0.0366,  0.1625, -0.1173,  0.0578,  0.0885,  0.0795, -0.0279,\n",
      "          0.0966,  0.2265, -0.0082, -0.0968,  0.1544, -0.0177, -0.0833, -0.0294,\n",
      "          0.0502, -0.1520,  0.1117,  0.0786, -0.0282, -0.1254, -0.0582,  0.0808,\n",
      "         -0.1701, -0.0076, -0.1343, -0.0174, -0.1298, -0.0760, -0.0093, -0.0526,\n",
      "          0.0641,  0.0873,  0.0920,  0.0170,  0.0591, -0.0053,  0.0125,  0.0697,\n",
      "          0.0351, -0.1147, -0.1147,  0.0011, -0.0274, -0.1307, -0.1444,  0.0165,\n",
      "         -0.0701,  0.1172,  0.0456,  0.0969,  0.0502,  0.1024,  0.1399, -0.1740,\n",
      "         -0.1500,  0.0137, -0.0495,  0.0711],\n",
      "        [ 0.0179,  0.0685,  0.0873,  0.0371,  0.0590,  0.0197,  0.1005,  0.0666,\n",
      "         -0.1361,  0.1357, -0.0054, -0.0601,  0.1132, -0.1546, -0.0455, -0.0954,\n",
      "         -0.0512,  0.1773, -0.1578, -0.0489, -0.0059, -0.0846, -0.0196,  0.1207,\n",
      "          0.0869, -0.0223, -0.0498, -0.0834, -0.0735,  0.0632,  0.0749,  0.0071,\n",
      "          0.0989, -0.0011, -0.0934, -0.0903, -0.0573,  0.0495,  0.0562, -0.0934,\n",
      "         -0.0274,  0.0376,  0.2146,  0.0413, -0.0074, -0.0715,  0.0373,  0.1217,\n",
      "         -0.0909,  0.1196,  0.0167,  0.0224,  0.0212, -0.0289,  0.0734, -0.1166,\n",
      "          0.0324,  0.1181, -0.1991, -0.1180,  0.0931, -0.0650, -0.0690, -0.0341,\n",
      "          0.0157,  0.1252, -0.0985,  0.0414,  0.1322, -0.1907, -0.0653, -0.0911,\n",
      "         -0.0773,  0.0222, -0.1590,  0.0777, -0.0225,  0.1439,  0.0272,  0.0664,\n",
      "         -0.0481, -0.1818,  0.0939,  0.0641, -0.1797,  0.0371,  0.1511, -0.0627,\n",
      "          0.0379, -0.0997, -0.1519,  0.0007,  0.0910, -0.0139,  0.0055, -0.0742,\n",
      "         -0.0375,  0.1489, -0.0454, -0.1793],\n",
      "        [ 0.0146, -0.0842,  0.0056,  0.0951, -0.0240,  0.0560,  0.0690,  0.0636,\n",
      "         -0.0916,  0.0020, -0.0134,  0.0796, -0.0850, -0.0028,  0.0361, -0.0669,\n",
      "         -0.3067,  0.1429, -0.0458,  0.0598,  0.0754,  0.0671, -0.0477,  0.1118,\n",
      "         -0.0051, -0.0064, -0.0240,  0.1009,  0.0136, -0.0055,  0.0861,  0.0048,\n",
      "         -0.2677, -0.1203, -0.0678, -0.0073, -0.1324,  0.0625, -0.0334,  0.0507,\n",
      "          0.0945,  0.0802, -0.1084, -0.0306, -0.0056,  0.0800, -0.1123,  0.0358,\n",
      "         -0.0649,  0.1931,  0.0560,  0.1092, -0.1128, -0.0712,  0.1549, -0.1540,\n",
      "          0.0716,  0.1764,  0.0486, -0.0538,  0.0456,  0.1606, -0.0910, -0.0560,\n",
      "          0.0678,  0.1936,  0.0395, -0.0298, -0.0671,  0.0566, -0.1658,  0.0905,\n",
      "          0.0633, -0.0620,  0.1562,  0.0469, -0.1265, -0.2520,  0.0131, -0.0754,\n",
      "         -0.2041,  0.1002,  0.0500, -0.0082,  0.1458, -0.2233, -0.0389,  0.0779,\n",
      "          0.0896,  0.1174,  0.0230,  0.1004,  0.0039,  0.0297,  0.0805, -0.0051,\n",
      "          0.0482, -0.0716,  0.0410,  0.1311]], requires_grad=True) \n",
      "\n",
      "\n",
      "10   torch.Size([10]) \n",
      "  Parameter containing:\n",
      "tensor([-0.0838,  0.0204, -0.0172,  0.0138,  0.0053, -0.0699,  0.0305,  0.0152,\n",
      "        -0.0514, -0.0417], requires_grad=True) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for parameter in net.parameters():\n",
    "    i+=1\n",
    "    print(i,\" \",parameter.shape,\"\\n \",parameter,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.110000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First we have to convert the nodes to points in some d dimenstion. Where d is the number of nodes in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 256])"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.state_dict()[\"fc1.weight\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not prune the layer with 256 nodes. We have to prune the layer with 300 nodes. So, that depends on the outgoing edges weights means fc1. So the similar kind of weights going on to a group of nodes of the next layer means, those weights are kind of looking for similar kind of patterns. \n",
    "\n",
    "Example: if node1 has incoming edge i1, and outgoing edge o1. And node2 have incoming edge form same previous incoming node as i2, and same outgoing edge weight as o2. Then if we club them up, the net effect will be some function of f(i1)* (o1+o2). assuming i1 and i2 are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 300])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print((net.state_dict()[\"fc2.weight\"]).shape)\n",
    "print(type(net.state_dict()[\"fc2.weight\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After changing the dimension of the layer with 300 nodes, to somehting less than 300. We also have to assing the incoming weight edges accordingly, and assign the outgoing edges equals to be mostly the avg of the nodes in same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first we need to form a space of dimension 100, with 300 points depending on fc2 weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300   256\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "nodes=defaultdict(list)\n",
    "for j in range(len(net.state_dict()[\"fc1.weight\"])):  #Means j is a node of next layer, L2.\n",
    "    nodes[j]=net.state_dict()[\"fc1.weight\"][j]\n",
    "\n",
    "print(len(nodes), \" \", len(nodes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nodes[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have nodes as a dictionary, where key is the node id, and values are the outgoing edges. So, now we have to find such node_ids who are having almost similar outgoing edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "distances=[]\n",
    "for i in nodes.keys():\n",
    "    for j in nodes.keys():\n",
    "        if(i!=j and j>i):\n",
    "            distances.append(euclidean_distance(nodes[i], nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9009067272656212\n"
     ]
    }
   ],
   "source": [
    "print(distances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44850\n"
     ]
    }
   ],
   "source": [
    "distances.sort()\n",
    "print(len(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "# plt.plot([x for x in range(len(distances))], distances) \n",
    "# plt.ylabel(\"Distances\")\n",
    "# plt.show() \n",
    "x=[k for k in range(len(distances))]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, distances)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=defaultdict(bool)\n",
    "setpoints=defaultdict(set)\n",
    "for i in nodes.keys():\n",
    "    cluster[i]=False\n",
    "    setpoints[i]={i}\n",
    "    \n",
    "# print(cluster)\n",
    "\n",
    "for i in nodes.keys():\n",
    "    for j in nodes.keys():\n",
    "        if(i!=j and j>i and cluster[i]==False and cluster[j]==False):\n",
    "            \n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            #########   The percentage of reduction in size depends on the cutoff chosen here ######\n",
    "            #########  The higher cut off will result in higher compression  ########################\n",
    "            \n",
    "            cut_off=0.74645\n",
    "            \n",
    "            if(euclidean_distance(nodes[i], nodes[j])<cut_off):\n",
    "                setpoints[i].add(j)\n",
    "                del setpoints[j]\n",
    "                cluster[j]=True\n",
    "    cluster[i]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ->  {0} -> 1\n",
      "1  ->  {1} -> 1\n",
      "2  ->  {2, 63} -> 2\n",
      "3  ->  {221, 3, 173} -> 3\n",
      "4  ->  {89, 4} -> 2\n",
      "5  ->  {5, 265, 202, 16, 150, 219} -> 6\n",
      "6  ->  {6} -> 1\n",
      "7  ->  {37, 7} -> 2\n",
      "8  ->  {8, 32, 40} -> 3\n",
      "9  ->  {9, 131} -> 2\n",
      "10  ->  {10} -> 1\n",
      "11  ->  {11} -> 1\n",
      "12  ->  {12} -> 1\n",
      "13  ->  {13, 135} -> 2\n",
      "14  ->  {14} -> 1\n",
      "15  ->  {119, 15} -> 2\n",
      "17  ->  {17} -> 1\n",
      "18  ->  {18} -> 1\n",
      "19  ->  {19} -> 1\n",
      "20  ->  {184, 20} -> 2\n",
      "21  ->  {21} -> 1\n",
      "22  ->  {137, 27, 45, 22} -> 4\n",
      "23  ->  {157, 23} -> 2\n",
      "24  ->  {24} -> 1\n",
      "25  ->  {25} -> 1\n",
      "26  ->  {26, 74, 141} -> 3\n",
      "28  ->  {160, 28, 199} -> 3\n",
      "29  ->  {29} -> 1\n",
      "30  ->  {30} -> 1\n",
      "31  ->  {31} -> 1\n",
      "33  ->  {33} -> 1\n",
      "34  ->  {34} -> 1\n",
      "35  ->  {35} -> 1\n",
      "36  ->  {36} -> 1\n",
      "38  ->  {38} -> 1\n",
      "39  ->  {39} -> 1\n",
      "41  ->  {41} -> 1\n",
      "42  ->  {42, 269} -> 2\n",
      "43  ->  {43} -> 1\n",
      "44  ->  {44} -> 1\n",
      "46  ->  {46} -> 1\n",
      "47  ->  {47} -> 1\n",
      "48  ->  {48} -> 1\n",
      "49  ->  {49} -> 1\n",
      "50  ->  {296, 50} -> 2\n",
      "51  ->  {51} -> 1\n",
      "52  ->  {52} -> 1\n",
      "53  ->  {121, 53} -> 2\n",
      "54  ->  {222, 54} -> 2\n",
      "55  ->  {55} -> 1\n",
      "56  ->  {56} -> 1\n",
      "57  ->  {57, 213} -> 2\n",
      "58  ->  {58} -> 1\n",
      "59  ->  {59} -> 1\n",
      "60  ->  {60} -> 1\n",
      "61  ->  {61} -> 1\n",
      "62  ->  {117, 62} -> 2\n",
      "64  ->  {64} -> 1\n",
      "65  ->  {65} -> 1\n",
      "66  ->  {66} -> 1\n",
      "67  ->  {67} -> 1\n",
      "68  ->  {68} -> 1\n",
      "69  ->  {69} -> 1\n",
      "70  ->  {70} -> 1\n",
      "71  ->  {73, 266, 71} -> 3\n",
      "72  ->  {72} -> 1\n",
      "75  ->  {75} -> 1\n",
      "76  ->  {76} -> 1\n",
      "77  ->  {77, 198} -> 2\n",
      "78  ->  {78} -> 1\n",
      "79  ->  {79} -> 1\n",
      "80  ->  {80, 297, 211, 125} -> 4\n",
      "81  ->  {81} -> 1\n",
      "82  ->  {82, 182} -> 2\n",
      "83  ->  {83} -> 1\n",
      "84  ->  {84} -> 1\n",
      "85  ->  {85} -> 1\n",
      "86  ->  {86} -> 1\n",
      "87  ->  {87} -> 1\n",
      "88  ->  {88} -> 1\n",
      "90  ->  {90, 101} -> 2\n",
      "91  ->  {91} -> 1\n",
      "92  ->  {92} -> 1\n",
      "93  ->  {93} -> 1\n",
      "94  ->  {113, 94} -> 2\n",
      "95  ->  {95} -> 1\n",
      "96  ->  {96} -> 1\n",
      "97  ->  {97} -> 1\n",
      "98  ->  {98} -> 1\n",
      "99  ->  {99} -> 1\n",
      "100  ->  {226, 100, 127} -> 3\n",
      "102  ->  {102, 278} -> 2\n",
      "103  ->  {168, 103} -> 2\n",
      "104  ->  {104} -> 1\n",
      "105  ->  {105} -> 1\n",
      "106  ->  {106} -> 1\n",
      "107  ->  {107} -> 1\n",
      "108  ->  {108} -> 1\n",
      "109  ->  {109} -> 1\n",
      "110  ->  {110} -> 1\n",
      "111  ->  {111} -> 1\n",
      "112  ->  {112} -> 1\n",
      "114  ->  {114} -> 1\n",
      "115  ->  {115} -> 1\n",
      "116  ->  {116} -> 1\n",
      "118  ->  {118} -> 1\n",
      "120  ->  {120} -> 1\n",
      "122  ->  {122} -> 1\n",
      "123  ->  {123} -> 1\n",
      "124  ->  {124, 262} -> 2\n",
      "126  ->  {126} -> 1\n",
      "128  ->  {128} -> 1\n",
      "129  ->  {129} -> 1\n",
      "130  ->  {130} -> 1\n",
      "132  ->  {132} -> 1\n",
      "133  ->  {235, 133} -> 2\n",
      "134  ->  {134} -> 1\n",
      "136  ->  {136} -> 1\n",
      "138  ->  {138} -> 1\n",
      "139  ->  {169, 139} -> 2\n",
      "140  ->  {140} -> 1\n",
      "142  ->  {142} -> 1\n",
      "143  ->  {143} -> 1\n",
      "144  ->  {144} -> 1\n",
      "145  ->  {145} -> 1\n",
      "146  ->  {146} -> 1\n",
      "147  ->  {147} -> 1\n",
      "148  ->  {148} -> 1\n",
      "149  ->  {149} -> 1\n",
      "151  ->  {153, 250, 151} -> 3\n",
      "152  ->  {152} -> 1\n",
      "154  ->  {154} -> 1\n",
      "155  ->  {155} -> 1\n",
      "156  ->  {156} -> 1\n",
      "158  ->  {240, 158} -> 2\n",
      "159  ->  {159} -> 1\n",
      "161  ->  {161} -> 1\n",
      "162  ->  {162} -> 1\n",
      "163  ->  {163} -> 1\n",
      "164  ->  {164, 165} -> 2\n",
      "166  ->  {166} -> 1\n",
      "167  ->  {295, 167} -> 2\n",
      "170  ->  {170, 290, 261} -> 3\n",
      "171  ->  {171} -> 1\n",
      "172  ->  {172} -> 1\n",
      "174  ->  {174} -> 1\n",
      "175  ->  {175} -> 1\n",
      "176  ->  {176} -> 1\n",
      "177  ->  {177, 237} -> 2\n",
      "178  ->  {178} -> 1\n",
      "179  ->  {179} -> 1\n",
      "180  ->  {257, 180} -> 2\n",
      "181  ->  {181} -> 1\n",
      "183  ->  {183} -> 1\n",
      "185  ->  {185} -> 1\n",
      "186  ->  {186} -> 1\n",
      "187  ->  {187} -> 1\n",
      "188  ->  {188} -> 1\n",
      "189  ->  {189} -> 1\n",
      "190  ->  {190} -> 1\n",
      "191  ->  {191} -> 1\n",
      "192  ->  {192} -> 1\n",
      "193  ->  {193} -> 1\n",
      "194  ->  {194} -> 1\n",
      "195  ->  {195} -> 1\n",
      "196  ->  {196} -> 1\n",
      "197  ->  {253, 197} -> 2\n",
      "200  ->  {200} -> 1\n",
      "201  ->  {201} -> 1\n",
      "203  ->  {203} -> 1\n",
      "204  ->  {243, 204} -> 2\n",
      "205  ->  {205} -> 1\n",
      "206  ->  {206} -> 1\n",
      "207  ->  {207} -> 1\n",
      "208  ->  {208} -> 1\n",
      "209  ->  {209} -> 1\n",
      "210  ->  {210} -> 1\n",
      "212  ->  {212} -> 1\n",
      "214  ->  {274, 214} -> 2\n",
      "215  ->  {215} -> 1\n",
      "216  ->  {216} -> 1\n",
      "217  ->  {217} -> 1\n",
      "218  ->  {218} -> 1\n",
      "220  ->  {220} -> 1\n",
      "223  ->  {247, 223} -> 2\n",
      "224  ->  {224} -> 1\n",
      "225  ->  {225} -> 1\n",
      "227  ->  {227} -> 1\n",
      "228  ->  {282, 228} -> 2\n",
      "229  ->  {229} -> 1\n",
      "230  ->  {230} -> 1\n",
      "231  ->  {231} -> 1\n",
      "232  ->  {232} -> 1\n",
      "233  ->  {233} -> 1\n",
      "234  ->  {234} -> 1\n",
      "236  ->  {236} -> 1\n",
      "238  ->  {238} -> 1\n",
      "239  ->  {239} -> 1\n",
      "241  ->  {241} -> 1\n",
      "242  ->  {242} -> 1\n",
      "244  ->  {244} -> 1\n",
      "245  ->  {245} -> 1\n",
      "246  ->  {246} -> 1\n",
      "248  ->  {248} -> 1\n",
      "249  ->  {249} -> 1\n",
      "251  ->  {251} -> 1\n",
      "252  ->  {252} -> 1\n",
      "254  ->  {254} -> 1\n",
      "255  ->  {255} -> 1\n",
      "256  ->  {256} -> 1\n",
      "258  ->  {258} -> 1\n",
      "259  ->  {259} -> 1\n",
      "260  ->  {260} -> 1\n",
      "263  ->  {263} -> 1\n",
      "264  ->  {264} -> 1\n",
      "267  ->  {267} -> 1\n",
      "268  ->  {268} -> 1\n",
      "270  ->  {270} -> 1\n",
      "271  ->  {271} -> 1\n",
      "272  ->  {272} -> 1\n",
      "273  ->  {273} -> 1\n",
      "275  ->  {275} -> 1\n",
      "276  ->  {276} -> 1\n",
      "277  ->  {277} -> 1\n",
      "279  ->  {279} -> 1\n",
      "280  ->  {280} -> 1\n",
      "281  ->  {281} -> 1\n",
      "283  ->  {283} -> 1\n",
      "284  ->  {284} -> 1\n",
      "285  ->  {285} -> 1\n",
      "286  ->  {286} -> 1\n",
      "287  ->  {287} -> 1\n",
      "288  ->  {288} -> 1\n",
      "289  ->  {289} -> 1\n",
      "291  ->  {291} -> 1\n",
      "292  ->  {292} -> 1\n",
      "293  ->  {293} -> 1\n",
      "294  ->  {294} -> 1\n",
      "298  ->  {298} -> 1\n",
      "299  ->  {299} -> 1\n"
     ]
    }
   ],
   "source": [
    "for key in setpoints.keys():\n",
    "    print(key,\" -> \",setpoints[key], \"->\", len(setpoints[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "print(len(setpoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will take the average of the same points in the same cluster and assign it as a new bucket, and delete all the other nodes. For FC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 256])\n",
      "240 torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc1.weight\"].shape)\n",
    "temp=[]\n",
    "for key in setpoints.keys():\n",
    "    row=torch.zeros(len(net.state_dict()[\"fc1.weight\"][0]), dtype=torch.float)\n",
    "    for points in setpoints[key]:\n",
    "        row+=net.state_dict()[\"fc1.weight\"][points]\n",
    "    row=row/len(setpoints[key])\n",
    "    temp.append(row)\n",
    "print(len(temp), temp[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0272,  0.0675,  0.0587,  ...,  0.0256, -0.0110, -0.0366],\n",
       "        [ 0.0592,  0.0532,  0.0201,  ..., -0.0201, -0.0062,  0.0536],\n",
       "        [-0.0039, -0.0355, -0.0470,  ...,  0.0263, -0.0019, -0.0098],\n",
       "        ...,\n",
       "        [-0.0603,  0.0397, -0.0045,  ..., -0.0434,  0.0040,  0.0199],\n",
       "        [-0.0064,  0.0383, -0.0633,  ...,  0.0619,  0.0373, -0.0214],\n",
       "        [ 0.0334,  0.0250,  0.0170,  ...,  0.0045, -0.0340,  0.0158]])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"fc1.weight\"].resize_(len(temp), len(temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(net.state_dict()[\"fc1.weight\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(temp)):\n",
    "    net.state_dict()[\"fc1.weight\"][i]=temp[i]\n",
    "# print(net.state_dict()[\"fc1.weight\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 256])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc1.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc1.bias\"].shape)\n",
    "temp_bias=torch.zeros(len(temp), dtype=torch.float)\n",
    "i=0\n",
    "for key in setpoints.keys():\n",
    "    for points in setpoints[key]:\n",
    "        temp_bias[i]+=net.state_dict()[\"fc1.bias\"][points]\n",
    "    temp_bias[i]/=len(setpoints[key])\n",
    "    i+=1\n",
    "print(temp_bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0496)\n",
      "tensor(0.0496)\n"
     ]
    }
   ],
   "source": [
    "net.state_dict()[\"fc1.bias\"].resize_(len(temp_bias))\n",
    "print(net.state_dict()[\"fc1.bias\"][1])\n",
    "for i in range(len(temp_bias)):\n",
    "    net.state_dict()[\"fc1.bias\"][i]=temp_bias[i]\n",
    "print(net.state_dict()[\"fc1.bias\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc1.bias\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have to change FC2 accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 300])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 100])\n"
     ]
    }
   ],
   "source": [
    "mat=net.state_dict()[\"fc2.weight\"].t()\n",
    "print(mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "temp=[]\n",
    "for key in setpoints.keys():\n",
    "    row=torch.zeros(len(mat[0]), dtype=torch.float)\n",
    "    for points in setpoints[key]:\n",
    "        row+=mat[points]\n",
    "    temp.append(row)\n",
    "print(len(temp), temp[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 240])\n"
     ]
    }
   ],
   "source": [
    "newmat=torch.stack(temp, dim=0)\n",
    "newmat=newmat.t()\n",
    "print(newmat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 300])\n",
      "tensor(0.0442)\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.weight\"].shape)\n",
    "net.state_dict()[\"fc2.weight\"].resize_(len(newmat), len(newmat[0]))\n",
    "print(net.state_dict()[\"fc2.weight\"][1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 240])\n",
      "tensor(-0.0190)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(newmat)):\n",
    "    net.state_dict()[\"fc2.weight\"][i]=newmat[i]\n",
    "print(net.state_dict()[\"fc2.weight\"].shape)\n",
    "print(net.state_dict()[\"fc2.weight\"][1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 240])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.bias\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.280000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to prune the next layer which consists of 100 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 240])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.state_dict()[\"fc2.weight\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.state_dict()[\"fc3.weight\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   240\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "nodes=defaultdict(list)\n",
    "for j in range(len(net.state_dict()[\"fc2.weight\"])):  #Means j is a node of next layer, L2.\n",
    "    nodes[j]=net.state_dict()[\"fc2.weight\"][j]\n",
    "\n",
    "print(len(nodes), \" \", len(nodes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "distances=[]\n",
    "for i in nodes.keys():\n",
    "    for j in nodes.keys():\n",
    "        if(i!=j and j>i):\n",
    "            distances.append(euclidean_distance(nodes[i], nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n"
     ]
    }
   ],
   "source": [
    "distances.sort()\n",
    "print(len(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "x=[k for k in range(len(distances))]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, distances)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=defaultdict(bool)\n",
    "setpoints=defaultdict(set)\n",
    "for i in nodes.keys():\n",
    "    cluster[i]=False\n",
    "    setpoints[i]={i}\n",
    "    \n",
    "# print(cluster)\n",
    "\n",
    "for i in nodes.keys():\n",
    "    for j in nodes.keys():\n",
    "        if(i!=j and j>i and cluster[i]==False and cluster[j]==False):\n",
    "            \n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            #########   The percentage of reduction in size depends on the cutoff chossen here ######\n",
    "            #########  The higher cut off will result in higher compression  ########################\n",
    "            \n",
    "            cut_off=0.7704\n",
    "            if(euclidean_distance(nodes[i], nodes[j])<cut_off):\n",
    "                setpoints[i].add(j)\n",
    "                del setpoints[j]\n",
    "                cluster[j]=True\n",
    "    cluster[i]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ->  {0, 75}\n",
      "1  ->  {1, 37, 73, 79, 20, 28}\n",
      "2  ->  {2}\n",
      "3  ->  {50, 3}\n",
      "4  ->  {4}\n",
      "5  ->  {76, 5}\n",
      "6  ->  {67, 6, 46, 82, 83, 23, 63, 31}\n",
      "7  ->  {7}\n",
      "8  ->  {8}\n",
      "9  ->  {9}\n",
      "10  ->  {10}\n",
      "11  ->  {11}\n",
      "12  ->  {12}\n",
      "13  ->  {13}\n",
      "14  ->  {14}\n",
      "15  ->  {15}\n",
      "16  ->  {16}\n",
      "17  ->  {17}\n",
      "18  ->  {18}\n",
      "19  ->  {19}\n",
      "21  ->  {25, 21}\n",
      "22  ->  {22}\n",
      "24  ->  {24}\n",
      "26  ->  {26}\n",
      "27  ->  {27}\n",
      "29  ->  {29}\n",
      "30  ->  {30}\n",
      "32  ->  {32}\n",
      "33  ->  {33}\n",
      "34  ->  {34}\n",
      "35  ->  {35}\n",
      "36  ->  {36}\n",
      "38  ->  {38}\n",
      "39  ->  {39}\n",
      "40  ->  {40}\n",
      "41  ->  {41}\n",
      "42  ->  {42}\n",
      "43  ->  {43}\n",
      "44  ->  {88, 44}\n",
      "45  ->  {45}\n",
      "47  ->  {47}\n",
      "48  ->  {48}\n",
      "49  ->  {49}\n",
      "51  ->  {51}\n",
      "52  ->  {52}\n",
      "53  ->  {53}\n",
      "54  ->  {54}\n",
      "55  ->  {55}\n",
      "56  ->  {56, 94}\n",
      "57  ->  {57, 78}\n",
      "58  ->  {58}\n",
      "59  ->  {59}\n",
      "60  ->  {60}\n",
      "61  ->  {61}\n",
      "62  ->  {62}\n",
      "64  ->  {64}\n",
      "65  ->  {65}\n",
      "66  ->  {66}\n",
      "68  ->  {68}\n",
      "69  ->  {69}\n",
      "70  ->  {70}\n",
      "71  ->  {71}\n",
      "72  ->  {72}\n",
      "74  ->  {74}\n",
      "77  ->  {77}\n",
      "80  ->  {80}\n",
      "81  ->  {81}\n",
      "84  ->  {84}\n",
      "85  ->  {85}\n",
      "86  ->  {86}\n",
      "87  ->  {89, 87}\n",
      "90  ->  {90}\n",
      "91  ->  {91}\n",
      "92  ->  {92}\n",
      "93  ->  {93}\n",
      "95  ->  {95}\n",
      "96  ->  {96}\n",
      "97  ->  {97}\n",
      "98  ->  {98}\n",
      "99  ->  {99}\n"
     ]
    }
   ],
   "source": [
    "for key in setpoints.keys():\n",
    "    print(key,\" -> \",setpoints[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(setpoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have i many nodes in place of 100. Now, we will first fix the incoming weights this i many nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 240])\n",
      "80 torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.weight\"].shape)\n",
    "temp=[]\n",
    "for key in setpoints.keys():\n",
    "    row=torch.zeros(len(net.state_dict()[\"fc2.weight\"][0]), dtype=torch.float)\n",
    "    for points in setpoints[key]:\n",
    "        row+=net.state_dict()[\"fc2.weight\"][points]\n",
    "    row=row/len(setpoints[key])\n",
    "    temp.append(row)\n",
    "print(len(temp), temp[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0402, -0.0434,  0.0275,  ..., -0.0646, -0.0236,  0.0146],\n",
       "        [ 0.0027,  0.0438, -0.0190,  ..., -0.0070,  0.0560,  0.0263],\n",
       "        [ 0.0155, -0.0348,  0.0732,  ...,  0.0446,  0.0367,  0.0531],\n",
       "        ...,\n",
       "        [ 0.0384, -0.0197, -0.0223,  ...,  0.0537,  0.0001,  0.0868],\n",
       "        [ 0.0125,  0.0371, -0.0263,  ...,  0.0192,  0.0348, -0.0642],\n",
       "        [ 0.0535,  0.0345,  0.0924,  ...,  0.0457, -0.0332,  0.0375]])"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"fc2.weight\"].resize_(len(temp), len(temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(temp)):\n",
    "    net.state_dict()[\"fc2.weight\"][i]=temp[i]\n",
    "# print(net.state_dict()[\"fc1.weight\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 240])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.bias\"].shape)\n",
    "temp_bias=torch.zeros(len(temp), dtype=torch.float)\n",
    "i=0\n",
    "for key in setpoints.keys():\n",
    "    for points in setpoints[key]:\n",
    "        temp_bias[i]+=net.state_dict()[\"fc2.bias\"][points]\n",
    "    temp_bias[i]/=len(setpoints[key])\n",
    "    i+=1\n",
    "print(temp_bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0087)\n",
      "tensor(0.0082)\n"
     ]
    }
   ],
   "source": [
    "net.state_dict()[\"fc2.bias\"].resize_(len(temp_bias))\n",
    "print(net.state_dict()[\"fc2.bias\"][1])\n",
    "for i in range(len(temp_bias)):\n",
    "    net.state_dict()[\"fc2.bias\"][i]=temp_bias[i]\n",
    "print(net.state_dict()[\"fc2.bias\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc2.bias\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have to change FC3 accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc3.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "mat=net.state_dict()[\"fc3.weight\"].t()\n",
    "print(mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "temp=[]\n",
    "for key in setpoints.keys():\n",
    "    row=torch.zeros(len(mat[0]), dtype=torch.float)\n",
    "    for points in setpoints[key]:\n",
    "        row+=mat[points]\n",
    "    temp.append(row)\n",
    "print(len(temp), temp[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 80])\n"
     ]
    }
   ],
   "source": [
    "newmat=torch.stack(temp, dim=0)\n",
    "newmat=newmat.t()\n",
    "print(newmat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n",
      "tensor(0.0796)\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc3.weight\"].shape)\n",
    "net.state_dict()[\"fc3.weight\"].resize_(len(newmat), len(newmat[0]))\n",
    "print(net.state_dict()[\"fc3.weight\"][1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 80])\n",
      "tensor(-0.1559)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(newmat)):\n",
    "    net.state_dict()[\"fc3.weight\"][i]=newmat[i]\n",
    "print(net.state_dict()[\"fc3.weight\"].shape)\n",
    "print(net.state_dict()[\"fc3.weight\"][1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 80])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc3.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()[\"fc3.bias\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.170000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the accuracy actually come up to 76.5, the actual accuracy without any pruning was 89.7. Let's try to freeze the previous layers, and do a fine_tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for parameter in net.parameters():\n",
    "    i+=1\n",
    "    if(i<5):\n",
    "        parameter.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=200,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   torch.Size([6, 1, 5, 5]) \n",
      "  Parameter containing:\n",
      "tensor([[[[-0.0192,  0.2511,  0.2183,  0.0210,  0.0190],\n",
      "          [-0.0522,  0.3645,  0.3561,  0.5513,  0.3440],\n",
      "          [ 0.0598,  0.5070,  0.6116,  0.6220,  0.2953],\n",
      "          [ 0.2985,  0.1301,  0.4567,  0.1638,  0.2149],\n",
      "          [-0.0246,  0.3206,  0.1249,  0.1288, -0.1632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1775, -0.0977,  0.0400,  0.2623,  0.2931],\n",
      "          [ 0.0318, -0.1988, -0.1956,  0.1583,  0.1678],\n",
      "          [-0.2439, -0.2983, -0.0792,  0.2235,  0.3461],\n",
      "          [-0.0280, -0.2769, -0.1121,  0.0926,  0.3560],\n",
      "          [-0.1970, -0.1645,  0.1938,  0.2115,  0.1824]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1362,  0.2118, -0.2154, -0.2076, -0.2334],\n",
      "          [ 0.2467, -0.0821, -0.1670, -0.0738,  0.2066],\n",
      "          [ 0.2564,  0.1952,  0.2048,  0.3864,  0.0963],\n",
      "          [ 0.0786,  0.3546,  0.3418, -0.0168,  0.1037],\n",
      "          [ 0.0178, -0.2039, -0.1759, -0.0146, -0.0048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299,  0.1694, -0.0029, -0.1827, -0.0718],\n",
      "          [-0.1909, -0.1638,  0.1044, -0.2096,  0.0405],\n",
      "          [ 0.0873, -0.1777, -0.1820, -0.0258, -0.1887],\n",
      "          [ 0.0190, -0.1462, -0.0486,  0.0288, -0.2104],\n",
      "          [ 0.1727, -0.1630, -0.0454, -0.2031, -0.1915]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1378,  0.0087, -0.1978, -0.2727,  0.0345],\n",
      "          [ 0.2285, -0.0154, -0.1342, -0.2998, -0.0739],\n",
      "          [ 0.2801,  0.2191,  0.1079,  0.2947,  0.1320],\n",
      "          [ 0.3344,  0.4029,  0.3628,  0.5148,  0.0400],\n",
      "          [ 0.0682,  0.0300,  0.3382,  0.2907,  0.3091]]],\n",
      "\n",
      "\n",
      "        [[[-0.2584, -0.2308, -0.1856,  0.0330,  0.0465],\n",
      "          [-0.2978, -0.1939, -0.2057,  0.0400,  0.1692],\n",
      "          [ 0.0883, -0.1518,  0.0861, -0.1625,  0.0874],\n",
      "          [ 0.1464,  0.2543,  0.0292,  0.0581, -0.0674],\n",
      "          [ 0.3818,  0.3657,  0.2362, -0.0540,  0.1097]]]]) \n",
      "\n",
      "\n",
      "2   torch.Size([6]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.2434, -0.0632,  0.2198,  0.1060,  0.1912,  0.0086]) \n",
      "\n",
      "\n",
      "3   torch.Size([16, 6, 5, 5]) \n",
      "  Parameter containing:\n",
      "tensor([[[[-0.0109, -0.0548, -0.0268,  0.0555,  0.0529],\n",
      "          [-0.0747, -0.0093, -0.0871,  0.0756,  0.0290],\n",
      "          [ 0.0085, -0.0744, -0.1693, -0.1346, -0.1455],\n",
      "          [ 0.0382, -0.0446, -0.1455, -0.0654, -0.0830],\n",
      "          [ 0.0522,  0.1614,  0.1331,  0.1559,  0.0276]],\n",
      "\n",
      "         [[-0.0135,  0.0531,  0.0238,  0.0102,  0.0312],\n",
      "          [ 0.0628, -0.0327,  0.0206, -0.0319,  0.0428],\n",
      "          [-0.0834, -0.1235, -0.0557,  0.0262, -0.0169],\n",
      "          [ 0.0244, -0.0334,  0.0382, -0.0140,  0.0345],\n",
      "          [ 0.1239, -0.0369, -0.0981, -0.0913, -0.0969]],\n",
      "\n",
      "         [[-0.0242, -0.0831,  0.0583,  0.0632, -0.0662],\n",
      "          [-0.0261, -0.0701,  0.0334, -0.0154,  0.0068],\n",
      "          [-0.0244,  0.0824, -0.1057, -0.1036,  0.0335],\n",
      "          [-0.0140,  0.1386,  0.0705,  0.0171, -0.0072],\n",
      "          [ 0.0018,  0.1739,  0.1223,  0.0542,  0.0006]],\n",
      "\n",
      "         [[ 0.0685,  0.0419, -0.0685, -0.0128, -0.0514],\n",
      "          [ 0.0531,  0.0907, -0.0022,  0.0279,  0.0706],\n",
      "          [ 0.0644,  0.0581,  0.0447,  0.0862, -0.0299],\n",
      "          [-0.0563, -0.0664,  0.0204, -0.0461, -0.0049],\n",
      "          [-0.0201, -0.0669, -0.0425, -0.0638,  0.0590]],\n",
      "\n",
      "         [[-0.0472,  0.0201, -0.0225, -0.0141, -0.0058],\n",
      "          [ 0.0624,  0.0067,  0.0042,  0.0046,  0.0419],\n",
      "          [ 0.0394,  0.0676, -0.0732, -0.1638, -0.0343],\n",
      "          [ 0.1362,  0.1999,  0.0978,  0.1239,  0.0379],\n",
      "          [ 0.0652,  0.1395,  0.1929,  0.1191,  0.0623]],\n",
      "\n",
      "         [[ 0.0314, -0.0123, -0.0633,  0.0803,  0.0407],\n",
      "          [-0.0420,  0.0859,  0.0217, -0.0667,  0.0375],\n",
      "          [ 0.0298,  0.0806,  0.0096,  0.0557, -0.0537],\n",
      "          [ 0.0555, -0.0591,  0.0920,  0.1185,  0.0150],\n",
      "          [-0.0733, -0.1120, -0.0059,  0.1112,  0.0869]]],\n",
      "\n",
      "\n",
      "        [[[-0.0222, -0.0632,  0.0230,  0.0518,  0.0471],\n",
      "          [-0.0131, -0.0666, -0.0177, -0.0039,  0.0601],\n",
      "          [ 0.0307,  0.0119,  0.1052, -0.0188,  0.0316],\n",
      "          [-0.0572, -0.0849, -0.0043,  0.1368,  0.0699],\n",
      "          [-0.0658, -0.1288, -0.1282, -0.0940, -0.0316]],\n",
      "\n",
      "         [[ 0.0440,  0.0498,  0.0480,  0.0980,  0.0044],\n",
      "          [-0.0486,  0.0267,  0.0349, -0.0436,  0.0280],\n",
      "          [-0.0089,  0.0524,  0.0789, -0.0944, -0.0586],\n",
      "          [-0.0659,  0.0035,  0.0432, -0.0714, -0.0228],\n",
      "          [ 0.0241, -0.0759, -0.0205,  0.0458, -0.0653]],\n",
      "\n",
      "         [[-0.0281,  0.0291, -0.0404,  0.0430, -0.0491],\n",
      "          [ 0.0727, -0.0770, -0.0016,  0.0005,  0.0026],\n",
      "          [-0.0638,  0.0402,  0.0268,  0.0278, -0.0408],\n",
      "          [-0.0593, -0.0857, -0.0497,  0.1150,  0.1128],\n",
      "          [-0.0520, -0.0732, -0.0377, -0.0443, -0.0022]],\n",
      "\n",
      "         [[-0.0036, -0.0737,  0.0009,  0.0342,  0.0625],\n",
      "          [ 0.0713, -0.0301, -0.0545, -0.0360, -0.0155],\n",
      "          [ 0.0323,  0.0786, -0.0489, -0.0150, -0.0732],\n",
      "          [-0.0087,  0.0101,  0.0736, -0.0299,  0.0564],\n",
      "          [ 0.0291, -0.0514,  0.0165, -0.0648, -0.0488]],\n",
      "\n",
      "         [[-0.0781,  0.0070, -0.0439,  0.0522,  0.0101],\n",
      "          [-0.0655, -0.0328,  0.0275, -0.0029, -0.0499],\n",
      "          [-0.0462,  0.0460,  0.0361,  0.1371,  0.0840],\n",
      "          [ 0.0280, -0.0768,  0.0399,  0.0632,  0.1553],\n",
      "          [ 0.0167,  0.0032, -0.0908, -0.1111, -0.0763]],\n",
      "\n",
      "         [[ 0.0044,  0.0608, -0.0532, -0.0601,  0.0587],\n",
      "          [-0.0058, -0.0142, -0.0047, -0.0417, -0.0031],\n",
      "          [-0.0824, -0.0439, -0.0758, -0.0655, -0.0323],\n",
      "          [ 0.0141, -0.0282,  0.0304,  0.0188, -0.0538],\n",
      "          [-0.0402,  0.0069,  0.0732,  0.0188, -0.0666]]],\n",
      "\n",
      "\n",
      "        [[[-0.0290, -0.0331, -0.0362, -0.0128, -0.1624],\n",
      "          [ 0.0264,  0.0704,  0.0559,  0.0057, -0.1444],\n",
      "          [ 0.0334, -0.0197,  0.0786,  0.1296, -0.0665],\n",
      "          [-0.1325, -0.0002, -0.0102,  0.0673,  0.1488],\n",
      "          [ 0.0281,  0.0752,  0.1247,  0.0654,  0.0795]],\n",
      "\n",
      "         [[ 0.0432,  0.0453, -0.0163, -0.0221, -0.0871],\n",
      "          [ 0.0297,  0.0078, -0.0213, -0.0102,  0.0096],\n",
      "          [-0.0610,  0.0428,  0.1683,  0.1447,  0.1145],\n",
      "          [ 0.0129,  0.1065,  0.1422,  0.0659,  0.0251],\n",
      "          [-0.0014,  0.0767,  0.0355,  0.0775,  0.0778]],\n",
      "\n",
      "         [[-0.0408,  0.0344,  0.0030, -0.0213, -0.0086],\n",
      "          [ 0.0332,  0.0192,  0.0254,  0.0032,  0.0026],\n",
      "          [-0.0381, -0.0148,  0.0117,  0.0037,  0.0669],\n",
      "          [ 0.0096, -0.0044,  0.0353, -0.0138,  0.0783],\n",
      "          [ 0.0304,  0.0076,  0.0405,  0.0741,  0.0048]],\n",
      "\n",
      "         [[-0.0272,  0.0014, -0.0568,  0.0614,  0.0953],\n",
      "          [ 0.0689, -0.0018,  0.0250,  0.0122,  0.0488],\n",
      "          [ 0.0131, -0.0347,  0.0192,  0.0197, -0.0680],\n",
      "          [ 0.0553, -0.0440, -0.0922, -0.0444,  0.0591],\n",
      "          [ 0.0409, -0.0933,  0.0049, -0.0250,  0.0385]],\n",
      "\n",
      "         [[ 0.0040,  0.0977,  0.0278,  0.0440, -0.1256],\n",
      "          [ 0.0426,  0.0925,  0.1221,  0.1534, -0.0139],\n",
      "          [-0.0277,  0.0244, -0.0303,  0.1574,  0.0935],\n",
      "          [-0.0314,  0.0749,  0.0602,  0.0665,  0.0982],\n",
      "          [ 0.0176,  0.0285,  0.0090,  0.0930, -0.0757]],\n",
      "\n",
      "         [[-0.0138,  0.1097,  0.0643,  0.1291,  0.1020],\n",
      "          [ 0.0214,  0.0248, -0.0334,  0.1013,  0.0531],\n",
      "          [-0.0008, -0.0636,  0.0248, -0.0394, -0.0282],\n",
      "          [ 0.1024,  0.0172,  0.0456, -0.0545,  0.0182],\n",
      "          [-0.0197,  0.1342,  0.0421,  0.0906,  0.0427]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0875,  0.0322,  0.0375,  0.0408,  0.0054],\n",
      "          [-0.0460, -0.0633, -0.0649,  0.0760, -0.0567],\n",
      "          [ 0.0322, -0.0105, -0.0606, -0.0797, -0.0679],\n",
      "          [-0.0232, -0.0553, -0.0724,  0.0120, -0.0809],\n",
      "          [ 0.0408, -0.0291, -0.0568, -0.0558, -0.0341]],\n",
      "\n",
      "         [[-0.0568, -0.0631,  0.0572, -0.0491, -0.0536],\n",
      "          [-0.0138,  0.0440,  0.0787,  0.0738,  0.0220],\n",
      "          [-0.0163,  0.0168, -0.0073,  0.0200, -0.0652],\n",
      "          [ 0.0009,  0.0010, -0.0487, -0.0715,  0.0072],\n",
      "          [-0.0719, -0.0316, -0.0134, -0.0032,  0.0724]],\n",
      "\n",
      "         [[ 0.0582, -0.0085,  0.0132, -0.0703,  0.0159],\n",
      "          [ 0.0671, -0.0448,  0.0393, -0.0356,  0.0394],\n",
      "          [ 0.0450,  0.0304, -0.0662,  0.0135,  0.0431],\n",
      "          [-0.0309,  0.0268, -0.0411, -0.0587,  0.0662],\n",
      "          [ 0.0126,  0.0711, -0.0226, -0.0170,  0.0568]],\n",
      "\n",
      "         [[ 0.0282, -0.0763, -0.0282, -0.0218, -0.0370],\n",
      "          [ 0.0586, -0.0726, -0.0211, -0.0571, -0.0420],\n",
      "          [ 0.0335,  0.0127,  0.0681,  0.0412,  0.0192],\n",
      "          [-0.0766, -0.0016, -0.0073,  0.0633, -0.0200],\n",
      "          [-0.0027, -0.0093,  0.0077, -0.0436,  0.0106]],\n",
      "\n",
      "         [[-0.0392,  0.0598, -0.0669,  0.0659, -0.0636],\n",
      "          [-0.0772, -0.0788, -0.0499, -0.0290,  0.0244],\n",
      "          [-0.0204, -0.0283,  0.0083, -0.0568,  0.0490],\n",
      "          [ 0.0234, -0.0123, -0.0521,  0.0399, -0.0175],\n",
      "          [ 0.0072, -0.0435, -0.0295, -0.0564, -0.0306]],\n",
      "\n",
      "         [[ 0.0560, -0.0579, -0.0484,  0.0134,  0.0548],\n",
      "          [-0.0322,  0.0032,  0.0593, -0.0421, -0.0552],\n",
      "          [ 0.0751,  0.0288, -0.0811, -0.0192, -0.0289],\n",
      "          [-0.0103,  0.0337, -0.0256, -0.0466,  0.0394],\n",
      "          [ 0.0175,  0.0412, -0.0659, -0.0206,  0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340,  0.0957,  0.0862, -0.0030,  0.0326],\n",
      "          [ 0.0625,  0.1338,  0.0669, -0.0977,  0.0272],\n",
      "          [ 0.1258,  0.2758,  0.0864, -0.1109, -0.1041],\n",
      "          [ 0.1510,  0.1942,  0.0006, -0.1443, -0.1476],\n",
      "          [ 0.1227,  0.0747, -0.0599, -0.0265, -0.0703]],\n",
      "\n",
      "         [[ 0.0696,  0.0604,  0.0966,  0.0341, -0.0003],\n",
      "          [ 0.2600,  0.1970,  0.0654, -0.0413,  0.0700],\n",
      "          [ 0.2760,  0.1238,  0.0613,  0.0552, -0.0365],\n",
      "          [ 0.2264,  0.0426,  0.0530, -0.0057,  0.0397],\n",
      "          [ 0.0955,  0.0771,  0.0420, -0.0585, -0.0408]],\n",
      "\n",
      "         [[-0.0805, -0.0571,  0.0686,  0.0679,  0.0019],\n",
      "          [ 0.0002, -0.0732,  0.0858,  0.0114, -0.0765],\n",
      "          [-0.0982,  0.0699,  0.1299,  0.0603, -0.0301],\n",
      "          [-0.0205,  0.0617,  0.0674,  0.0328,  0.0286],\n",
      "          [-0.0963,  0.0384,  0.1049,  0.1011,  0.0627]],\n",
      "\n",
      "         [[ 0.0357,  0.0340,  0.0522,  0.0495,  0.0708],\n",
      "          [-0.0845, -0.0224, -0.0304, -0.0551, -0.0190],\n",
      "          [-0.0668,  0.0409, -0.0619, -0.0562,  0.0009],\n",
      "          [ 0.0180, -0.0264,  0.0483,  0.0653,  0.0173],\n",
      "          [ 0.0295,  0.0553,  0.0585, -0.0805,  0.0243]],\n",
      "\n",
      "         [[ 0.1076, -0.0023, -0.0163, -0.0140,  0.0564],\n",
      "          [ 0.0750,  0.0792, -0.0273,  0.0257, -0.0842],\n",
      "          [ 0.0222,  0.0533, -0.0139, -0.0419, -0.0244],\n",
      "          [ 0.0319,  0.0712,  0.0382,  0.0173,  0.0096],\n",
      "          [-0.0934,  0.0405,  0.1524,  0.0470,  0.0904]],\n",
      "\n",
      "         [[-0.0850,  0.0575,  0.0297,  0.0397,  0.0481],\n",
      "          [-0.0801, -0.0908, -0.0910, -0.0494,  0.0574],\n",
      "          [ 0.1137, -0.0567, -0.1489, -0.0140,  0.0657],\n",
      "          [ 0.0604, -0.0574, -0.0979,  0.0109,  0.0994],\n",
      "          [ 0.0521, -0.0905,  0.0108,  0.0659,  0.0968]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1651,  0.1899,  0.1213,  0.0844,  0.0381],\n",
      "          [ 0.0872,  0.0726,  0.1388,  0.1072,  0.0381],\n",
      "          [-0.0822, -0.0235, -0.0253,  0.0575,  0.0126],\n",
      "          [-0.1143, -0.0686,  0.0064,  0.0676,  0.0519],\n",
      "          [-0.0414, -0.0143, -0.0119,  0.0260,  0.0315]],\n",
      "\n",
      "         [[-0.0832, -0.0186,  0.0401, -0.0030,  0.0215],\n",
      "          [-0.0418,  0.0453,  0.0292,  0.0065,  0.0488],\n",
      "          [ 0.0127, -0.1047,  0.0074,  0.0770,  0.0699],\n",
      "          [-0.1040,  0.0111, -0.0547,  0.0259,  0.0403],\n",
      "          [-0.0534, -0.0387, -0.0023,  0.0301, -0.0905]],\n",
      "\n",
      "         [[ 0.0518,  0.1405,  0.1145,  0.0954, -0.0037],\n",
      "          [-0.0601,  0.0425, -0.0251,  0.0921, -0.0411],\n",
      "          [-0.0104, -0.0991, -0.0232, -0.0040,  0.0175],\n",
      "          [ 0.0217, -0.0569, -0.0210, -0.0877,  0.0530],\n",
      "          [ 0.0366, -0.0513,  0.0025, -0.0891,  0.0799]],\n",
      "\n",
      "         [[ 0.0528,  0.0358, -0.0843,  0.0293,  0.0443],\n",
      "          [ 0.0218,  0.0180, -0.0699, -0.0587,  0.0115],\n",
      "          [ 0.0040,  0.0050,  0.0047,  0.0408,  0.0669],\n",
      "          [ 0.0741, -0.0620,  0.0226,  0.0472,  0.0682],\n",
      "          [ 0.0717,  0.0853, -0.0000, -0.0112, -0.0270]],\n",
      "\n",
      "         [[ 0.1038,  0.1426,  0.0998,  0.0320,  0.1111],\n",
      "          [-0.1177, -0.0672,  0.0677,  0.1034, -0.0451],\n",
      "          [-0.0871, -0.1239, -0.0099, -0.0290, -0.0613],\n",
      "          [-0.0878, -0.0596, -0.0144, -0.0210,  0.0641],\n",
      "          [-0.0289,  0.0519, -0.0234,  0.0174,  0.0056]],\n",
      "\n",
      "         [[-0.0465,  0.0461,  0.0285,  0.0367,  0.0337],\n",
      "          [ 0.0520, -0.0826, -0.0852, -0.0947, -0.1000],\n",
      "          [ 0.0594,  0.0626,  0.0371,  0.0097, -0.0623],\n",
      "          [ 0.0788, -0.0128,  0.0158, -0.0668, -0.0158],\n",
      "          [-0.0014,  0.0584, -0.0293, -0.0480, -0.0613]]]]) \n",
      "\n",
      "\n",
      "4   torch.Size([16]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0730,  0.0296,  0.0032,  0.0657,  0.0553,  0.0152, -0.0441, -0.0667,\n",
      "         0.0277,  0.0222, -0.0358,  0.0246,  0.0430, -0.0688, -0.0241,  0.0027]) \n",
      "\n",
      "\n",
      "5   torch.Size([240, 256]) \n",
      "  Parameter containing:\n",
      "tensor([[ 0.0272,  0.0675,  0.0587,  ...,  0.0256, -0.0110, -0.0366],\n",
      "        [ 0.0592,  0.0532,  0.0201,  ..., -0.0201, -0.0062,  0.0536],\n",
      "        [-0.0185, -0.0463, -0.0285,  ..., -0.0039,  0.0103,  0.0060],\n",
      "        ...,\n",
      "        [-0.0527, -0.0293,  0.0001,  ...,  0.0308, -0.0021, -0.0423],\n",
      "        [-0.0405,  0.0220, -0.0051,  ...,  0.0582, -0.0126, -0.0565],\n",
      "        [ 0.0613,  0.0368, -0.0311,  ..., -0.0378, -0.0014,  0.0345]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "6   torch.Size([240]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0125,  0.0496,  0.0044,  0.0121,  0.0070, -0.0100, -0.0080,  0.0135,\n",
      "        -0.0241, -0.0519, -0.0182, -0.0311, -0.0108,  0.0160, -0.0341,  0.0336,\n",
      "        -0.0497, -0.0399, -0.0287, -0.0264, -0.0028,  0.0136,  0.0220, -0.0397,\n",
      "         0.0408, -0.0093, -0.0023,  0.0218, -0.0139,  0.0099, -0.0040,  0.0283,\n",
      "        -0.0100, -0.0033,  0.0250,  0.0024,  0.0368, -0.0242, -0.0286, -0.0415,\n",
      "         0.0308, -0.0437,  0.0435,  0.0390, -0.0533, -0.0025,  0.0445, -0.0422,\n",
      "         0.0325, -0.0621, -0.0068,  0.0149, -0.0306,  0.0367, -0.0544, -0.0037,\n",
      "         0.0219,  0.0466,  0.0304,  0.0454,  0.0361,  0.0541, -0.0258, -0.0537,\n",
      "        -0.0236, -0.0620,  0.0299,  0.0376,  0.0496, -0.0519,  0.0416,  0.0005,\n",
      "        -0.0177, -0.0145,  0.0305, -0.0425,  0.0313,  0.0406,  0.0041,  0.0194,\n",
      "        -0.0015, -0.0381,  0.0463,  0.0631,  0.0469, -0.0166, -0.0199, -0.0257,\n",
      "        -0.0614, -0.0028,  0.0063, -0.0429, -0.0006,  0.0413, -0.0580,  0.0460,\n",
      "         0.0103,  0.0018,  0.0094,  0.0230,  0.0387, -0.0001, -0.0575,  0.0499,\n",
      "        -0.0169,  0.0148,  0.0406, -0.0347, -0.0067,  0.0232,  0.0232, -0.0032,\n",
      "         0.0292, -0.0203,  0.0126, -0.0544,  0.0207,  0.0525,  0.0590, -0.0264,\n",
      "        -0.0445, -0.0070, -0.0509,  0.0348,  0.0296, -0.0262, -0.0463,  0.0530,\n",
      "        -0.0203, -0.0352,  0.0414,  0.0606,  0.0647, -0.0044, -0.0362,  0.0146,\n",
      "        -0.0190,  0.0125,  0.0080, -0.0085, -0.0305,  0.0045,  0.0094, -0.0257,\n",
      "         0.0368,  0.0030, -0.0122, -0.0206, -0.0216,  0.0122, -0.0050, -0.0112,\n",
      "        -0.0105, -0.0145,  0.0082,  0.0559, -0.0274, -0.0482, -0.0504, -0.0238,\n",
      "         0.0578,  0.0111, -0.0489,  0.0005,  0.0606, -0.0305, -0.0007, -0.0125,\n",
      "         0.0277, -0.0471, -0.0419, -0.0444, -0.0058,  0.0355,  0.0439, -0.0299,\n",
      "        -0.0286, -0.0086, -0.0059,  0.0480,  0.0576, -0.0442, -0.0399,  0.0131,\n",
      "         0.0269, -0.0147, -0.0120,  0.0264,  0.0351,  0.0585,  0.0324, -0.0166,\n",
      "        -0.0555,  0.0493, -0.0615,  0.0566,  0.0540, -0.0159, -0.0540, -0.0359,\n",
      "         0.0491, -0.0252,  0.0568, -0.0021, -0.0235,  0.0607,  0.0233,  0.0298,\n",
      "         0.0329,  0.0625,  0.0386, -0.0303,  0.0186, -0.0094,  0.0109, -0.0081,\n",
      "         0.0345, -0.0037, -0.0351,  0.0441,  0.0274,  0.0627, -0.0417,  0.0531,\n",
      "         0.0050,  0.0328, -0.0356, -0.0267,  0.0032, -0.0200,  0.0475, -0.0275,\n",
      "        -0.0526,  0.0087,  0.0063,  0.0208, -0.0253, -0.0119,  0.0094,  0.0507],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "7   torch.Size([80, 240]) \n",
      "  Parameter containing:\n",
      "tensor([[ 0.0084, -0.0361, -0.0175,  ..., -0.0620,  0.0029, -0.0146],\n",
      "        [ 0.0143,  0.0071,  0.0128,  ..., -0.0011,  0.0090,  0.0324],\n",
      "        [ 0.0155, -0.0348,  0.0732,  ...,  0.0446,  0.0367,  0.0531],\n",
      "        ...,\n",
      "        [ 0.0570, -0.0697,  0.0636,  ..., -0.0592, -0.0513, -0.0107],\n",
      "        [-0.0099,  0.0150,  0.0400,  ...,  0.0377, -0.0225,  0.0330],\n",
      "        [-0.0361,  0.0464, -0.0141,  ..., -0.0551, -0.0425, -0.0736]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "8   torch.Size([80]) \n",
      "  Parameter containing:\n",
      "tensor([ 0.0096,  0.0082, -0.0203,  0.0111,  0.0399,  0.0049,  0.0011,  0.0263,\n",
      "        -0.0335,  0.0092, -0.0015, -0.0373,  0.0526,  0.0600,  0.0099, -0.0135,\n",
      "         0.0540,  0.0312,  0.0291,  0.0140,  0.0221, -0.0266,  0.0078, -0.0094,\n",
      "         0.0693,  0.0248,  0.0119,  0.0599, -0.0495, -0.0417, -0.0585, -0.0123,\n",
      "        -0.0193, -0.0215,  0.0462, -0.0107, -0.0284,  0.0102, -0.0220,  0.0080,\n",
      "        -0.0338,  0.0525, -0.0359,  0.0639,  0.0477,  0.0078,  0.0393,  0.0211,\n",
      "        -0.0302,  0.0180, -0.0399, -0.0345, -0.0042, -0.0272,  0.0004, -0.0283,\n",
      "         0.0067, -0.0566,  0.0067,  0.0128, -0.0366, -0.0135, -0.0255,  0.0588,\n",
      "         0.0009,  0.0142,  0.0051,  0.0548,  0.0299,  0.0294, -0.0410,  0.0204,\n",
      "         0.0366, -0.0392, -0.0465, -0.0336, -0.0278,  0.0443,  0.0489, -0.0206],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "9   torch.Size([10, 80]) \n",
      "  Parameter containing:\n",
      "tensor([[-0.0733, -0.1590,  0.0154,  0.1316, -0.0871, -0.0998,  0.4671, -0.0554,\n",
      "         -0.0946, -0.0866, -0.1241,  0.0442, -0.0847,  0.1099, -0.0807, -0.0870,\n",
      "         -0.0052,  0.0141, -0.1187,  0.0135, -0.0824,  0.0971, -0.0481, -0.0263,\n",
      "         -0.1137,  0.1041,  0.0754, -0.0155,  0.1341, -0.0931, -0.0810,  0.0836,\n",
      "          0.1427, -0.1458, -0.1428,  0.0642,  0.0834, -0.0122,  0.0625, -0.0866,\n",
      "         -0.0585,  0.1132, -0.0641,  0.0277,  0.0624, -0.0949, -0.0213,  0.1714,\n",
      "         -0.2095, -0.2436, -0.0149,  0.1095, -0.0326, -0.0129,  0.1646, -0.1588,\n",
      "         -0.0397, -0.0864,  0.0431,  0.0588, -0.0666,  0.0588,  0.0749, -0.0775,\n",
      "          0.0007, -0.1363,  0.0115,  0.0489, -0.0417, -0.0408,  0.3115, -0.0059,\n",
      "         -0.0451, -0.0528,  0.1137,  0.1330, -0.0167, -0.0833, -0.0043,  0.0967],\n",
      "        [ 0.1953, -0.0254, -0.1559, -0.1036, -0.0907, -0.1453,  0.0577, -0.1048,\n",
      "          0.1263, -0.1483,  0.1830, -0.1227,  0.1579,  0.0976, -0.0630, -0.0642,\n",
      "          0.0642,  0.0171,  0.0748, -0.0418,  0.2843,  0.1188, -0.0569,  0.1514,\n",
      "          0.1882, -0.0848, -0.0153,  0.1883, -0.0972, -0.0972, -0.0510, -0.1072,\n",
      "         -0.0642,  0.1352,  0.0150, -0.0173, -0.1752,  0.1126, -0.1057,  0.0020,\n",
      "         -0.0967,  0.1348, -0.1442, -0.1404,  0.1202, -0.0748, -0.1379,  0.0576,\n",
      "          0.2143, -0.1642,  0.1180, -0.1181,  0.1069,  0.1197,  0.1634,  0.1384,\n",
      "         -0.0552,  0.0004, -0.0005, -0.0646, -0.0370, -0.0633, -0.0284, -0.1731,\n",
      "         -0.1579,  0.0633, -0.0766, -0.0060, -0.0381,  0.0882,  0.0085,  0.0548,\n",
      "         -0.0701, -0.0373, -0.1803, -0.0843, -0.0158, -0.1036,  0.0492, -0.0733],\n",
      "        [ 0.0547, -0.0277,  0.0872, -0.0189, -0.0968,  0.1214, -0.1844, -0.0014,\n",
      "         -0.0788,  0.0679,  0.0367,  0.0653,  0.2598, -0.0424, -0.0725,  0.0351,\n",
      "          0.0587,  0.0893, -0.1037, -0.0686, -0.1004,  0.1333,  0.0858, -0.0039,\n",
      "         -0.0715, -0.0254,  0.0300,  0.1893,  0.0260, -0.0539,  0.0239,  0.2075,\n",
      "         -0.1141,  0.0259,  0.0277, -0.0784, -0.0361, -0.0954, -0.0077, -0.0387,\n",
      "         -0.1756,  0.0812,  0.0002,  0.1229,  0.0020, -0.0592,  0.0071,  0.0194,\n",
      "          0.0358, -0.3431, -0.0266, -0.0476, -0.0040, -0.1298,  0.0797, -0.0125,\n",
      "         -0.0713,  0.0886, -0.1828,  0.1266, -0.0312, -0.1565, -0.0008, -0.0532,\n",
      "          0.2707,  0.0780,  0.1225, -0.1364, -0.1227, -0.0573,  0.2000, -0.0558,\n",
      "         -0.0893,  0.0545, -0.0031, -0.1526, -0.1887,  0.0908, -0.0388,  0.1185],\n",
      "        [-0.1684, -0.0802, -0.0778, -0.0139, -0.1300,  0.0979, -0.2545,  0.0063,\n",
      "         -0.0726, -0.1231,  0.0053,  0.0911, -0.0725, -0.1458, -0.0792,  0.0743,\n",
      "          0.0184,  0.0899,  0.2005, -0.1634, -0.2034, -0.0801, -0.1274, -0.0943,\n",
      "          0.0627,  0.0798,  0.0219,  0.1264,  0.0201,  0.1231, -0.0776, -0.0175,\n",
      "          0.2067,  0.1731,  0.1828,  0.0337, -0.0757,  0.1103, -0.0044, -0.0247,\n",
      "         -0.1289, -0.1138,  0.1339,  0.0968, -0.1750, -0.0275, -0.1629, -0.1297,\n",
      "          0.1268, -0.0789, -0.0119, -0.0819, -0.0517,  0.1247, -0.0554, -0.1113,\n",
      "          0.0962, -0.1201,  0.1305, -0.0559, -0.1068, -0.1020, -0.0739,  0.1114,\n",
      "          0.1589,  0.0888,  0.0635, -0.0781,  0.0995, -0.0672,  0.0309, -0.1168,\n",
      "          0.0129, -0.0206,  0.0918, -0.0447,  0.0651,  0.0980, -0.0730,  0.1064],\n",
      "        [ 0.0226,  0.1309, -0.1721, -0.0327,  0.0103, -0.0129,  0.0674, -0.0764,\n",
      "         -0.0590,  0.0773,  0.0813,  0.1136,  0.0045, -0.0662,  0.0648, -0.1574,\n",
      "          0.0122, -0.0855, -0.0298, -0.1251,  0.0953,  0.1165, -0.1141, -0.1431,\n",
      "         -0.0167, -0.0586, -0.1808, -0.1278,  0.0701, -0.1002,  0.0631, -0.0676,\n",
      "          0.0505, -0.0883, -0.0518,  0.0285,  0.0013, -0.1970,  0.0169,  0.0546,\n",
      "          0.0711, -0.0467, -0.0935, -0.0608,  0.0216,  0.0222,  0.0968, -0.0574,\n",
      "          0.1574,  0.2586,  0.1699,  0.1728,  0.2329, -0.0513,  0.0553,  0.0237,\n",
      "         -0.1188,  0.1711,  0.0112,  0.0978,  0.0454,  0.0862,  0.0530,  0.0068,\n",
      "         -0.1468,  0.0660,  0.0521,  0.1892, -0.1432,  0.0500,  0.0205, -0.1290,\n",
      "          0.0328, -0.0114, -0.1939,  0.0200, -0.0886, -0.0748, -0.0414, -0.0459],\n",
      "        [-0.1061,  0.2310, -0.1089, -0.0214,  0.1510,  0.2443,  0.3093, -0.0335,\n",
      "          0.0098, -0.0335, -0.0313,  0.0474, -0.2167,  0.0931,  0.0591,  0.1774,\n",
      "          0.0661, -0.1510, -0.0437,  0.0292,  0.0199, -0.1072,  0.1941, -0.0666,\n",
      "          0.1146, -0.0447,  0.0142, -0.0295,  0.0406,  0.1028,  0.1352,  0.0855,\n",
      "         -0.0241, -0.0670,  0.0759,  0.0260, -0.1360,  0.1547,  0.0266, -0.0346,\n",
      "          0.0071, -0.1181, -0.0383,  0.0827,  0.0534,  0.0518, -0.1492,  0.0719,\n",
      "         -0.1352,  0.2724, -0.0322,  0.0634, -0.1965,  0.0355, -0.1877, -0.0153,\n",
      "          0.1204, -0.0848, -0.0121,  0.0730, -0.0186,  0.0754,  0.0164,  0.1594,\n",
      "         -0.0528, -0.0707, -0.0935, -0.0352,  0.0623, -0.0253, -0.3023,  0.1316,\n",
      "         -0.1226, -0.0330,  0.1635, -0.0575,  0.2134, -0.1288, -0.0973, -0.1462],\n",
      "        [-0.1933, -0.0051,  0.0945, -0.0279,  0.0137, -0.2741, -0.0436, -0.0381,\n",
      "          0.0615,  0.1791,  0.0682, -0.0547, -0.1165,  0.0160, -0.0677,  0.1215,\n",
      "          0.1270,  0.0552,  0.0252,  0.0310,  0.0038, -0.0192,  0.1169, -0.0670,\n",
      "         -0.1821, -0.0484, -0.0966, -0.1199, -0.0362, -0.0371, -0.0377, -0.0871,\n",
      "         -0.0467, -0.1680, -0.0357,  0.0793, -0.1178, -0.0236,  0.0928, -0.0153,\n",
      "         -0.0225,  0.0352, -0.2038,  0.0369, -0.0268,  0.0086,  0.0528,  0.2330,\n",
      "         -0.1044,  0.0625, -0.0252,  0.0119, -0.1204, -0.0712, -0.1209,  0.0809,\n",
      "          0.0293,  0.0905, -0.0375, -0.0021,  0.1119,  0.0360, -0.0401, -0.1174,\n",
      "         -0.0625, -0.0951, -0.1556, -0.0276,  0.1907,  0.1193,  0.1887, -0.0323,\n",
      "         -0.2167, -0.0499, -0.0226,  0.2766,  0.0613,  0.0723, -0.1194,  0.0362],\n",
      "        [ 0.1423, -0.0397, -0.0470, -0.0024, -0.0911,  0.2437, -0.0178,  0.0406,\n",
      "          0.0422, -0.0780, -0.0071, -0.2156, -0.0201, -0.0144, -0.0221, -0.0102,\n",
      "         -0.0019, -0.1399,  0.1202, -0.0799,  0.0633,  0.0086, -0.1784,  0.0851,\n",
      "          0.0469, -0.0062, -0.1095,  0.0282,  0.0649, -0.0995, -0.0791,  0.1417,\n",
      "         -0.0900,  0.0731, -0.1580,  0.0366,  0.1625, -0.1173, -0.0123,  0.0885,\n",
      "         -0.0279,  0.0966,  0.2265, -0.0968,  0.1544, -0.0177, -0.0833, -0.0294,\n",
      "          0.1901, -0.1395,  0.1117,  0.0786, -0.0282, -0.1254, -0.0582, -0.1701,\n",
      "         -0.0076, -0.1343, -0.1298, -0.0760, -0.0093, -0.0526,  0.0641,  0.0920,\n",
      "         -0.0053,  0.0351, -0.1147, -0.0274, -0.1307, -0.1444,  0.1338,  0.0456,\n",
      "          0.0969,  0.0502,  0.1024, -0.1740, -0.1500,  0.0137, -0.0495,  0.0711],\n",
      "        [ 0.0956,  0.1271,  0.0873,  0.0539,  0.0590, -0.0028,  0.4310,  0.0666,\n",
      "         -0.1361,  0.1357, -0.0054, -0.0601,  0.1132, -0.1546, -0.0455, -0.0954,\n",
      "         -0.0512,  0.1773, -0.1578, -0.0489, -0.1068, -0.0196,  0.0869, -0.0498,\n",
      "         -0.0834,  0.0632,  0.0749,  0.0989, -0.0011, -0.0934, -0.0903, -0.0573,\n",
      "          0.0562, -0.0934, -0.0274,  0.0376,  0.2146,  0.0413,  0.0304, -0.0715,\n",
      "          0.1217, -0.0909,  0.1196,  0.0224,  0.0212, -0.0289,  0.0734, -0.1166,\n",
      "          0.0379,  0.1453, -0.1991, -0.1180,  0.0931, -0.0650, -0.0690,  0.0157,\n",
      "          0.1252, -0.0985,  0.1322, -0.1907, -0.0653, -0.0911, -0.0773, -0.1590,\n",
      "          0.1439, -0.0481, -0.1818, -0.1797,  0.0371,  0.1511, -0.1623, -0.1519,\n",
      "          0.0007,  0.0910, -0.0139, -0.0742, -0.0375,  0.1489, -0.0454, -0.1793],\n",
      "        [ 0.0616, -0.0701,  0.0056,  0.1511, -0.0240, -0.0705,  0.0293,  0.0636,\n",
      "         -0.0916,  0.0020, -0.0134,  0.0796, -0.0850, -0.0028,  0.0361, -0.0669,\n",
      "         -0.3067,  0.1429, -0.0458,  0.0598,  0.0607, -0.0477, -0.0051, -0.0240,\n",
      "          0.1009, -0.0055,  0.0861, -0.2677, -0.1203, -0.0678, -0.0073, -0.1324,\n",
      "         -0.0334,  0.0507,  0.0945,  0.0802, -0.1084, -0.0306,  0.0840,  0.0800,\n",
      "          0.0358, -0.0649,  0.1931,  0.1092, -0.1128, -0.0712,  0.1549, -0.1540,\n",
      "          0.1520,  0.1895,  0.0486, -0.0538,  0.0456,  0.1606, -0.0910,  0.0678,\n",
      "          0.1936,  0.0395, -0.0671,  0.0566, -0.1658,  0.0905,  0.0633,  0.1562,\n",
      "         -0.2520, -0.2041,  0.1002,  0.1458, -0.2233, -0.0389,  0.1952,  0.0230,\n",
      "          0.1004,  0.0039,  0.0297, -0.0051,  0.0482, -0.0716,  0.0410,  0.1311]],\n",
      "       requires_grad=True) \n",
      "\n",
      "\n",
      "10   torch.Size([10]) \n",
      "  Parameter containing:\n",
      "tensor([-0.0838,  0.0204, -0.0172,  0.0138,  0.0053, -0.0699,  0.0305,  0.0152,\n",
      "        -0.0514, -0.0417], requires_grad=True) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for parameter in net.parameters():\n",
    "    i+=1\n",
    "    print(i,\" \",parameter.shape,\"\\n \",parameter,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 20] loss: 0.0007781430166214705\n",
      "[1, 40] loss: 0.0006963789463043212\n",
      "[1, 60] loss: 0.0006210292764008045\n",
      "[1, 80] loss: 0.0006234911577776075\n",
      "[1, 100] loss: 0.0007067974004894495\n",
      "[1, 120] loss: 0.0005666194641962647\n",
      "[1, 140] loss: 0.0005698428861796856\n",
      "[1, 160] loss: 0.0005996511951088905\n",
      "[1, 180] loss: 0.0005987825999036431\n",
      "[1, 200] loss: 0.0006606169948354363\n",
      "[1, 220] loss: 0.0005293490635231138\n",
      "[1, 240] loss: 0.0006380279948934913\n",
      "[1, 260] loss: 0.0006147282095625996\n",
      "[1, 280] loss: 0.0007151836287230253\n",
      "[1, 300] loss: 0.000667616417631507\n",
      "[2, 20] loss: 0.0006024905927479267\n",
      "[2, 40] loss: 0.0006141729280352593\n",
      "[2, 60] loss: 0.0006498260907828808\n",
      "[2, 80] loss: 0.0004808340426534414\n",
      "[2, 100] loss: 0.0005606071883812547\n",
      "[2, 120] loss: 0.00056076773814857\n",
      "[2, 140] loss: 0.0005572921084240079\n",
      "[2, 160] loss: 0.0006054048491641879\n",
      "[2, 180] loss: 0.0006371511779725552\n",
      "[2, 200] loss: 0.0005526713412255049\n",
      "[2, 220] loss: 0.0005881285266950727\n",
      "[2, 240] loss: 0.0005790233127772808\n",
      "[2, 260] loss: 0.0006287927152588964\n",
      "[2, 280] loss: 0.0005651360284537076\n",
      "[2, 300] loss: 0.0006342274239286781\n",
      "[3, 20] loss: 0.0006321870349347592\n",
      "[3, 40] loss: 0.0005571961663663387\n",
      "[3, 60] loss: 0.0005771349668502808\n",
      "[3, 80] loss: 0.000572165279649198\n",
      "[3, 100] loss: 0.0005184684945270419\n",
      "[3, 120] loss: 0.0005923586338758469\n",
      "[3, 140] loss: 0.0005123651064932347\n",
      "[3, 160] loss: 0.0005542675694450736\n",
      "[3, 180] loss: 0.0005371045051142573\n",
      "[3, 200] loss: 0.0007006477559916675\n",
      "[3, 220] loss: 0.0005542758759111165\n",
      "[3, 240] loss: 0.0005894057173281908\n",
      "[3, 260] loss: 0.0005208871182985603\n",
      "[3, 280] loss: 0.0005147141702473163\n",
      "[3, 300] loss: 0.0005232056733220816\n",
      "[4, 20] loss: 0.0005520684784278274\n",
      "[4, 40] loss: 0.0005057947146706283\n",
      "[4, 60] loss: 0.0005297206714749336\n",
      "[4, 80] loss: 0.000490843165665865\n",
      "[4, 100] loss: 0.0004432899607345462\n",
      "[4, 120] loss: 0.0006323963748291135\n",
      "[4, 140] loss: 0.0005013253903016448\n",
      "[4, 160] loss: 0.0005514756171032787\n",
      "[4, 180] loss: 0.0006192658571526408\n",
      "[4, 200] loss: 0.0004863710021600127\n",
      "[4, 220] loss: 0.0005960252382792532\n",
      "[4, 240] loss: 0.0005913625434041023\n",
      "[4, 260] loss: 0.0005157364420592784\n",
      "[4, 280] loss: 0.0006606097100302577\n",
      "[4, 300] loss: 0.0006023496929556131\n",
      "[5, 20] loss: 0.0005838261060416699\n",
      "[5, 40] loss: 0.00048467141576111314\n",
      "[5, 60] loss: 0.00047051091957837346\n",
      "[5, 80] loss: 0.0006289510782808065\n",
      "[5, 100] loss: 0.00045932860812172296\n",
      "[5, 120] loss: 0.0004773911153897643\n",
      "[5, 140] loss: 0.0005329903792589903\n",
      "[5, 160] loss: 0.0005594803290441632\n",
      "[5, 180] loss: 0.0006045601023361086\n",
      "[5, 200] loss: 0.0005981564903631806\n",
      "[5, 220] loss: 0.0006070067677646875\n",
      "[5, 240] loss: 0.0005593218002468347\n",
      "[5, 260] loss: 0.0005481728580780328\n",
      "[5, 280] loss: 0.000545680433511734\n",
      "[5, 300] loss: 0.0005048443581908941\n",
      "[6, 20] loss: 0.0005937767969444394\n",
      "[6, 40] loss: 0.00046769251860678194\n",
      "[6, 60] loss: 0.0004462561197578907\n",
      "[6, 80] loss: 0.0004607306597754359\n",
      "[6, 100] loss: 0.0005059792781248689\n",
      "[6, 120] loss: 0.0006267363261431456\n",
      "[6, 140] loss: 0.0005193045428022742\n",
      "[6, 160] loss: 0.0005781857268884778\n",
      "[6, 180] loss: 0.0005302653154358268\n",
      "[6, 200] loss: 0.0005597433932125568\n",
      "[6, 220] loss: 0.0005473572732880712\n",
      "[6, 240] loss: 0.0004439831916242838\n",
      "[6, 260] loss: 0.0005386204840615392\n",
      "[6, 280] loss: 0.0004617302897386253\n",
      "[6, 300] loss: 0.0005296187866479158\n",
      "[7, 20] loss: 0.0004367616344243288\n",
      "[7, 40] loss: 0.00045033903094008563\n",
      "[7, 60] loss: 0.00043277396820485593\n",
      "[7, 80] loss: 0.0006264698435552419\n",
      "[7, 100] loss: 0.0005452802143990994\n",
      "[7, 120] loss: 0.0005013646148145198\n",
      "[7, 140] loss: 0.0005324430456385016\n",
      "[7, 160] loss: 0.0005769238965585828\n",
      "[7, 180] loss: 0.0004681818978860974\n",
      "[7, 200] loss: 0.00047793574258685114\n",
      "[7, 220] loss: 0.0005255672298371791\n",
      "[7, 240] loss: 0.00046127729257568716\n",
      "[7, 260] loss: 0.0005479356832802295\n",
      "[7, 280] loss: 0.0004926476869732141\n",
      "[7, 300] loss: 0.0005245732581242919\n",
      "[8, 20] loss: 0.0005421804380603135\n",
      "[8, 40] loss: 0.0004710888788104057\n",
      "[8, 60] loss: 0.000578335352241993\n",
      "[8, 80] loss: 0.0005424957992509007\n",
      "[8, 100] loss: 0.0006063882978633046\n",
      "[8, 120] loss: 0.0004084602822549641\n",
      "[8, 140] loss: 0.00035630210069939497\n",
      "[8, 160] loss: 0.0004173887688666582\n",
      "[8, 180] loss: 0.00046909137163311245\n",
      "[8, 200] loss: 0.0005790302902460099\n",
      "[8, 220] loss: 0.0004406953305006027\n",
      "[8, 240] loss: 0.000416154763661325\n",
      "[8, 260] loss: 0.0005165750766173005\n",
      "[8, 280] loss: 0.000555642512626946\n",
      "[8, 300] loss: 0.0005783792398869991\n",
      "[9, 20] loss: 0.0004299379866570234\n",
      "[9, 40] loss: 0.0005064859949052334\n",
      "[9, 60] loss: 0.0005501205176115037\n",
      "[9, 80] loss: 0.00044497048109769824\n",
      "[9, 100] loss: 0.0005216845641843975\n",
      "[9, 120] loss: 0.00044519247394055126\n",
      "[9, 140] loss: 0.0004364016354084015\n",
      "[9, 160] loss: 0.0004067330458201468\n",
      "[9, 180] loss: 0.000474648233037442\n",
      "[9, 200] loss: 0.0004963202569633722\n",
      "[9, 220] loss: 0.0005189288007095456\n",
      "[9, 240] loss: 0.00040074606332927943\n",
      "[9, 260] loss: 0.00042056114226579665\n",
      "[9, 280] loss: 0.0005117282047867775\n",
      "[9, 300] loss: 0.0005616380111314357\n",
      "[10, 20] loss: 0.00043421732308343054\n",
      "[10, 40] loss: 0.0005731634385883808\n",
      "[10, 60] loss: 0.0004525377182289958\n",
      "[10, 80] loss: 0.0005064201224595308\n",
      "[10, 100] loss: 0.0004892386263236403\n",
      "[10, 120] loss: 0.0003648840524256229\n",
      "[10, 140] loss: 0.00046598228812217714\n",
      "[10, 160] loss: 0.00041279716696590184\n",
      "[10, 180] loss: 0.0004218361340463161\n",
      "[10, 200] loss: 0.00039794149529188874\n",
      "[10, 220] loss: 0.00048495864495635033\n",
      "[10, 240] loss: 0.0005770388506352902\n",
      "[10, 260] loss: 0.0004511049007996917\n",
      "[10, 280] loss: 0.00046070537297055125\n",
      "[10, 300] loss: 0.0005561720253899694\n",
      "[11, 20] loss: 0.0004681832203641534\n",
      "[11, 40] loss: 0.00040938206808641554\n",
      "[11, 60] loss: 0.0004499653587117791\n",
      "[11, 80] loss: 0.0003738473481498659\n",
      "[11, 100] loss: 0.0005143324425444007\n",
      "[11, 120] loss: 0.00046371083706617353\n",
      "[11, 140] loss: 0.00043494098028168084\n",
      "[11, 160] loss: 0.0004373628017492592\n",
      "[11, 180] loss: 0.00045394614757969976\n",
      "[11, 200] loss: 0.0004860265264287591\n",
      "[11, 220] loss: 0.0004858738612383604\n",
      "[11, 240] loss: 0.0004548461325466633\n",
      "[11, 260] loss: 0.0004917546701617538\n",
      "[11, 280] loss: 0.0004067078800871968\n",
      "[11, 300] loss: 0.00045100464951246976\n",
      "[12, 20] loss: 0.0003484410298988223\n",
      "[12, 40] loss: 0.00045326484087854625\n",
      "[12, 60] loss: 0.0005127134914509952\n",
      "[12, 80] loss: 0.0004267682731151581\n",
      "[12, 100] loss: 0.0004901476390659809\n",
      "[12, 120] loss: 0.0004572049146518111\n",
      "[12, 140] loss: 0.00037362427450716496\n",
      "[12, 160] loss: 0.00036002229899168015\n",
      "[12, 180] loss: 0.0004616046715527773\n",
      "[12, 200] loss: 0.0005030184239149094\n",
      "[12, 220] loss: 0.0004924813536927104\n",
      "[12, 240] loss: 0.0004453773549757898\n",
      "[12, 260] loss: 0.0004212856749072671\n",
      "[12, 280] loss: 0.00047225248627364634\n",
      "[12, 300] loss: 0.0004977383296936751\n",
      "[13, 20] loss: 0.0005170862204395234\n",
      "[13, 40] loss: 0.0004405929995700717\n",
      "[13, 60] loss: 0.0004596748556941748\n",
      "[13, 80] loss: 0.0005236714584752918\n",
      "[13, 100] loss: 0.0003997714933939278\n",
      "[13, 120] loss: 0.0003676965250633657\n",
      "[13, 140] loss: 0.0004619064973667264\n",
      "[13, 160] loss: 0.000455895921215415\n",
      "[13, 180] loss: 0.0003957887226715684\n",
      "[13, 200] loss: 0.00034106369689106944\n",
      "[13, 220] loss: 0.00043110010866075755\n",
      "[13, 240] loss: 0.0005087302727624774\n",
      "[13, 260] loss: 0.00046820365078747273\n",
      "[13, 280] loss: 0.0004364851415157318\n",
      "[13, 300] loss: 0.000430891209281981\n",
      "[14, 20] loss: 0.0004155737399123609\n",
      "[14, 40] loss: 0.000379936165176332\n",
      "[14, 60] loss: 0.0004878921890631318\n",
      "[14, 80] loss: 0.00038769157882779837\n",
      "[14, 100] loss: 0.0003504661452025175\n",
      "[14, 120] loss: 0.0005014170967042446\n",
      "[14, 140] loss: 0.0004427214139141142\n",
      "[14, 160] loss: 0.0004348690016195178\n",
      "[14, 180] loss: 0.0004794037993997335\n",
      "[14, 200] loss: 0.0003346252553164959\n",
      "[14, 220] loss: 0.0004617485040798783\n",
      "[14, 240] loss: 0.00048613621853291986\n",
      "[14, 260] loss: 0.0004300395757891238\n",
      "[14, 280] loss: 0.00047255536541342737\n",
      "[14, 300] loss: 0.0004086196185089648\n",
      "[15, 20] loss: 0.00035167886316776276\n",
      "[15, 40] loss: 0.0003466470977291465\n",
      "[15, 60] loss: 0.0004216395956464112\n",
      "[15, 80] loss: 0.00043514391500502827\n",
      "[15, 100] loss: 0.00041669674776494505\n",
      "[15, 120] loss: 0.0005379571318626404\n",
      "[15, 140] loss: 0.00036313189985230567\n",
      "[15, 160] loss: 0.0004937001867219806\n",
      "[15, 180] loss: 0.000399334276560694\n",
      "[15, 200] loss: 0.0004286566409282386\n",
      "[15, 220] loss: 0.0004910965822637081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 240] loss: 0.00040409001894295215\n",
      "[15, 260] loss: 0.0004090298707596958\n",
      "[15, 280] loss: 0.00047984640346840026\n",
      "[15, 300] loss: 0.00039042309997603297\n",
      "[16, 20] loss: 0.00037332237511873245\n",
      "[16, 40] loss: 0.0003823210075497627\n",
      "[16, 60] loss: 0.00038801103504374623\n",
      "[16, 80] loss: 0.0003589499536901712\n",
      "[16, 100] loss: 0.0003279660036787391\n",
      "[16, 120] loss: 0.0005147481691092253\n",
      "[16, 140] loss: 0.0003691333970054984\n",
      "[16, 160] loss: 0.0003622701009735465\n",
      "[16, 180] loss: 0.0005367193538695574\n",
      "[16, 200] loss: 0.0005411702785640955\n",
      "[16, 220] loss: 0.00044743637926876543\n",
      "[16, 240] loss: 0.0004599529905244708\n",
      "[16, 260] loss: 0.00033929993072524665\n",
      "[16, 280] loss: 0.0003903031670488417\n",
      "[16, 300] loss: 0.00046895410725846887\n",
      "[17, 20] loss: 0.0004412261159159243\n",
      "[17, 40] loss: 0.0004207261330448091\n",
      "[17, 60] loss: 0.0003953906502574682\n",
      "[17, 80] loss: 0.0003657355126924813\n",
      "[17, 100] loss: 0.0004216242833063006\n",
      "[17, 120] loss: 0.0003840871420688927\n",
      "[17, 140] loss: 0.00047985035087913275\n",
      "[17, 160] loss: 0.00036968138348311187\n",
      "[17, 180] loss: 0.0003764083236455917\n",
      "[17, 200] loss: 0.00033534957794472575\n",
      "[17, 220] loss: 0.0003433856121264398\n",
      "[17, 240] loss: 0.00039732387522235514\n",
      "[17, 260] loss: 0.0005311766499653458\n",
      "[17, 280] loss: 0.0004280176251195371\n",
      "[17, 300] loss: 0.0003969321339391172\n",
      "[18, 20] loss: 0.0003745455220341682\n",
      "[18, 40] loss: 0.0004610842252150178\n",
      "[18, 60] loss: 0.00036316206958144905\n",
      "[18, 80] loss: 0.0003979153749532998\n",
      "[18, 100] loss: 0.00039194370014593007\n",
      "[18, 120] loss: 0.0003701247754506767\n",
      "[18, 140] loss: 0.00042307580634951594\n",
      "[18, 160] loss: 0.0004650389198213816\n",
      "[18, 180] loss: 0.0004043661253526807\n",
      "[18, 200] loss: 0.0003466932433657348\n",
      "[18, 220] loss: 0.0003276892085559666\n",
      "[18, 240] loss: 0.0003707932706456631\n",
      "[18, 260] loss: 0.00037965139048174026\n",
      "[18, 280] loss: 0.0004031665050424635\n",
      "[18, 300] loss: 0.0005040868567302823\n",
      "[19, 20] loss: 0.000306541012134403\n",
      "[19, 40] loss: 0.0004489783076569438\n",
      "[19, 60] loss: 0.0003312134123407304\n",
      "[19, 80] loss: 0.00035667573707178233\n",
      "[19, 100] loss: 0.0004099393058568239\n",
      "[19, 120] loss: 0.000415689286775887\n",
      "[19, 140] loss: 0.0004157838514074683\n",
      "[19, 160] loss: 0.0004008936500176787\n",
      "[19, 180] loss: 0.00044357144460082054\n",
      "[19, 200] loss: 0.00034832672495394945\n",
      "[19, 220] loss: 0.0003166099609807134\n",
      "[19, 240] loss: 0.00035183580592274666\n",
      "[19, 260] loss: 0.00039505906030535697\n",
      "[19, 280] loss: 0.0004297626316547394\n",
      "[19, 300] loss: 0.0004207198417279869\n",
      "[20, 20] loss: 0.0003839859873987734\n",
      "[20, 40] loss: 0.0003979184376075864\n",
      "[20, 60] loss: 0.00034014521492645145\n",
      "[20, 80] loss: 0.0003579460172913969\n",
      "[20, 100] loss: 0.00039022556971758605\n",
      "[20, 120] loss: 0.0003553870590403676\n",
      "[20, 140] loss: 0.00034298786241561174\n",
      "[20, 160] loss: 0.00033061670092865826\n",
      "[20, 180] loss: 0.00041297622583806517\n",
      "[20, 200] loss: 0.000428572247736156\n",
      "[20, 220] loss: 0.00048620374221354723\n",
      "[20, 240] loss: 0.00038739363430067897\n",
      "[20, 260] loss: 0.0003272555242292583\n",
      "[20, 280] loss: 0.0003193573676981032\n",
      "[20, 300] loss: 0.0003590956456027925\n",
      "Finished Reraining\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # print(inputs.shape)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print(\"[{}, {}] loss: {}\".format\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Reraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"LeNET_x_y_MNIST_Model_My_Exiperiment_2_Fine_Tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, len(net.state_dict()[\"fc1.weight\"]))\n",
    "        self.fc2 = nn.Linear(len(net.state_dict()[\"fc1.weight\"]), len(net.state_dict()[\"fc2.weight\"]))\n",
    "        self.fc3 = nn.Linear(len(net.state_dict()[\"fc2.weight\"]), 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(\"LeNET_x_y_MNIST_Model_My_Exiperiment_2_Fine_Tuned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 240]          61,680\n",
      "            Linear-6                   [-1, 80]          19,280\n",
      "            Linear-7                   [-1, 10]             810\n",
      "================================================================\n",
      "Total params: 84,342\n",
      "Trainable params: 84,342\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.32\n",
      "Estimated Total Size (MB): 0.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device=torch.device(\"cpu\")\n",
    "model=Net().to(device)\n",
    "summary(model, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, number of parametes reduced down to 86k from 110 k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.540000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "[https://towardsdatascience.com/how-to-cluster-in-high-dimensions-4ef693bacc6] [1/11/2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
